---
title: 'Building broad-shouldered giants: meta-analytic methods for reproducible research'
author:
- affiliation: 1
  name: Christina Bergmann
- affiliation: 1
  name: Sho Tsuji
- affiliation: 2
  name: Page Piccinini
- affiliation: 3
  name: Molly Lewis
- affiliation: 3
  name: Mika Braginsky
- affiliation: 3
  name: Michael C. Frank
- affiliation: 1
  name: Alejandrina Cristia
affiliation:
- id: 1
  institution: Laboratoire de Sciences Cognitives et Psycholinguistique, ENS
- id: 2
  institution: NeuroPsychologie Interventionnelle, ENS
- id: 3
  institution: Department Psychology, Stanford University

keywords: replicability, reproducibility, meta-analysis, developmental psychology,
  language acquisition
lineno: no
note: |
  Correspondence concerning this article should be addressed to Christina Bergmann, Laboratoire de Sciences Cognitives et Psycholinguistique, ENS. 29 Rue d'Ulm, 75005 Paris, France. E-mail: chbergma@gmail.com
abstract: null
shorttitle: MetaLab Education
Overall goal: How can developmental scientists improve their practices towards more
  robust, reliable, and reproducible research?
wordcount: XXXX

class: man
lang: english
figsintext: yes
bibliography:
  - metalab_education.bib
header-includes:
- \usepackage{setspace}
- \usepackage{float}
- \usepackage{graphicx}
- \AtBeginEnvironment{tabular}{\singlespacing}
- \usepackage{pbox}

output: papaja::apa6_pdf
---

```{r Preamble, echo = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE, fig.pos = "T")
require("papaja")
apa_prepare_doc() # prepare document for rendering

source("../../dashboard/global.R", chdir = TRUE)
library(metafor)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(purrr)
library(knitr)
library(broom)
library(pwr)
library(lme4)

```

# Introduction


Psychology has seen a recent "crisis of confidence" in key findings, as many subfields are plagued by issues of low reliability and validity of their data [CITE].
Replicability, that is conducting conceptually similar experiment with new stimuli and in a slightly different population but following the same procedure and analyses (based on the published report) with the same outcome as reported (allowing for a margin of error), is a core concept in this recent crisis. Being able to (repeatedly) successfully replicate a study can be taken as an indicator that the phenomenon under investigation is true and theories can be built on it. This means that a single published report is not sufficient to establish the existence of a phenomenon, and misleading reports might be caused by a number of issues. Next to spurious findings (which can occur even when following best practices), a number of habits in psychological research might result in outcomes not reflecting whether or not a phenomenon is present in the population. These habits include running underpowered studies as well as confining non-significant results to the file-drawer. 

The above mentioned issues are potentially exacerbated in child studies, because the population under investigation is difficult and costly to both recruit and test. Small sample sizes and noisy measures are a consequence, which in turn lead to habitually underpowered studies. It is thus no surprise that some assume the next "crisis of confidence" brought about by low replicability of core effects will be concerning the field of developmental psychology [@Manybabies]. The present paper aims to quantify both some of the potentially problematic habits of developmental researchers, and show a way forward. We are thus adding to a recently emerging literature that critically examines long-held standards and practices in order to make the whole field more reliable and robust [@Mills-Smith2015, @Csibra]. 




# The status quo


***TO DO: Expand ***

In this section, we survey current practices in the field of developmental psychology. 
We focus on reporting and experiment planning practices. 




## The Dataset: MetaLab


The subsequent analyses are based on MetaLab, an online collection of meta-analyses on early language development. Meta-analyses are built on a collection of standardized effect sizes on a single, well-defined phenomenon. By accumulating effect sizes and weighting them by their reliability, it is possible to compute an estimate of the population effect. Consequently, meta-analyses do not rely on one (possibly false) study outcome, be it significant or not. Despite their overall utility, meta-analyses are not frequently conducted in most branches of developmental psychology. Instead, narrative summaries are the dominant tool to build theories, and that single studies are cited as evidence for the presence or absence of an ability instead of meta-analyses. Currently, MetaLab contains 12 meta-analyses, but it is open to submissions and updates. The present analyses thus are a snapshot; through dynamic reports on the website, and by downloading the freely available data, it is continuously possible to obtain the most recent data. 

In MetaLab, various meta-analyses are combined that address phenomena ranging from infant-directed speech preference to mutual exclusivity. Those datasets were either added by the authors (n=XXX) or extracted from published papers (n=XXXX). In the former case, we attempted to code as much detail as possible for each entered experiment (note that a paper can contain many experiments). A high level of detail allows not only to retrieve general measures of interest, but also to conduct follow-up analyses into different possible research questions and prospective power calculations taking as much methodological detail as possible into account.

Overall, parts of each meta-analysis are standardized to allow for the computation of common effect size estimates and for analyses that span different phenomena. These standardized variables include study descriptors (such as citation and peer review status), participant characteristics (including mean age and age range, percent female participants), methodological infomation (for example what dependent variable was measured), and information necessary to compute effect sizes (number of participants, if available means and standard deviations of the dependent measure, otherwise test statistics, such as t-values or F scores). 

As dependent measure, we report Cohen's *d*, a standardized effect size based on comparing sample means. This effect size was calculated when possible from sample means and standard deviations across designs with the appropriate formula. When these data were not available, we used test statistics, more precisely t-values or F scores of the test assessing the main hypothesis. We also computed the variance of this effect size, which allows to weigh each effect size when aggregating across studies. The variance is mainly determined by the number of participants; intuitively effect sizes based on larger samples will be weighted higher. Note that for research designs testing participants in two conditions that need to be compared (for example exposing the same infants to infant- and adult-directed speech), correlations between those two measures are needed to estimate the effect size variance. This measure is usually not reported. Some correlations could be obtained through direct contact with the original authors (see e.g., [@InWordDB] for details), for others we estimated this factor. 

Descriptions of all phenomena covered by MetaLab, including which papers and other sources have been considered, can be found on the companion website at metalab.stanford.edu and in the supporting information. Table XX shows an overview of the phenomena covered.

## Average sample size, effect size, and power per phenomenon

A first assessment of current practices in developmental psychology concerns which variables of interest are habitually reported, and what crucial information is ommitted in published papers. To this end we survey the data available in MetaLab, with a focus on participant descriptors, that is age, age range, total participant number and percentage of girls tested; and outcome variables, including the mean and standard deviation of the dependent variable, a test statistic, that is a t-value or F-score, for the main hypothesis test (for example whether looking times differ), and the correlation between outcome variables when infants were tested in multiple conditions (frequently target and control). The data are summarized in Table 1. We break these statistics down by topic and provide the number of papers and experiments (one paper usually contains several experiments). Note that the values are compiled after authors were contacted when information was missing from the published report (details are available in the respective publications). Further, we note whether effect sizes in the form of a variant of Cohen's *d* were already available; this number includes meta-analyses that were entered based on published papers [@DunstHamby, @Colonessi]. 

APA recommendations [@APA2001] have for almost two decades included the requirement to always report and interpret an effect size for all measures of relevance, along with p-values. Effect sizes should, according to those recommendations, be interpreted both when the p-value is above the significance threshold as well as when the significance criterion is met. However, current reproting habits do not follow this recommendation, especially for nonsignificant findings, and if effect sizes are reported, their interpretation is either lacking or misleading [@Mills-Smith2015]. This practice has strong implications for theory building, as the data they are based on might not be reliable. 

The table below provides summary information for each meta-analysis in MetaLab regarding a number of factors, including the number of single effect sizes and that of papers contributing to a given dataset. 
Phenomena differ in the age groups typically tested and the age range covered. This is of high importance, both theoretically, as younger infants might generate more noisy behaviors and are not as advanced in their linguistic abilities, and practically, as older infants might be subjected to more robust methods and could be a more readily available participant pool. 
The typical sample size as well as the minimum and maximum (allowing to estimate the range in our data) is noted as well. Based on the meta-analytical effect size and the average number of participants, we calculated typical power. Note that recommendations are for this value to be above 80%, which refers to a likelihood that 4 out of 5 studies show a significant outcome for an effect truly present in the population. 

Underpowered studies, that is studies with a low probability to detect an effect given it is present in the population, pose a problem for branches of developmental studies that interpret both significant and nonsignificant findings; for example when tracking the emergence of an ability as children mature or when examining the boundary conditions of an ability. This practice is problematic for two reasons: On one hand, the null hypothesis, for example that two groups do not differ, is not being tested, so it cannot be adopted based on a high p-value. Instead, p-values can only support rejections of the null hypothesis with a certainty that the data at hand are incompatible with it below a pre-set threshold. On the other hand, even in the most rigorous study design and execution, null results will occur ever so often; for example in a study with 80% power (a number typically deemed sufficient), every fifth result will not reflect that there is a true effect present in the population. Disentangling whether a non-significant finding indicates the absence of a skill, random measurement noise, or the lack of experimental power to detect this skill reliably and with statistical support is impossible based on p-values.

```{r Get Descriptive Information, echo = FALSE}

# Add typical method??
# Add age

data_rma = all_data %>%
  nest(-dataset, .key = information) %>%
  mutate(model = map(information, ~rma(d_calc, sei=d_var_calc, data=.))) %>%
  mutate(d = map(model, "b")) %>%
  mutate(se = map(model, "se")) %>%
  select(dataset, d, se) %>%
  mutate(d = as.numeric(as.character(d))) %>%
  mutate(se = as.numeric(as.character(se)))

# This function is a hack, should update to just use map with MA_descriptives
get_power = function(df){
  pwr.t.test(n = df$n_dataset, d = df$d, sig.level = 0.05)
}

MA_descriptives = all_data %>%
  mutate(n_total = n) %>% #ifelse(!is.na(n_2), n_1 + n_2, n_1)) %>% I think n does the same thing
  group_by(dataset) %>%
  summarise(age_dataset = mean(mean_age_months),
            n_dataset = mean(n_total),
            n_min = min(n_total),
            n_max = max(n_total),
            n_records = n(),
            n_papers = length(unique(short_cite))) %>%
  ungroup() %>%
  inner_join(data_rma)

MA_power = MA_descriptives %>%
  nest(-dataset, .key = descriptives) %>%
  mutate(power = map(descriptives, get_power)) %>%
  mutate(power = map(power, "power")) %>%
  select(dataset, power) %>%
  mutate(power = as.numeric(as.character(power)))

MA_summary = inner_join(MA_descriptives, MA_power)
  
kable(MA_summary, col.names = c("Meta Analysis (MA)", "Mean Age in Months", "Mean Sample Size", "Min. Sample Size", "Max. Sample Size", "# Effect Sizes", "# Papers", "d", "SE", "Avg Power"), digits = 2, caption = "Descriptions of meta-analyses currently in MetaLab.")
```





***TO DO: Exclude dbs based on published MAs from total d reported?!***

```{r Data Availability, echo = FALSE}
#From report Reproducibility by ML and CB
counts = all_data %>%
    mutate(corr_within_two  = ifelse(participant_design == "within_two", as.numeric(corr), NA)) %>% # creates a new column where any time design is not within_two "corr" becomes an NA
  summarise(test_statistic = sum(!is.na(t) | !is.na(F) | !is.na(r)),
                      means = sum(!is.na(x_1)),
                      SD = sum(!is.na(SD_1)),
                      d = sum(!is.na(d)),
                      #corr = sum(!is.na(corr)), 
                      corr_within_two = sum(!is.na(corr_within_two)),
                      mean_age = sum(!is.na(mean_age_1)),
                      age_range = sum(!is.na(age_range_1)),
                      gender = sum(!is.na(gender_1))) %>%
  gather("coded_variable", "n_coded") %>%
  #mutate(total = nrow(all_data))  %>%
  mutate(total = ifelse(coded_variable == "corr_within_two",
                        nrow(subset(all_data, participant_design == "within_two")),
                                    nrow(all_data))) %>%
  mutate(coded_variable = factor(coded_variable)) %>%
  mutate(n_uncoded = total - n_coded) %>%
  select(coded_variable, n_coded, n_uncoded, total)
        

kable(counts, col.names = c("Variable", "# Coded", "# Uncoded", "Total"), caption = "Reporting practices of study outcome measures and demographic information for all papers in MetaLab.")
```
*** TO DO: Add summary across datasets? ***


###  Power: Comparing meta-analytic effect size and oldest paper
As Table 1 shows, experimenters are habitually not including a sufficient number of participants to observe a given effect, assuming the meta-analytic estimate for a given topic. It might, however, be possible, that power has been determined based on a seminal paper to be replicated. Initial reports tend to overestimate effect sizes [CITE], possibly explaining the lack of power in some sub-domains. We extracted for each dataset the oldest paper and therein the largest reported effect size and re-calculated power accordingly. The results are shown in the table below. It turns out that in some cases, such as tests into native and non-native vowel discrimination, sample size choices match well with the oldest report. 

```{r Find Largest d for Oldest Paper, echo = FALSE}
oldest = all_data %>%
  group_by(dataset, short_cite) %>%
  summarise(year = max(year),
           largest_d = max(d_calc)) %>%
  ungroup() %>%
  group_by(dataset) %>%
  arrange(year) %>%
  filter(row_number() == 1) %>%
  ungroup()
```


```{r Compute Meta-analytic d for All Papers, echo = FALSE}
overall_es <- function(ma_data){
  model = metafor::rma(ma_data$d_calc, ma_data$d_var_calc, method = "REML",
               control = list(maxiter = 1000, stepadj = 0.5))
    data.frame(dataset = ma_data$dataset[1],
               overall.d = model$b,
               ci_lower = model$ci.lb,
               ci_upper = model$ci.ub)
}

MA_summary_sub = select(MA_summary, c(dataset, n_dataset))

get_power_oldest = function(df){
  pwr.t.test(n = df$n_dataset, d = df$largest_d, sig.level = 0.05)
}

d_comparison = inner_join(oldest, MA_summary_sub) %>%
  select(dataset, short_cite, largest_d, n_dataset)

d_comparison_power = d_comparison %>%
  nest(-dataset, .key = descriptives) %>%
  mutate(power = map(descriptives, get_power_oldest)) %>%
  mutate(power = map(power, "power")) %>%
  select(dataset, power) %>%
  mutate(power = as.numeric(as.character(power)))

d_comparison_summary = inner_join(d_comparison, d_comparison_power)

kable(d_comparison_summary, col.names = c("Meta-analysis (MA)", "Oldest Paper", "Oldest d", "Mean Sample Size", "Power"), digits = 2, caption = "For each meta-analysis, largest d from oldest paper and meta-analytic d.")
```

*** To Do: align with table 1***


```{r Plot of Difference of d Values, echo = FALSE, fig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap="Correlation of largest d from oldest paper and difference between oldest d and meta-analytic d."}

d_comparison_full = inner_join(d_comparison, MA_summary) %>%
  select(dataset, n_dataset, d, largest_d) %>%
  mutate(diff_d = largest_d - d)

d_comparison_diff.plot = ggplot(d_comparison_full, aes(x = largest_d, y = diff_d)) +
 geom_point(aes(color = dataset)) +
 geom_smooth(method = "lm", color = "black") +
 xlab("Largest d for Oldest Paper") +
 ylab("Difference between largest d and meta-analytic d") +
 labs(color = "Meta-analysis") +
 theme_classic() +
 theme(axis.line.x = element_line(), axis.line.y = element_line(),
       legend.position = "top")
 
d_comparison_diff.plot
```

To illustrate the disparity between the oldest effect size and the meta-analytic effect, we plot the difference between both against the oldest effect. The difference is larger as oldest effect size increases. This plot showcases that researchers might want to be wary of large effects, as they are more likely to be non-representative of the true phenomenon compared to smaller initial effects being reported. 


## What is the effect of method choice?



### Do researchers chose efficient methods? 

Are methods chosen based on the proportion of participants that can be retained for data analysis? 



```{r Method vs excluded, echo = FALSE}
# Centering mean age
method_exclude_data = all_data %>%
  mutate(ageC = ifelse(participant_design == "between",
                       ((mean_age_1 - mean(mean_age_1)) + (mean_age_1 - mean(mean_age_1)))/2,
                       ((mean_age_1 - mean(mean_age_1))))) %>%
  mutate(keep = ifelse(is.na(n_2), n_1, n_1 + n_2)) %>%
  mutate(dropout = ifelse(is.na(n_excluded_1), NA, ifelse(is.na(n_excluded_2), n_excluded_1, n_excluded_1+n_excluded_2))) %>%
  mutate(total_run = keep + dropout) %>%
  filter(!is.na(dropout)) %>%
  mutate(percent_dropout = dropout / total_run) %>%
  group_by(method) %>%
  mutate(number = n()) %>%
  ungroup() %>%
  filter(number >10) %>%
  mutate(method = factor(method)) %>%
  select(percent_dropout, keep, dropout, total_run, dataset, ageC, method)


method_exclude.m <- lmer(percent_dropout ~ method + ageC +
                           (1|dataset), data = method_exclude_data)
method_exclude.m_sum = summary(method_exclude.m)

kable(coef(method_exclude.m_sum), caption = "Method vs Dropout")
```


```{r Effect of Method, echo = FALSE}
# Centering mean age
method_data = all_data %>%
  mutate(ageC = ifelse(participant_design == "between",
                       ((mean_age_1 - mean(mean_age_1)) + (mean_age_1 - mean(mean_age_1)))/2,
                       ((mean_age_1 - mean(mean_age_1))))) %>%
  group_by(method) %>%
  mutate(number = n()) %>%
  ungroup() %>%
  filter(number >20) %>%
  mutate(method = factor(method))

method.rma <- rma.mv(d_calc, d_var_calc, mods = ~ageC * relevel(method, "head-turn preference procedure"), random = ~ short_cite |  dataset, data = method_data)

method.rma_sum = summary(method.rma)

kable(coef(method.rma_sum), caption = "Effect of d by method with hpp as baseline method.")
```



We built a meta-analytic model with the effect size measure Cohen's *d* as the dependent variable, method and mean age of population centered as independent variables. The model also includes the variance of *d* for sampling variance, and paper within meta-analysis as a random effect (because we assume that within a paper experiments and thus effect sizes will be more similar to each other than across papers). Only methods with at least 20 associated effect sizes in MetaLab were included in the model. Thus, the present analyses are limited to `r nrow(method_data)` observations. The included methods are `r levels(method_data$method)`. Since the model compares one method as the baseline to all other methods, a baseline method had to be chosen. "Head-turn preference procedure" was included as the baseline method, as it appears most frequently in MetaLab (`r sum(method_data$method=="head-turn preference procedure")` times out of `r nrow(all_data)` total entries). 



```{r Plot of Effect of Method, echo = FALSE, ig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "Effect size as explained by different methods and mean age of participants."}


method.plot = ggplot(method_data, aes(x = mean_age_months, y = d_calc, color = method)) +
  # Commenting out points for now since they make the rest of the graph kind of hard to see
  #geom_point() +
  geom_smooth(method = "lm") +
  xlab("Mean age of participants in months") +
  ylab("Effect size (d)") +
  labs(color = "Method") +
  theme_classic() +
  theme(axis.line.x = element_line(), axis.line.y = element_line(),
        legend.position = "top")

method.plot


```

***TO DO: Add caveats***

#General Discussion



## Recommendation: appreciate meta-analyses more

Meta-analyses and meta-analytic thinking might help solve some of the issues we uncovered.

The reluctance to appreciate meta-analyses is  evident when comparing citation rates of the initial paper with a meta-analysis on the same phenomenon. Consider the example of infant-directed speech preference, where infants listen longer to speech stimuli showing the typical characteristics of parents talking to their young children. This phenomenon is both theoretically and practically highly relevant and thus receives substantial attention from the field, not the least in a recent large-scale replication attempt [@Manybabies]. 
A meta-analysis on this phenomenon was published in 2012 [@DunstHamby], taking 34 studies into account. The oldest paper stems from 1983 [Cite Glenn & Cunningham "What do babies listen to most? A developmental study of auditory preferences in nonhandicapped infants and infants with Down's syndrome."], and the seminal work (measured by the number of citations) was published in 1990 [CITE Cooper Aslin "Preference for infant-directed speech in the first month after birth"]. Comparing these three papers by the number of citations divided by the years since publication (retreived from google scholar on September 2, 2016) shows that the seminal paper is cited an order of magnitude more every year (on average 24.3 times) than the meta-analysis (2.75 times). This is indicative of practices both when constructing theories and planning experiment: The quantified evidence is under-appreciated, despite providing a number of useful measures, such  as effect sizes for different age groups and for various methodological decisions such as stimulus type (synthetic versus natural speech, the own mother versus a stranger, among other things). This is both highly relevant for theories, as the observation of an increased preference for infant-directed speech is a qualitative observation that can allow for more fine-grained hypothesizing. Practically, the information about effect size changes and the impact of method allow for more robust experiment planning and power calculations. We will come back to the issue of power and the impact of considering a seminal paper versus a meta-analysis below. 
Similar observations hold for other meta-analyses currently available [CITE inphondb, inworddb, others outside language development?]. 

While anecdotal, this survey showcases current practices and points to one reason for underpowered studies. If authors only consider a single seminal paper to estimate the number of participants necessary, they might habitually run under-powered studies. We show this in dedicated analyses on meta-analytic versus seminal effect size and the resulting typical power in a literature. 

### Why don't we do MAs?


Meta-analyses are also seldomly conducted. This is due to high hurdles and few rewards. Conducting a meta-analysis is a laborious process, particularly according to common practice where only a few people do the work, with little ready-to-use support tools and educational materials available. Incentives for creating meta-analyses are low, as public recognition is tied to a single publication. The benefits of meta-analyses for the field, for instance the possibility to conduct power analyses, are often neither evident nor accessible to individual researchers, as the data are not shared and traditional meta-analyses remain static after publication, aging quickly as new results emerge. 


A final impediment to meta-analyses in developmental science are, as we illustrated in more detail in section XXX, current reporting standards, which make it difficult and at times even impossible to compute effect sizes from the published literature. As consequence, both systematic, full-scale meta-analyses, and a targeted priori calculation of power and thus the determination of appropriate sample sizes are not yet common practice. Our analyses span various journals and publication years and are thus complementary to recent reports on the overall lack of power in (developmental) psychology based on single-journal/-year samples [e.g., @Marszalek2011]. 

## Going forward: How can MetaLab help change practices?


MetaLab is built on two core principles: lowering hurdles to foster the implementation of practices which are rapidly becoming standard procedure, not only in other branches of psychology (such as effect size estimation and power calculation), and crowdsourcing to decrease the workload of single researchers. MetaLab is based on the recently proposed concept of community-augmented meta-analyses [CAMAs; @Tsuji2014], which combine meta-analyses and open repositories. The advantages of this union are that meta-analyses are shared and get updated continuously, so they can capture the most recent state of the literature and are open to contributions of unpublished results. 

MetaLab expands on CAMAs by providing an infrastructure for a range of uses. It is possible to gain an overview of the literature, get insights into specific topics through dynamically rendered reports, conduct power calculations, and contribute not only single recent or unpublished datasets, but whole meta-analyses that can then be opened to contributions and analysis. All meta-analyses share a core of 20 variables which not only allow for the computation of effect sizes across vastly different studies, but also provide the basis for further comparisons. These comparisons are both of practical and theoretical importance, for example can we compare which method is more robust and suitable for various ages. Researchers then can both better compare existent findings and plan their own research to be more effective. This becomes possible by focusing on a high-level but specific and constrained topic, in the case of MetaLab this is early language development and adjacent phenomena. 

MetaLab also poses several advantages compared to existing software for meta-analyses. First, adding meta-analyses is supported not only by sharing standardized formats but also by offering guidance in identifying the correct data to enter, based on the extant data and examples from the developmental psychology literature. Further, novice users can easily engage with the platform to estimate, for example, effect sizes, or decide on sample sizes with a simple interactive tool. Secondly, since all data and scripts are freely and openly available, it is possible to inspect and if needed correct all computations. Errors are thus removed much more swiftly than would be the case for (commercial) software, without losing the benefit of a stable platform. By changing current practices, we aim to increase the reliability of developmental findings and thus the credibility of the field, which has recently come under fire [e.g., @Peterson2016].

## With these tools, what do we have to do?

[Tutorial section]

On the individual level:

-	How to determine participants: Power calculator, typical N in the field
-	How to run the best possible study: Make design choices to have a more robust measure (smaller sample and more power)
-	How do I report my data? Best reporting practices (include correlations for within, always report means and SD); and possibly best visualization practices

Further individual benefits:

-	Don't despair when a null result occurs, you can still help the community with it 
-	For replication / training purposes possible to compare ES and select robust ones

On the general level:

-	Evidence becomes more reliable
-	New evidence can be integrated with previous work directly without much effort 
-	Complete, unbiased overview of a research literature 
    -	Identify unexplained variance
    -	Where are gaps? 
    -	Which moderators (do not) affect outcomes 
        -	Examples from published MAs: 
            -	InWordDB lack of age effect (predicted and strongly assumed in the field)
            -	InPhonDB confirmation of diverging effects for native / nonnative, with a quantitative timeline


# References



---
title: 'Building broad-shouldered giants: meta-analytic methods for reproducible research'
author:
- affiliation: 1
  name: Christina Bergmann
- affiliation: 1
  name: Sho Tsuji
- affiliation: 2
  name: Page Piccinini
- affiliation: 3
  name: Molly Lewis
- affiliation: 3
  name: Mika Braginsky
- affiliation: 3
  name: Michael C. Frank
- affiliation: 1
  name: Alejandrina Cristia
affiliation:
- id: 1
  institution: Laboratoire de Sciences Cognitives et Psycholinguistique, ENS
- id: 2
  institution: NeuroPsychologie Interventionnelle, ENS
- id: 3
  institution: Department Psychology, Stanford University

keywords: replicability, reproducibility, meta-analysis, developmental psychology,
  language acquisition
lineno: no
note: |
  Correspondence concerning this article should be addressed to Christina Bergmann, Laboratoire de Sciences Cognitives et Psycholinguistique, ENS. 29 Rue d'Ulm, 75005 Paris, France. E-mail: chbergma@gmail.com
abstract: null
shorttitle: MetaLab Education
Overall goal: How can developmental scientists improve their practices towards more
  robust, reliable, and reproducible research?
wordcount: XXXX

class: man
lang: english
figsintext: yes
bibliography:
  - metalab_education.bib
header-includes:
- \usepackage{setspace}
- \usepackage{float}
- \usepackage{graphicx}
- \AtBeginEnvironment{tabular}{\singlespacing}
- \usepackage{pbox}

output: papaja::apa6_pdf

---

```{r Preamble, echo = FALSE, warning = FALSE, message = FALSE}
# These packages are need in the here for some in-document commands
library(knitr)
library(dplyr)
library(pwr)

knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE, fig.pos = "T")
require("papaja")
apa_prepare_doc() # prepare document for rendering

#source("save_analyses.R") # if changes have been made to any analyses run this, comment below
load("educationpaper_environment.RData") # if no changes to analyses run this, comment above
```

# Introduction

Psychology has recently seen a  "crisis of confidence", as recent findings challenge both the validity  of certain key findings CITE-MANYLABS1 as well as the general replicability rate of published findings CITE-MANYLABS2. In this process, certain generalized practices, ranging from data collection and analysis to publication, are discussed as being potentially intrinsically flawed CITE-FALSEPOSPSYC,SCIUTOPIA1,2. In other words, psychology is facing today the same issues that caused substantial changes in research practices within the medical sciences a decade ago CITE-http://www.nejm.org/doi/full/10.1056/nejme048225. The present paper aims to discuss to what extent these issues are potentially present in developmental psychology, provide a quantitative estimation of the prevalence certain problematic practices, and discuss potential solutions. 

## Relevance of the confidence crisis for developmental psychology
The problems underlying recent confidence crises are thought to be true of empirical sciences at large, given the current reward structure CITE-UTOPIA. Specifically,  at present researchers are valued on the basis of the quantity and impact of their publications, and publication on a high-impact venue is dependent, among other factors, on (a) the topic being "hot"; (b) the result being surprising; and (c) the result being statistically significant. One of the obvious consequences of this reward structure is that replications and even conceptual extensions are less likely to be undertaken (since they are not rewarded as much), and even if they are, they are unlikely to be published, particularly if they reveal a non-significant result. Furthermore, modeling suggests that these issues may be exacerbated depending on the characteristics of a given subfield  [@Ioannidis2005], being enhanced in fields where the underlying effect sizes and common sample sizes are smaller.

We believe that all of these descriptions are saliently relevant to developmental studies, particularly those focusing early and middle childhood. Since the population under investigation is costly to recruit and difficult test, there is a pressure towards small sample sizes. Moreover, the populations are intrinsically variable, likely leading to greater variance and smaller underlying effect sizes. At the confluence of these two factors, one expects to find habitually underpowered studies, which is problematic both when assessing whether an effect is truly present or absent, and when estimating its magnitude. 

There is a conceptually separate set of issues, which is nonetheless relevant given the aforementioned  warning signs for the developmental psychology. One of the habits that has become under attack recently pertains flexibility in data collection and analysis, as arguably this flexibility exacerbates the incidence of false positives CITE-FALSEPOSPSYC. And yet as developmentalists we often feel the need to "tweak" our paradigms, and explore new ones, in the constant search for more reliable and functional methods. As a result, multiple ways to tap into the same phenomenon have been developed, but it is an open question to what extent these lead to comparable results. Moreover, although current discussions appear to discourage flexibility, we believe a strong argument can be made for continuous exploration and evaluation of alternative methods, given that another responsibility we have as scientists is to gather data of the highest quality and interpretability. In that sense, sticking to old methods for the sake of comparability would be uneconomical.

Such a description would  lead one to believe that developmental psychology, particularly that focusing early childhood, should be particularly liable to the issues that have been identified and discussed in other subfields. In the present project, we seek to provide a more informed overview of a subset of questions that we have identified as key issues. 

## Key issues

### Sample size and statistical power 

***General question: Leave here to move to introduction?***

Underpowered studies, that is studies with a low probability to detect an effect given it is present in the population, pose a problem for branches of developmental studies that interpret both significant and nonsignificant findings; for example when tracking the emergence of an ability as children mature or when examining the boundary conditions of an ability. This practice is problematic for two reasons: On one hand, the null hypothesis, for example that two groups do not differ, is not being tested, so it cannot be adopted based on a high p-value. Instead, p-values can only support rejections of the null hypothesis with a certainty that the data at hand are incompatible with it below a pre-set threshold. On the other hand, even in the most rigorous study design and execution, null results will occur ever so often; for example in a study with 80% power (a number typically deemed sufficient), every fifth result will not reflect that there is a true effect present in the population. Disentangling whether a non-significant finding indicates the absence of a skill, random measurement noise, or the lack of experimental power to detect this skill reliably and with statistical support is impossible based on p-values.

A second problem emerges when underpowered studies yield significant outcomes, as the effects reported in such cases will be over-estimating the true effect. This makes appropriate planning for future research which aims to build on this report more difficult, as sample sizes will be too small, leading to null-results which do not speak to the phenomenon under investigation. This poses a serious hindrance for work building on seminal studies, including replications across languages and extensions. However, aggregating over such null-results using a graded estimate, i.e. a standardized effect size, can reveal whether a phenomenon is present in the population and correct for the initial over-estimation. In short, even a true positive result is insufficient in the quest for the truth when it is underpowered. 

### Procedural variability
***AC - seeing the whole paper together makes me see that our topic choices have been confounding a number of issues. If we want to talk about this, then the intro needs to be about more than biases; see an attempt above***
Experiment planning goes beyond deciding how many participants to test. Often there is more than one way to measure a specific construct available, even though the number of paradigms available in developmental research when testing infants and children, is somewhat limited by a number of factors, such as time. Consider for example a measurement of preference, such as when trying to establish that infants distinguish infant-directed from adult-directed speech and in fact prefer the former. This preference can be measured in a number of ways, as it is something children bring to the lab. In the meta-analysis on IDS preference there are `r length(unique(all_data[all_data$short_name=="idspref",]$method))` different methods, all aiming to pick up the very same phenomenon, and this specific line of investigation is no exception, as four datasets of the `r length(unique(all_data$dataset))` included datasets contain three or more methods. 


### P-hacking and publication biases


### Working towards cumulativity



# Methods
## Source data: MetaLab
***AC- Not all information provided below seems relevant to the goal of documenting good/bad practices (e.g. % female participants) from the point of view of the reader. Shouldn't the goal of this section be to provide the minimal relevant info for the reader to understand how we can answer our questions?***

We make use of a collection of meta-analyses on child development so as to be able to comment on general trends in the field. Meta-analyses are built on a collection of standardized effect sizes on a single, well-defined phenomenon. By accumulating effect sizes and weighting them by their reliability, it is possible to compute an estimate of the population effect, as well as its structured variance. By harnessing data from hundreds of studies, we can quantify patterns important for experimental practices. Furthermore, combining multiple meta-analysis, each centered on a different research question, allows ut to assess whether current practices differ across different topics. Based on such an assessment, we can provide recommendations  that are likely to be relevant across the board. 



Given that all `r length(MA_descriptives$dataset)` meta-analyses focus on language acquisition in early childhood, our suggestions will be most relevant to this subfield. We present our methods and results to the general developmental psychology population to encourage other developmentalists to build similar meta-meta-analyses, allowing them to explore the state of their own subfields.

The analyses in this paper are based on MetaLab, an online collection of meta-analyses on early language development. Currently, MetaLab contains `r length(MA_descriptives$dataset)` meta-analyses, but it is open to submissions and updates. The present analyses thus are a snapshot; through dynamic reports on the website, and by downloading the freely available data, it is continuously possible to obtain the most recent data. 

In MetaLab, parts of each meta-analysis are standardized to allow for the computation of common effect size estimates and for analyses that span across different phenomena. These standardized variables include study descriptors (such as citation and peer review status), participant characteristics (including mean age and age range, percent female participants), methodological information (for example what dependent variable was measured), and information necessary to compute effect sizes (number of participants, if available means and standard deviations of the dependent measure, otherwise test statistics, such as t-values or F scores). 

***Question: More on sampling datasets?***


MetaLab contains datasets that address phenomena ranging from infant-directed speech preference to mutual exclusivity, sampled  opportunistically based on data collected with involvement of (some) authors of this paper (n=`r length(MA_descriptives$dataset)-2` datasets) or they were extracted from published meta-analyses related to language development (n=2, i.e. [Colonessi, @DunstHamby]). In the former case, we attempted to document as much detail as possible for each entered experiment (note that a paper can contain many experiments). 
Detailed descriptions of all phenomena covered by MetaLab, including which papers and other sources have been considered, can be found on the companion website at metalab.stanford.edu and in the supporting information. Further, a throughout investigation into data quality within MetaLab, including publication biases, and a meta-meta-analyses have been conducted based on the same data [@SynthesisPaper]. ***<-- AC: I think the question of publication biases should ABSOLUTELY be targeted in this paper, we need to explain what all this means for our field!!!! ***
***AC- recommend removal of the following; to me it sounds like we selected only replicable findings, which would therefore not constitute a representative or random sample of the field as a whole (i.e., we would use a biased sample)*** It turns out that all databases have evidential value and can thus be utilized for further investigation. 



***AC- now this just sounds like unsupported bashing, too bad you had to remove the justification. Incidentally, I don't think it's relevant in this paper that is on practices, and not on main effects*** Consequently, meta-analyses do not rely on one (possibly inaccurate) study outcome, be it significant or not. Despite their overall utility, meta-analyses are not frequently conducted in most branches of developmental psychology. Instead, narrative summaries are the dominant tool to build theories, and that single studies are cited as evidence for the presence or absence of an ability instead of meta-analyses. 


## Statistical approach

As dependent measure, we report Cohen's *d*, a standardized effect size based on comparing sample means and their variance. This effect size was calculated when possible from means and standard deviations across designs with the appropriate formula. When these data were not available, we used test statistics, more precisely t-values or F scores of the test assessing the main hypothesis. We also computed effect size variance, which allows to weigh each effect size when aggregating across studies. The variance is mainly determined by the number of participants; intuitively effect sizes based on larger samples will be weighted higher. Note that for research designs testing participants in two conditions that need to be compared (for example exposing the same infants to infant- and adult-directed speech), correlations between those two measures are needed to estimate the effect size variance. This measure is usually not reported, despite being necessary for effect size calculation. Some correlations could be obtained through direct contact with the original authors (see e.g., [@InWordDB] for details), for others we estimated this factor based on the information in our database. 

To aggregate effect sizes within a phenomenon, we used a multilevel approach, which takes into account not only the effect sizes and their variance of single studies, but also that effect sizes from the same paper will be based on more similar studies than effect sizes from different papers [@konstantopoulos2011], implemented in the metafor package [@metafor] of R [@R]. We excluded as outliers effect sizes that were more than three standard deviations away from the median effect size within each dataset, thus accounting for the difference in median effect size.

# Results and discussion

## Average sample size, effect size, and power per phenomenon

The table below provides summary information for each meta-analysis in MetaLab regarding a number of factors, including the number of single effect sizes and that of papers contributing to a given dataset, in the order of typical age in a specific dataset. 
The typical sample size as well as the minimum and maximum (allowing to estimate the range in our data) is noted as well. Based on the meta-analytical effect size and the average number of participants, we calculated typical power (using the pwr package [@pwr]). Note that recommendations are for this value to be above 80%, which refers to a likelihood that 4 out of 5 studies show a significant outcome for an effect truly present in the population [CITE]. 

*** Question: Is the next bit over-interpreting our data? ***

Phenomena in MetaLab differ in the age groups typically tested and the age range covered, with the mean age ranging between 4.5 months (infant directed speech preference) and 2.5 years (mutual exclusivity). One might expect a relationship between effect sizes and infant age both for theoretical and practical reasons. On one hand, younger infants might show a smaller effect in general because they are not yet as proficient in their native language, having had less experience, and because they are a more immature in terms of their information processing abilities [CITE]. On the practical side, methods -- a topic we will investigate in depth in the next section -- might be more noisy for younger infants and they could be a more difficult population to recruit. 

While there is no strict linear relationship between infant age and sample size, effect size, and the derived power, we observe a difference between studies typically testing infants younger than one year and those testing older infants. First, sample sizes are much lower for younger infants, which do usually not test more than 20 infants (although all datasets contain studies with larger samples). This is not the case for older children. The only exception is the dataset addressing mutual exclusivity, which habitually tests around 16 children. This low number of participants, however, is at least somewhat off-set by a comparatively large effect size. Additionally, the number of participants tested within each dataset ranges a great deal, between single-digit numbers and in some cases more the tenfold amount. This might indicate that researchers are mostly limited by their resources and participant availability in planning their studies. 

Turning to effect size, we see a similar split by age group in our data. Younger infants show both a greater range and include lower effect sizes which fall into the classical range of small effects (Cohen's *d* below .5), which is not the case for older children. Power is directly related to sample size and effect size, so it is not surprising that typical power is greater for older children. Interestingly, however, there seems to be little to no relationship between effect sizes and number of participants typically tested. For phenomena with large effects, this means that studies are very high-powered (see gaze following, online word recognition, as two examples).
For younger children, because sample sizes and effect sizes are both small, power is habitually very low, and the only dataset which typically achieves appropriate power near 80% is non-native vowel discrimination. For older children, power is solely caused by lower effect sizes. The lack of a relationship between overall meta-analytic power and sample size might indicate that researchers' experiment planning is not impacted by the phenomenon under investigation. Studies might instead be designed and conducted with pragmatic considerations in mind, such as participant availability. 

Besides this very general point, we refrain here from strong conclusions based on the above-discussed observations, since the present dataset is not exhaustive and topics typically investigated in younger children are over-represented. However, we sampled in an opportunistic and thus to some degree random fashion and the phenomena covered span very different aspects of language acquisition and linguistic processing.

```{r Descriptive Information, echo = FALSE}
kable(arrange(MA_summary, age_dataset), col.names = c("Topic", "Mean Age (Months)", "Median Sample Size", "Min. Sample Size", "Max. Sample Size", "# Effect Sizes", "# Papers", "d", "SE", "Avg Power"), digits = 2, caption = "Descriptions of meta-analyses currently in MetaLab.")
```



***TODO: Visualize power***

### Comparing meta-analytic effect size and oldest paper to estimate power

As Table 1 shows, experimenters are habitually not including a sufficient number of participants to observe a given effect, assuming the meta-analytic estimate is accurate. It might, however, be possible, that power has been determined based on a seminal paper to be replicated and/or built on. Initial reports tend to overestimate effect sizes [@jennions2002relationships], possibly explaining the lack of power in some datasets. We extracted for each dataset the oldest paper and therein the largest reported effect size and re-calculated power accordingly, using again the median sample size. The results are shown in the table below. It turns out that in some cases, such as native and non-native vowel discrimination, sample size choices match well with the oldest report. The difference in power, noted in the last column, can be substantial, with native vowel discrimination and phonotactic learning being the two most salient examples. Here, sample sizes match well with the oldest report and studies would be appropriately powered if this estimate were representative of the true effect. 

```{r Compute Meta-analytic d for All Papers, echo = FALSE}
kable(arrange(d_comparison_summary, old_power), col.names = c("Meta-analysis (MA)", "Oldest Paper", "Oldest d", "Median Sample Size", "Power", "Improvement"), digits = 2, caption = "For each meta-analysis, largest d from oldest paper and power, along with the difference between power based on meta-analytic and oldest d.")
```

```{r Plot of Difference of d Values, echo = FALSE, fig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap="Correlation of largest d from oldest paper and difference between oldest d and meta-analytic d."}
d_comparison_diff.plot
```

To illustrate the disparity between the oldest effect size and the meta-analytic effect, and consequently the difference in power, we plot the difference between both against the oldest effect. This difference is larger as oldest effect size increases, with an average of `r mean(d_comparison_full$diff_d)` compared with an average effect size of `r mean(abs(d_comparison_full$d))` (note that we based this on the absolute value). The plot showcases that researchers might want to be wary of large effects, as they are more likely to be non-representative of the true phenomenon compared to smaller initial effects being reported. Especially when making decisions about sample sizes, large effect might thus not be the best guide. Taking the above-mentioned mean values as example, a realistic sample size to ensure 80% power would be `r pwr.t.test(n = NULL,d = mean(abs(d_comparison_full$d)), sig.level = .05, power=.80)$n` participants, instead of `r pwr.t.test(n = NULL,d = mean(abs(d_comparison_full$largest_d)), sig.level = .05, power=.80)$n` participants suggested by the first paper. While these numbers average over research questions and methods, which all influence the specific number of participants necessary, this example showcases that experimenters should take into account as much evidence as available to be able to plan for robust and reproducible studies. 

### Power over time

The comparison of initial and meta-analytic effect size has a number of caveats, for example, as we will lay out in the next section, methods might be different between initial reports and our overall sample; the availability of methods changes over time, as new approaches are being developed and automated procedures become more common. Further, the largest effect size from a seminal paper might have been spurious, and the research community could well be aware of that. In additional, as infant research becomes more common, recruitment and obtaining funds might both become easier, thereby increasing typical sample size over the years. 
For a more continuous approach, we thus investigate power (which is determined by effect size and sample size) as follows. We first generate a meta-analytic model for each dataset that takes into account infant age and method and then derive the respective to be expected effect size base on those data for each entry in this dataset. Power is then estimated based on the sample size actually tested. 



***TO DO: Add***


```{r Plot of power over time, echo = FALSE, fig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap="To BE UPDATED: Summary plot of power across years and meta-analyses"}
power_year.plot
```


## Procedure comparison

In this section we address how methods might be chosen from two angles. We first take a pragmatic, resource-oriented approach and compare methods with respect to their dropout rate. Then we compare how effect size across phenomena is affected by method choice. 

### Drop-out rates across methods and age

Choosing a robust method can help increase the power of studies, such that more precise measurements lead to larger effects and thus require fewer participants to be tested. However, the number of participants relates to the final sample and not how many infants had to be invited into the lab. We thus first quantify whether methods differ in their typical drop-out rate, as the available participant pool might inform method choice. To this end we consider all methods across datasets in MetaLab which have more than 10 associated effect sizes and for which information on the number of dropouts was reported; this information is not always available in the published report, and in the case of the two meta-analyses we added based on published reports, the information was not added. Therefore, the following analyses only cover `r length(unique(method_exclude_data$method))` methods and `r length(method_exclude_data$method)` data points.

The results of the linear mixed effect model predicting dropout rate by method and mean participant age (while accounting for the different effects being tested) are summarized in the table below. The results show that, taking central fixation as baseline, conditioned headturn and stimulus alternation have significantly more drop-outs. Figure XXX underlines this observation, and illustrates the relationship of drop-out rate with age. Overall, stimulus alternation leads to the highest drop-out rates, which lies at around 50% across all age groups. While age is not significantly impacting drop-out rates, it interacts with the different methods. We observe an increase in drop-out rates, which is most prominent in conditioned headturn (a significant interaction) and headturn preference procedure (where the interaction approaches significance).  

Interestingly, the methods with lower drop-out rates, namely central fixation and headturn preference procedure, are among the most frequent ones in MetaLab and certainly more frequent than those with higher drop-out rates, indicating that drop-out rate might inform researchers' choices. Being able to retain more participants as a factor in method choice points to the mentioned limitations regarding the participant pool we mentioned before, as more participants will have to be tested to arrive at the same sample size. 

*** Question: method by total participants run (aka ressource-intensity)?***


```{r Method vs excluded, echo = FALSE}
kable(coef(method_exclude.m_sum), caption = "Method vs Dropout", digits = 2)
```

```{r Plot of Effect of Method on Dropout Rate, echo = FALSE, ig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "Percent dropout as explained by different methods and mean age of participants."}
method_exclude.plot
```

### The effect of method choice on effect sizes (and thus power)

Methods which retain a lot of participants might either be more suitable to test infants, decreasing noise as most participants are on task, or less selective, thus increasing noise as participants who for example are fussy are more likely to enter the data pool. We operationalize precision as the size of the effect measured. Some datasets contain only one method, making it thus difficult to disentangle the effect size of a phenomenon with the change of effect size introduced by different methods. To avoid this confound, we limited this investigation to the `r length(unique(method_data$dataset))` datasets that contain three or more different methods. We further only investigate those methods that have at least 10 effect sizes in our overall dataset. Thus, the present analyses are limited to `r nrow(method_data)` observations. 

```{r Effect of Method, echo = FALSE}
kable(coef(method.rma_sum), caption = "Effect of d by method with central fixation as baseline method.", digits = 3)
```

We built a meta-analytic model with the effect size measure Cohen's *d* as the dependent variable, method and mean age centered as independent variables. The model also includes the variance of *d* for sampling variance, and paper within meta-analysis as a random effect (because we assume that within a paper experiments and thus effect sizes will be more similar to each other than across papers). Since the model compares one method as the baseline to all other methods, a baseline method had to be chosen. "Central fixation" was included as the baseline method, as it appears most frequently in the `r length(unique(method_data$dataset))` datasets included in this analysis (`r sum(method_data$method=="central fixation")` times out of `r nrow(method_data)` total entries of the selected meta-analyses). 

```{r Plot of Effect of Method, echo = FALSE, ig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "Effect size as explained by different methods and mean age of participants."}
method.plot

```

The model results in Table XXX show that compared to central fixation only conditioned headturn yields reliably higher effect sizes, all other methods do not statistically differ from this baseline. When factoring in age, no interaction reaches significance, while this factor on its own is marginally below the significance threshold, indicating that as infants mature effect sizes increase across methods -- an observation consistent with the view that infants and toddlers become more proficient language users and are increasingly able to react appropriately in the lab. 

Comparing our analyses (Table XXX) and Figure YY in this section with those in the previous section, it seems that high drop-out rates might be offset by high effect sizes in the case of conditioned headturn. While drop-out rates are around 40-50%, effect sizes are above 1. Only high-amplitude sucking seems to generate even higher effect sizes, but for this method we did not have enough information on drop-out rates available, so we cannot examine the relationship between the two. Further, due to the few data points available (13 associated effect sizes) the difference between high-amplitude sucking and central fixation was not significant. Stimulus alternation does not fall into this pattern of high drop-out rates being correlated with high effect sizes, as the observed outcomes are in the range typical for meta-analyses in our dataset. 

There is an important caveat to this interpretation that some methods, specifically conditioned headturn, which have higher dropout rates are better at generating high effect sizes due to decreased noise (e.g., by excluding infants that are not on task). Studies with fewer participants (thanks to higher drop-out rates) might simply be underpowered, and thus any significant finding is likely to over-estimate the effect. Due to publication biases, we might not have access to all null results using the same method, and thus the overestimation is directly reflected in our effect size estimate. We take this caveat into account and compare 

## P-hacking and publication biases


## Working towards cumulativity
***REMOVED: Data availability. Awaiting coding for those MAs where it is tracable, will be incorporated in text***

# Summary and suggestions for the future
We set out to discuss potentially problematic practices. We see X Y and Z. We are thus adding to a recently emerging literature that critically examines long-held standards and practices in order to make the whole field more reliable and robust [@Mills-Smith2015, @Csibra]. 

## Concrete recommendations for developmental psychologists
***AC: this section really shows the change in direction of the paper; it's all about metalab and MA, whereas I thought you wanted to write about practices in the field... Do we change the introduction to be specific about metalab (and then potentially go to a different journal), or change this section (to address the topics set out in the intro)?***


- Reporting: Make it easier to conduct MAs (include correlations, report means and SDs)
- Reporting: Share failures for a more accurate measure, either in MetaLab or on emerging platforms (both allow for anonymzation if you do not want to associate your name for fear of repercussions)

- Study planning: prospective power
- MiniMA if not available in MetaLab: share then



To support the improvement current practices, we propose to make meta-analyses available in the form of ready-to-use online tools, dynamic reports, and as raw data. These different levels allow researchers with varying interest and expertise interests to make the best use of the extant record on infant language development, including study planning by choosing robust methods and appropriate sample sizes. There are additional advantages for interpreting single results and for theory building that emerge from our dataset. On one hand, researchers can easily check whether their study result falls within the expected range of outcomes for their research question -- indicating whether or not a potential moderator influenced the result. On the other hand, aggregating over many data points allows for the tracing of emerging abilities over time, quantifying their growth, and identifying possible trajectories and dependencies across phenomena (for a demonstration see [@Lewis]). Finally, by making our data and source code open, we also invite contributions and can update our data, be it by adding new results, filedrawer studies, or new datasets. Our implementation of this proposal is freely online available at metalab.stanford.edu.

## Facilitating the creation and use of meta-analyses

We have shown that power varies greatly across phenomena and that method choice is important. It turns out, however, that researchers do not choose the most robust methods. This might to be due to a lack of consideration of meta-analytic effect size estimates. One of the reasons for this is a lack of information on and experience in how to interpret effect size estimates and use them for study planning [cite infancy paper]. 
Meta-analyses on infant language development are also rare, as showcased by the fact that the present dataset relied on the authors' involvement, and only two out of 12 meta-analyses used could be extracted from the extant work, and an extensive search in the present literature did not yield additional candidates (excluding clinical contexts). Conducting a meta-analysis is a laborious process, particularly according to common practice where only a few people do the work, with little support tools and educational materials available. Incentives for creating meta-analyses are low, as public recognition is tied to a single publication. The benefits of meta-analyses for the field, for instance the possibility to conduct power analyses, are often neither evident nor accessible to individual researchers, as the data are not shared and traditional meta-analyses remain static after publication, aging quickly as new results emerge.

A second possible reason lies in the availability of data allowing for the conclusions we were able to draw here, be it in the form of reported effect sizes within paper or as ready-to-use dataset. As noted elsewhere, researchers do not report effect sizes [@Panneton], despite long-standing recommendations to move beyond the persistent focus on p-values (eg. APA Recommendations). 
A final impediment to meta-analyses in developmental science are current reporting standards, which make it difficult and at times even impossible to compute effect sizes from the published literature. 

***TODO: ADD data from systematic MAs on author contact***








# References

# TEMPORARILY REMOVED MATERIAL
Replicability is a core concept in this recent crisis, as exactly this property of scientific studies and whole fields has come under fire. Replicating a study means conducting a conceptually similar experiment with new stimuli and in a slightly different population but following the same procedure and analyses (based on the published report) with the same outcome as reported (allowing for a margin of error). Being able to (repeatedly) successfully replicate a study can be taken as an indicator that the phenomenon under investigation is true and theories can be built on it. In addition, testing different populations and using different, yet comparable stimuli, implies generalizability across supposedly irrelevant dimensions. 

Science is a cumulative effort, meaning that a single published report is not sufficient to establish whether an effect is truly present in the population, because misleading reports speaking for or against the existence of a phenomenon might be caused by a number of issues. Next to spurious findings (which can occur even when following best practices due to sampling error alone), a number of habits in psychological research might result in outcomes not reflecting whether or not a phenomenon is truly present in the population. 
These habits include confining null-results to the filedrawer and running studies with too few participants to reliably detect the effect.




In the next section we discuss in detail the possible causes and unwanted negative consequences of underpowered studies, while examining the status quo in developmental research. 


This flexibility is sometimes boosted by the fact that some of our methods are rather indirectly related to the underlying process we are trying to measure. For instance, when working with infants, one often needs to rely on visual attention as the sole behavioral index, and thus our paradigms are built on designing conditions that provoke measurably different levels of visual attention.-- @CB could you please explain? I thought I understood, but not sure I do... In what ways is CF more indirect than say lexical decision judgments? If you're interested in phonotactics, your measure is indirect in both cases. Perhaps remove this para?


## Introducing MetaLab


MetaLab is built on two core principles: lowering hurdles to foster the implementation of practices which are rapidly becoming standard procedure, not only in other branches of psychology (such as effect size estimation and power calculation), and crowdsourcing to decrease the workload of single researchers. MetaLab is based on the recently proposed concept of community-augmented meta-analyses (CAMAs; [@Tsuji2014]), which combine meta-analyses and open repositories. The advantages of this union are that meta-analyses are shared and get updated continuously, so they can capture the most recent state of the literature and are open to contributions of unpublished results. 

MetaLab expands on CAMAs by providing an infrastructure for a range of uses. It is possible to gain an overview of the literature, get insights into specific topics through dynamically rendered reports, conduct power calculations, and contribute not only single recent or unpublished datasets, but whole meta-analyses that can then be opened to contributions and analysis. All meta-analyses share a core of 20 variables which not only allow for the computation of effect sizes across vastly different studies, but also provide the basis for further comparisons. These comparisons are both of practical and theoretical importance, for example can we compare which method is more robust and suitable for various ages. Researchers then can both better compare existent findings and plan their own research to be more effective. This becomes possible by focusing on a high-level but specific and constrained topic, in the case of MetaLab this is early language development and adjacent phenomena. 

MetaLab also poses several advantages compared to existing software for meta-analyses. First, adding meta-analyses is supported not only by sharing standardized formats but also by offering guidance in identifying the correct data to enter, based on the extant data and examples from the developmental psychology literature. Further, novice users can easily engage with the platform to estimate, for example, effect sizes, or decide on sample sizes with a simple interactive tool. Secondly, since all data and scripts are freely and openly available, it is possible to inspect and if needed correct all computations. Errors are thus removed much more swiftly than would be the case for (commercial) software, without losing the benefit of a stable platform. By changing current practices, we aim to increase the reliability of developmental findings and thus the credibility of the field. 




------


## With these tools, what do we have to do?

[Tutorial section]

On the individual level:

-	How to determine participants: Power calculator, typical N in the field
-	How to run the best possible study: Make design choices to have a more robust measure (smaller sample and more power)
-	How do I report my data? Best reporting practices (include correlations for within, always report means and SD); and possibly best visualization practices

Further individual benefits:

-	Don't despair when a null result occurs, you can still help the community with it 
-	For replication / training purposes possible to compare ES and select robust ones

On the general level:

-	Evidence becomes more reliable
-	New evidence can be integrated with previous work directly without much effort 
-	Complete, unbiased overview of a research literature 
    -	Identify unexplained variance
    -	Where are gaps? 
    -	Which moderators (do not) affect outcomes 
        -	Examples from published MAs: 
            -	InWordDB lack of age effect (predicted and strongly assumed in the field)
            -	InPhonDB confirmation of diverging effects for native / nonnative, with a quantitative timeline
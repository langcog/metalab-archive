---
title             : "Assessing experimental practices in language acquisition research through meta-analyses"
shorttitle        : "Practices in language acquisition research"

author: 
  - name          : "Christina Bergmann"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Ecole Normale Su{\'e}rieure, Laboratoire de Sciences Cognitives et Psycholinguistique, 29, rue d'Ulm, 75005 Paris, France."
    email         : "chbergma@gmail.com"
  - name          : "Sho Tsuji"
    affiliation   : "2"
  - name          : "Page E. Piccinini"
    affiliation   : "3"
  - name          : "Molly L. Lewis"
    affiliation   : "4"
  - name          : "Mika Braginsky"
    affiliation   : "5"
  - name          : "Michael C. Frank"
    affiliation   : "6"
  - name          : "Alejandrina Cristia"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Ecole Normale Sup{\'e}rieure, PSL Research University, D{\'e}partement d'Etudes Cognitives, Laboratoire de Sciences Cognitives et Psycholinguistique (ENS, EHESS, CNRS)"
  - id            : "2"
    institution   : "University of Pennsylvania, Department of Psychology"
  - id            : "3"
    institution   : "Ecole Normale Sup{\'e}rieure, PSL Research University, D{\'e}partement d'Etudes Cognitives, Neuropsychologie Interventionnelle (ENS, EHESS, CNRS)"
  - id            : "4"
    institution   : "University of Chicago, Computation Institute/University of Wisconsin-Madison, Department of Psychology"
  - id            : "5"
    institution   : "Massachusetts Institute of Technology, Department of Brain and Cognitive Sciences"
  - id            : "6"
    institution   : "Stanford University, Department of Psychology, Language and Cognition Lab"

author_note: >
  

abstract: >
  Replicability is a critical feature of scientific research, and sufficiently powered studies are a key factor. Across a collection of meta-analyses on language development observed power for experiments was calculated. With a median effect size Cohen's *d* = .57, and a typical sample size of 17 participants, power is at 60% (ranging between 6% and 99% across meta-analyses). This suggests that researchers do not habitually consider effect sizes in their experiment planning. Further analyses reveal that seminal publications typically overestimate effect sizes, and methods vary in the resulting effect size. Further, this literature overall shows only limited evidence of publication bias and questionable research practices. Recommendations for experimental planning and the use of meta-analysis in developmental research conclude the paper. 

keywords          : "replicability, reproducibility, meta-analysis, language acquisition, power"
wordcount         : "X"

bibliography      : ["metalab_education.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---

```{r Preamble, echo = FALSE, warning = FALSE, message = FALSE}
# These packages are need in the here for some in document commands
library(knitr)
library(dplyr)
library(pwr)

knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE, fig.pos = "T")
require("papaja")
#apa_prepare_doc() # prepare document for rendering

options(scipen=1, digits=2)

#source("save_analyses.R") # if changes have been made to any analyses run this, comment below
load("educationpaper_environment.RData") # if no changes to analyses run this, comment above
```


Empirical research is built on a never-ending conversation between theory and data, between expectations and observations. Theories lead to new experimental questions and new data in turn help us refine our theories. This process relies crucially on access to reliable empirical data. Unfortunately, investigators of the scientific process have noted that the assessment of the value of empirical data points can be biased by concerns about publishability [@nosek2012scientific2], which in turn often depends on the observation of statistically significant and theoretically-surprising outcomes [@Sterling1995]. If researchers aim for publishability, this is likely to lead to practices that undermine the quality and reliability of their data. It has therefore been suggested that theories should rely on replicable findings. Replicability is crucial in experimental sciences, particularly for developmental research: Theories should be based on robust findings and their boundary conditions have to be explored with sufficiently powered studies to avoid an excess of false negatives. Further, translating findings on child development into practice requires a solid knowledge base.

According to some, inappropriate research and reporting practices may be to blame for the surprisingly high proportion of non-replicable findings in psychology [@Simmons2011]. Simulating the scientific process, @Ioannidis2005 speculated that most empirical research findings may even be false. The proportion of false findings in these simulations was dependent on several features, including the underlying effect size of a particular phenomenon, the typical sample sizes used by researchers, and the degree of flexibility in data collection and analysis. All of these factors are highly relevant to developmental research. 


In the current paper, we survey and quantify methodological practices in developmental research using meta-analytic tools, focusing on language development. We take a different approach from the typical meta-analysis by aggregating over multiple datasets. Using a collection of standardized meta-analyses, we focus on key experimental design choices: sample size (and the ensuing statistical power) and experimental method. In doing so, we provide what is, to our knowledge, the first assessment of typical practices of developmental research. Based on our findings and experiences with building meta-analyses and using meta-analytic tools, we end this paper with suggestions for change. 

The data we analyze are part of MetaLab, a database of meta-analyses of language acquisition that, covers a variety of methods (`r length(unique(all_data$method))` in total) and participant ages, from newborns to `r round(max(all_data$mean_age_months)/12, digits=1)`-year-olds. Since our work is comprised of open data and scripts, accompanied by extensive educational materials, completely open data and scripts, and we build on open source software [specifically R, @R],, our approach can easily be extended to other domains of child development research and we strongly encourage fellow researchers to build similar collections of meta-analyses describing and quantifying phenomena in their sub-domain of developmental research. 



## Key concerns for robust research in developmental science

In this section we review potential hindrances to developmental research being robust and reproducible, and briefly describe how we will assess the status quo. Note that all these descriptions are by necessity brief, for extended discussions we provide references to suitable readings. 


### Statistical power 

Power refers to the probability of detecting an effect and correctly rejecting the null hypothesis if an effect is indeed present in a population; power is therefore dependent on the underlying effect size and the sample size. Of course, low power is problematic in terms of increased chances of type-II errors (i.e., failure to find a significant result when there is an underlying effect). It has become increasingly clear that low power is also problematic in the case of type-I errors, or false positives, as the effects reported in such cases will be over-estimating the true effect [see also @Ioannidis2005; @Simmons2011; @button2013power]. This makes appropriate planning for future research more difficult, as sample sizes will be too small, leading to null results due to insensitive research designs rather than the absence of the underlying effect. This poses a serious hindrance for work building on seminal studies, including replications and extensions. 

Underpowered studies pose an additional and very serious problem for developmental researchers that interpret significant findings as indicating that a skill is "present" and non-significant findings as a sign that it is "absent". In fact, even in the most rigorous study design and execution, null results will occur regularly; consider a series of studies with 80% power (a number typically deemed sufficient), where every fifth result will be a false negative, that means it will not reflect that there is a true effect present in the population. This observation was recently demonstrated by @oakes2017power by using data from a high-powered looking time study. 

To investigate the status quo, we first compute typical power per phenomenon, based on meta-analytic effect sizes and typical sample size. We explore which effect sizes would be detectable with the sample sizes present in our datasets. We additionally investigate how researchers might determine sample sizes using a different heuristic, following the first paper on their phenomenon of interest.


### Method choice

Improving procedures in developmental research can be considered both an economical and ethical necessity, because the population is difficult to recruit and test. For this reason, developmentalists often "tweak" paradigms and develop new ones to increase reliability and robustness, all with the aim of obtaining a clearer signal. Especially given the time constraints, we aim to collect a maximum of data in the short time span infants and children are willing to participate in a study. Emerging technologies, such as eye-tracking and tablets, have consequently been eagerly adopted [@tablet]. As a result, multiple ways to tap into the same phenomenon exist; consider for example the fact that both headturn-based paradigms and offline as well as online measurements of eye movements are frequently being employed to measure infant-directed speech preference [@IDS_MA; @Manybabies1]. 

It remains an open question to what extent these different methods lead to comparable results. It is possible that some are more robust, but it is difficult to extract such information based on single studies that use different materials and test various age groups [but see the large-scale experimental approach by @Manybabies1]. Aggregating over experimental results via meta-analytic tools, in contrast, allows us to extract general patterns of higher or lower noise by comparison of effect sizes, which are directly affected by the variance of the measurement. 

We assess how much the different methods used in the studies within the present collection of meta-analyses vary in the resulting effect size. Further, taking possible resource limitations into account, we consider drop-out rates as a potential measure of interest and discuss whether higher exclusion rates coincide with more precise measures, yielding higher effect sizes. 


### Questionable research practices

Undisclosed flexibility during data collection and analysis is a problem independent of the availability of various methods to conduct developmental studies. To begin with, using flexible stopping rules, where the decision to stop or continue testing depends on the result of  a statistical test, increases the likelihood to obtain a "significant" outcome well beyond the expected 5%. 

As for analytic flexibility, researchers might conduct multiple significance tests with several more or less related dependent variables without correcting for this practice. In developmental research, this encompasses transforming the same measured data into multiple dependent variables (such as mean scores, difference scores, percentages, and so on) as well as selectively excluding trials and re-testing the new data for statistical significance. Next, multiple conditions that selectively can be dropped from the final report increase the number of significance tests. Finally, it is problematic to post hoc introduce covariates, most prominently gender, and test for an interaction with the main effect, and solely report those outcomes as confirmatory hypothesis test. Combining two or more of these strategies again increase the number of significant results that occur by chance even if there is no effect present in the population. All these practices might seem innocuous and geared towards "bringing out" an effect  the researcher believes is real, yet they can inflate the number of significant *p* values, effectively rendering *p* values and the notion of statistical significance meaningless [@Ioannidis2005; @Simmons2011]. 

It is typically not possible to assess whether undisclosed flexibility during data collection or analysis led to a false positive in a given report. However, we can measure "symptoms" of such practices in a whole literature. We focus in this paper on flexibility in stopping data collection, a practice that was found to be present, but not predominant in infancy research in a recent anonymous survey [@eason2017survey]. Since our data span over `r min(all_data$year)-max(all_data$year)` years (publications date range from `r min(all_data$year)` to `r max(all_data$year)`), it might be the case that recent discussions of best practices have improved lab practices, but older reports could still have applied this seemingly innocuous practice of adding participants to "bring out" the effect of interest. 

# Methods


All scripts used in this paper and information how to obtain the source data from MetaLab are shared on Open Science Framework at https://osf.io/uhv3d/?view_only=5d81b03a2fa64697b15ced2627036292. 


## Data

The data presented and analyzed in the present paper are part of a standardized collection of meta-analyses (MetaLab), and are freely available via the companion website http://metalab.stanford.edu. Currently, MetaLab contains `r length(MA_descriptives$dataset)` meta-analyses, or datasets, where core parts of each meta-analysis are standardized to allow for the computation of common effect size estimates and for analyses that span across different phenomena. These standardized variables include study descriptors (such as citation and peer review status), participant characteristics (including mean age, native language), methodological information (for example what dependent variable was measured), and information necessary to compute effect sizes (number of participants, if available means and standard deviations of the dependent measure, otherwise test statistics of the key hypothesis test, such as *t* values or *F* scores). This way, the analyses presented in this paper become possible.

MetaLab contains datasets that address phenomena ranging from infant-directed speech preference to mutual exclusivity, sampled opportunistically. Meta-analyses are either based on data made available on MetaLab by their original authors (n=`r length(MA_descriptives$dataset)-2` datasets) or they were extracted from previously published meta-analyses related to language development [n=2, @PointingMA; @IDS_MA]. In the former case, the original authors attempted to document as much detail as possible for each entered experiment (note that a paper can contain many experiments, as shown in Table 1), as recommended for reproducible and dynamic meta-analyses [@Tsuji2014]. 
Detailed descriptions of all phenomena covered by MetaLab, including which papers and other sources have been considered, can be found at http://metalab.stanford.edu.

## Statistical approach

As dependent measure, we report Cohen's *d*, a standardized effect size based on comparing sample means and their variance. Effect size was calculated when possible from means and standard deviations across designs with the appropriate formulae [@Lipsey; @morris2002combining; @dunlap1996meta; @metafor]. When these data were not available, we used test statistics, more precisely *t* values or *F* scores of the test assessing the main hypothesis. We also computed effect size variance, which allows to weight each effect size when aggregating across studies. The variance is mainly determined by the number of participants; intuitively effect sizes based on larger samples will be assigned more weight. Note that for research designs testing the same participants in two conditions (for example measuring reactions of the same infants to infant- and adult-directed speech), correlations between those two measures are needed to estimate the effect size variance. This measure is usually not reported, despite being necessary for effect size calculation. Some correlations could be obtained through direct contact with the original authors [see e.g., @InWordDB for details], the remaining ones were imputed. We report details of effect size calculation in the supplementary materials and make available all scripts used in the present paper.  Excluded as outliers were effect sizes more than three standard deviations away from the median effect size within each dataset, thus accounting for the difference in median effect size across phenomena.

### Meta-analytic model

Meta-analytic effect sizes were estimated using random-effect models where effect sizes were weighted by their inverse variance. We further used a multilevel approach, which takes into account not only the effect sizes and variance of single studies, but also that effect sizes from the same paper will be based on more similar studies than effect sizes from different papers [@konstantopoulos2011]. We relied on the implementation in the metafor package [@metafor] of R [@R].

### Power calculation

We calculated typical power using the pwr package [@pwr] based on the meta-analytical effect size and the median number of participants within each phenomenon. This approach is insightful, because meta-analytic effect size estimates are (typically) more reliable than those of single studies. For targeted analyses of the power of the seminal paper, we extracted the largest effect size and used this value for power calculation, taking in both cases the median number of participants in a meta-analysis into account [for a similar approach see e.g., @button2013power].  

# Results

## Statistical power

Table 1 provides a summary of typical sample sizes and effect sizes by phenomenon.  We remind the reader that recommendations are for this value to be above 80%, which refers to a likelihood that four out of five studies show a significant outcome for an effect truly present in the population.

As could be expected, sample sizes are small across all phenomena, with the overall median in our data being `r median(all_data$n)`. Effect sizes tend to fall into ranges of small to medium effects, as defined by Cohen [@cohen]. The overall median effect size of all datasets is Cohen's *d* = `r median(MA_summary$d)`. As a result of those two factors, studies are typically severely under-powered: Assuming a paired t-test (within-participant designs are the most frequent in the present data) it is possible to detect an effect in 80% of all studies when Cohen's *d* = `r round(pwr.t.test(n=median(all_data$n), d=NULL, power = .8, type = "paired")$d,  digits = 2)`; in other words, this sample size would be appropriate when investigating a medium to large effect. When comparing two independent groups, the effect size that would be detectable with a sample size of `r median(all_data$n)` participants per group increases to Cohen's *d* = `r round(pwr.t.test(n=median(all_data$n), d=NULL, power = .8, type = "two.sample")$d,  digits = 2)`, a large effect that is rarely observed as meta-analytic effect size in the present collection of developmental meta-analyses. 

Inversely, to detect the typical effect of Cohen's *d* = `r median(MA_summary$d)`, studies would have to test `r round(pwr.t.test(n=NULL, d=median(MA_summary$d), power = .8, type = "paired")$n,  digits = 0)` participants in a paired design; `r round(pwr.t.test(n=NULL, d=median(MA_summary$d), power = .8, type = "paired")$n-median(all_data$n), digits = 0)` more than are included on average. It should be noted that this disparity between observed and necessary sample size varies greatly across phenomena, leading to drastic differences in observed power to detect the main effect at stake. While studies on phonotactic learning and word segmentation apparently typically run dramatically underpowered studies (with typical power being under 10%), experiments on gaze following and online word recognition are very highly powered (95% and 99%, respectively). 


  
  
```{r DescriptiveInformation, echo = FALSE, results='asis'}
MA_summary_table$power <- printnum(MA_summary_table$power)
apa_table(MA_summary_table, col.names = c("Meta-Analysis", "Age", "Sample Size", "N Effect Sizes", "N Papers", "Effect Size (SE)", "Power"), align = c("l", "r", "r", "r", "r", "r", "r"), caption = "Descriptions of meta-analyses. Age is reported in months, sample size is based on the median in a given meta-analysis, effect size is reported as meta-anlytic weighted median Cohen's d, and average power is computed based on meta-anlytic effect size estimate Cohen's d and median sample size.")
```


  
### The role of participant age

Participant age can be assumed to interact with effect size both for conceptual and practical reasons. Younger participants might show smaller effects in general because they are more immature in terms of their information processing abilities, and they are not yet as experienced with, and proficient in, their native language in particular. As to practical reasons, measurements might be more noisy for younger participants, as they could be a more difficult population to recruit and test. We find no linear relationship between participant age and sample size, effect size, and derived power on the level of meta-analyses. In addition, the prediction that older participants might be easier to recruit and test is not reflected in the observed sample sizes. However, the only two datasets, gaze following and online word recognition, with power over 80% typically test participants older than one year. 


### Seminal papers as basis for sample size planning

As Table 1 shows, experimenters are frequently not including a sufficient number of participants to observe a given effect -- assuming the meta-analytic estimate is accurate. It might, however, be possible, that power has been determined based on a seminal paper to be replicated and expanded. Initial reports tend to overestimate effect sizes [@jennions2002relationships], possibly explaining the lack of power in some datasets and studies. 

We extracted for each dataset the oldest paper and therein the largest reported effect size and re-calculated power accordingly, using the median sample size of a given dataset. The results are shown in Table 2. It turns out that in some cases, such as native and non-native vowel discrimination, sample size choices match well with the oldest report. The difference in power, noted in the last column, can be substantial, with native vowel discrimination and phonotactic learning being the two most salient examples. Here, sample sizes match well with the oldest report and studies would be appropriately powered if this estimate were representative of the true effect. For four datasets neither the seminal paper nor meta-analytic effect size seem to be basis for sample size decisions. 


```{r SeminalPaper, echo = FALSE, results='asis'}
#d_comparison_summary$largest_d <- printnum(d_comparison_summary$largest_d)
#d_comparison_summary$old_power <- printnum(d_comparison_summary$old_power)
#d_comparison_summary$difference <- printnum(d_comparison_summary$difference)

apa_table(arrange(d_comparison_summary, old_power), col.names = c("Meta-Analysis", "Effect Size \n Seminal Paper", "Effect Size \n Overall", "Sample Size", "Power \n Seminal Paper", "Difference \n Overall Power"), align = c("l", "r", "r", "r", "r"), caption = "For each meta-analysis, largest effect size Cohen's *d* and derived power based on the first paper, along with the difference between power based on meta-analytic and oldest effect size.")

#kable(arrange(d_comparison_summary, old_power), col.names = c("Meta-analysis (MA)", "Oldest Effect Size", "Meta-analytic Effect Size", "Sample Size", "Power based on \n first report", "Difference to\n meta-analytic power"), caption = "For each meta-analysis, largest effect size Cohen's *d* and derived power based on the first paper, along with the difference between power based on meta-analytic and oldest effect size.", digits = 2)
```


## Method choice

In most of our meta-analyses, multiple methods were used to tap into the phenomenon at stake. Choosing a robust method can help increase power, because more precise measurements lead to larger effects and thus require fewer participants to be tested. However, the number of participants relates to the final sample and not how many participants had to be invited into the lab. We thus first quantify whether methods differ in their typical drop-out rate, as economic considerations might drive method choice. To this end we consider all methods across datasets which have more than 10 associated effect sizes and for which information on the number of dropouts was reported; this information is not always reported in published papers. In the case of the two meta-analyses we added based on published reports, the information of drop-out rates was not available. Therefore, the following analyses only cover `r length(unique(method_exclude_data$method))` methods and `r length(method_exclude_data$method)` data points.



### Drop-out rates across procedures 


The results of a linear mixed effects model predicting dropout rate by method and mean participant age (while controlling for the different phenomena and associated underlying effect sizes being tested) are summarized in the table below. The results show that, taking the most frequently used method central fixation as the baseline, conditioned headturn and stimulus alternation have significantly more drop-outs, while forced choice has significantly fewer. Figure 1 underlines this observation. Overall, stimulus alternation leads to the highest drop-out rates, which lies at around 50% (see Figure 1), and forced choice to the lowest. Participant age interacts with the different methods. We observe an increase in drop-out rates, which is most prominent in conditioned headturn (a significant interaction) and headturn preference procedure (where the interaction approaches significance).  

Interestingly, the methods with lower drop-out rates, namely central fixation and headturn preference procedure, are among the most frequent ones in our data and certainly more frequent than those with higher drop-out rates. The proportion of participants that can be retained might thus indeed inform researchers' choice. This observation points to the previously mentioned limitations regarding the participant pool, as more participants will have to be tested to arrive at the same final sample size. 


```{r DropoutModel, echo = FALSE, results='asis'}
method_exclude_apa <- method_exclude.m_sum$coefficients


clean<-function(x){
  x<-gsub("relevel(, \"central fixation\")","",x,fixed=T)
  x<-gsub("intrcpt","Intercept",x,fixed=T)
  x<-gsub("(Intercept)","Intercept",x,fixed=T)
  x<-gsub("method","",x)
  x<-gsub("conditioned head-turn","CondHT",x)
  x<-gsub("forced-choice","FC",x)
  x<-gsub("head-turn preference procedure","HPP",x)
  x<-gsub("looking while listening","LwL",x)
  x<-gsub("stimulus alternation","SA",x)
  x<-gsub("ageC", "Age",x)
  x<-gsub(":", "*",x)}


rownames(method_exclude_apa)<-clean(rownames(method_exclude_apa))

method_exclude_apa= round(method_exclude_apa,3)

colnames(method_exclude_apa)<-c("Est.", "SE Est", "t")

#method_exclude_apa[,1] <- printnum(method_exclude_apa[,1])
#method_exclude_apa[,2] <- printnum(method_exclude_apa[,2])
#method_exclude_apa[,3] <- printnum(method_exclude_apa[,3])
apa_table(method_exclude_apa, caption = "Linear mixed effects model predicting dropout rate by method and participant age while accounting for the specific phenomenon.")


#kable(coef(method_exclude_apacaption = "Linear mixed effects model predicting dropout rate by method and participant age while accounting for the specific phenomenon.", digits = 3)
```

```{r DropoutPlot, echo = FALSE, ig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "Percent dropout as explained by different methods. CF = central fixation, CondHT = conditioned headturn, FC = forced choice, HPP = headturn preference procedure, LwL = looking while listening, SA = stimulus alternation."}
method_exclude.plot
```

Methods which retain a higher percentage of participants might either be more suitable, because they are decreasing noise as most participants are on task, or less selective, thus increasing noise as participants who for example are fussy are more likely to enter the data pool. We thus turn to a meta-analytic assessment of the same methods discussed here.

### Effect sizes as a function of procedure

We built a meta-analytic model with Cohen's *d* as the dependent variable, method and mean age centered as independent variables, which we allowed to interact. The model includes the variance of *d* for sampling variance, and paper within meta-analysis as a random effect nested within phenomenon (because we assume that within a paper experiments and thus effect sizes will be more similar to each other than across papers). We again selected the most frequently used method central fixation as the baseline and limited this analysis to the same methods that we investigated above.

```{r MethodEffect, echo = FALSE, results='asis'}
method_rma_apa <- method.rma_coef

method_rma_apa=round(method_rma_apa,3)
method_rma_apa[,1]<-paste0(method_rma_apa[,1]," [",method_rma_apa[,5],",",method_rma_apa[,6],"]")
method_rma_apa=method_rma_apa[,1:4]
rownames(method_rma_apa)<-clean(rownames(method_rma_apa))
colnames(method_rma_apa)<-c("Est. (CI)","SE","z","p")


apa_table(method_rma_apa, caption ="Meta-analytic regression predicting effect size Cohen's *d* with participant age and method (central fixation is baseline method).")
#kable(method.rma_coef, caption = "Effect of d by method with central fixation as baseline method.", digits = 3)
```



```{r MethodPlot, echo = FALSE, ig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "Effect size by different methods. CF = central fixation, CondHT = conditioned headturn, FC = forced choice, HPP = headturn preference procedure, LwL = looking while listening, SA = stimulus alternation."}
method.plot
```

The model results show that compared to central fixation, conditioned headturn and forced choice yield reliably higher effect sizes, all other methods do not statistically differ from this baseline (note that looking while listening is approaching significance). When factoring in age, looking while listening shows a significant interaction, and conditioned headturn approaches significance, indicating an increase in effect sizes as infants mature. Age is marginally above the significance threshold, the positive estimate further underlines that overall effect sizes increase for older participants -- an observation consistent with the view that infants and toddlers become more proficient language users and are increasingly able to react appropriately in the lab. 


## Questionable research practices

To assess whether researchers selectively add participants to obtain a significant *p* value, we assess the relationship between (absolute) observed effect sizes in single studies and the associated sample size. The rationale behind this analysis is simple: The smaller the effect size, the larger the sample needed for a significant *p* value. If sample size decisions are made before data collection and all results are published, we expect no relation between observed effect size and sample size. A significant non-parametric correlation indicates that only those studies with significant outcomes were published [@pubbias]. 


```{r Bias, echo = FALSE, ig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "For each dataset observed effect size per study plotted against sample size."}
bias_grid
```

We illustrate the relationship between effect size and sample size, separated by meta-analysis, in Figure 4. The regression line is plotted on top of points indicating single experiments. The test results for a significant negative relationship can be found in Table XX. 
Four datasets turn out to have a significant negative relationship between sample size and effect size, indicating bias; two assessing infants' ability to discriminate vowels, one on word segmentation, and one testing whether children use mutual exclusivity during word learning. The last case might be driven by a single high-powered study, however. We further observe a positive relationship between sample size and observed effect size in two datasets, namely infant directed speech preference and categorization bias. 


```{r BiasData, echo = FALSE, results='asis'}

apa_table(data_bias, col.names = c("Meta-analysis", "Kendall's Tau", "p"), align = c("l", "r", "r"), caption = "Non-parametric correlations between sample sizes and effect sizes for each dataset. A significant value indicates bias.")


#kable(data_bias, col.names = c("Meta-analysis", "Kendall's Tau", "p"), digits = 2, caption = "Non-parametric correlations between sample sizes and effect sizes for each dataset. A significant value indicates bias.")
```

# Discussion 
 
 
In this paper, we made use of a collection of standardized meta-analyses to assess the status quo in developmental research regarding typical effect sizes, sample size, power, and methodological choices in 13 meta-analyses on language development. With an average meta-analytic effect size of .57 and a typical sample size of only 17 participants per cell, we find that power is at 60%. 

The lack of power is particularly salient for phenomena typically tested on younger children, because sample sizes and effect sizes are both small; the one exception for research topics tested mainly with participants younger than one year is non-native vowel discrimination, which can be attributed to a large meta-analytic effect size estimate. Phenomena targeting older children tend towards larger effects, and here some studies turn out to be high-powered (see for example online word recognition). Both observations are first indicators that effect size estimates might not be considered when determining sample size. It might, in the case of apparently over-powered studies however be possible that next to testing a main effect, such as whether children recognize a given word online, studies aimed to tap into factors affecting this ability. As consequence, studies would be powered appropriately, as an interaction effect will be more difficult to detect than a main effect. 


We investigated the possibility that researchers base their sample size on the effect size reported in the seminal paper of their research topic instead of meta-analytic effect size. This turns out to be an unsuitable strategy: As described in the results section, the larger the original effect size, the more likely is an overestimation of the meta-analytic effect size. Researchers should thus be wary of reports implying a strong, robust effect with infants and toddlers in the absence of corroborating data in the form of multiple replications. The lack of a relationship between either overall meta-analytic effect size or seminal reported effect size and sample size across phenomena indicates that researchers' experiment planning is not impacted by an estimated effect size of the phenomenon under investigation. Studies might instead be designed and conducted with pragmatic considerations in mind, such as participant availability. We conclude by and large, we find that studies are habitually underpowered, because sample sizes typically remain close to what can be called a "field standard" of 15 to 20 participants. 

The practice of conducting studies with a sample size that is based on "field standards"is highly problematic for many reasons: As we show, those studies are highly likely to be underpowered. This is a problem for two main reasons. First, many experiments will not yield significant outcomes despite the presence of a true, but small effect. Researchers might thus be inclined to conclude that an ability is absent in a population or these data will not be published at all. If an underpowered study is published because the outcome is significant, this study will overestimate the size of the underlying effect, thereby on one hand adding biased results to the available literature [and thus further biasing any meta-analytic effect size estimate, @Sterling1995, @yarkoni2009] and on the other hand perpetuating the practice of sampling only so few participants. At worst, this practice can lead to the perpetuation of a false hypothesis [consider for example the meta-analysis of romantic priming by @shanks2015romance].


We investigated the possibility that publication bias and underpowered studies interact in a final set of analyses through the relationship between observed effect size and sample size. This analysis might reflect whether researchers selectively add participants to obtain a significant result. In the supplementary materials we further report on funnel plot asymmetry, a complementary test of publication bias. We observed that in four datasets smaller effect sizes coincided with larger sample sizes, which might be an indication of questionable research practices. At the same time we find two (numerically) positive correlations, an unexpected result as it means that larger sample sizes coincide with larger effects. One possible reason for this might be that for example older infants are both easier to test and yield larger effects. This explanation is in line with our finding when investigating the effect of method that higher participant age is linked to larger effect sizes. 

 For the observed negative correlations alternative explanations to questionable research practices are possible: As soon as researchers are aware that they are measuring a more subtle effect and adjust sample sizes accordingly, we expect to observe this negative correlation. Consider for example vowel discrimination, which can be studied with very different vowels such as in "bit" and "but" or with subtler constrasts like in "bad" and "bed". In fact, in the presence of consequent and accurate a priori power calculations, a correlation between sample size and effect size must be observed. However, our previous analyses indicate that power is not considered when making sample size decisions. 
 
 To conclude that questionable practices are the basis for our observations, we thus checked for funnel plot asymmetry, which indicates whether a set of studies was missing from the literature, for example due to unexpected non-significant outcomes. For three datasets that showed a negative correlation between sample size and effect size, we also observe funnel plot asymmetry (both datasets on vowel discrimination as well as mutual exclusivity). For those three datasets we can thus conclude that publication bias underlies the observed link between sample size and effect size.  
 

## Concrete recommendations for developmental scientists

In this section, we aim to show how to move on from the status quo and improve the reliability of developmental research. 

### 1. Calculate power prospectively

Our results indicate that most studies testing infants and toddlers are severely underpowered, even when aiming to detect a main effect. Interactions will show smaller effect sizes and thus will be even harder to detect in most cases. Further, power varies greatly across phenomena, which mostly is due to differences in effect sizes. Sample sizes are not adjusted accordingly across phenomena, but remain close to the typical sample size of `r median(all_data$n)`. 

Our first recommendation is thus to assess in advance how many participants would be needed to detect an effect [see also @lakens2014sailing for a more detailed discussion and practical recommendations]. Note that we based our power estimations on whole meta-analyses, an analysis approach most suitable to make general statements about the status quo. It might, however, be the case that specific studies might want to base their power estimates on a subset of effect sizes to match age group and method. Both factors can, as we showed in our results, influence the to be expected effect size. To facilitate such analyses, all meta-analyses are shared on MetaLab and for each as much detail pertaining procedure and measurements have been coded as possible [see also @Tsuji2014]. 

In lines of research where no meta-analytic effect size estimate is available -- either because it is a novel phenomenon being investigated or simply due to the absence of meta-analyses -- we recommend considering typical effect sizes for the method used and the age group being tested. This paper is a first step towards establishing such measures, but more efforts and investigations are needed for robust estimates [see for example @Manybabies; @Manybabies1; @TestRetest]. 

### 2. Carefully consider method choice

One way to increase power is the use of more sensitive measurements; and we do find striking differences between methods. On one hand, drop-out rates varied a great deal (with medians between 5.9% for forced-choice and 45% for stimulus alternation). However, high drop-out rates can be offset by high effect sizes -- at least in the case of conditioned headturn. While drop-out rates are around 30-50%, effect sizes are above 1. Stimulus alternation, in contrast, does not fall into this pattern of high drop-out rates being correlated with high effect sizes, as the observed effect sizes associated with this method are in the range typical for meta-analyses in our dataset. 
The interpretation of this finding might be that some methods, specifically conditioned headturn, which have higher dropout rates, are better at generating high effect sizes due to decreased noise (e.g., by excluding participants that are not on task). However, there is an important caveat: Studies with fewer participants (thanks to higher drop-out rates) might simply be underpowered, and thus any significant finding is likely to over-estimate the effect. 

Nevertheless, when possible, it seems important to consider the paradigm being used, and possibly use a more sensitive way of measuring infants' capabilities. One reason that researchers do not choose the most robust methods might be due to a lack of consideration of meta-analytic effect size estimates, which in turn might be (partially) due to a lack of information on and experience in how to interpret effect size estimates and use them for study planning [@Mills-Smith2015]. We thus recommend to change this practice and take method effects into account. Further, current efforts to estimate the impact of method choice experimentally are an important endeavor in developmental research [@Manybabies].

### 3. Report all data

A possible reason for prospective power calculations and meta-analyses being rare lies in the availability of data in published reports.  Reports and discussions of effect sizes in experimental studies are rare, but despite long-standing recommendations to move beyond the persistent focus on *p* values [such as @APA2001], a shift towards effect sizes or even the reporting of them has not (yet) been widely adopted [@Mills-Smith2015]. 

A second impediment to meta-analyses in developmental science are current reporting standards, which make it difficult and at times even impossible to compute effect sizes from the published literature. For example, for within-participant measures it is necessary to report the correlation between conditions if two types of results are reported (most commonly outcomes of a treatment and control condition). However, this correlation, necessary to both compute effect sizes and their variance, is habitually not reported and has to be obtained via direct contact with the original authors [see for example @InWordDB] or estimated [as described in @InStatDB]. 
In addition, reporting (as well as analysis) of results is generally highly variable, with raw means and standard deviations not being available for all papers. 

We suggest reporting the following information, in line with current APA guidelines: Means and standard deviations of dependent measures being statistically analyzed (for within-participant designs with two dependent variables, correlations between the two should be added), test statistic, exact *p* value (when computed), and effect sizes (for example Cohen's *d* as used in the present paper) where possible. Such a standard not only follows extant guidelines but also creates coherence across papers and reports, thus improving clarity [@Mills-Smith2015]. A step further would be the supplementary sharing of all anonymized results on the participant level, thus allowing for the necessary computations and opening the door for other types of cumulative analyses, for example in direct replications comparing raw results. 


## How to increase the use and availability of meta-analyses

Conducting a meta-analysis is a laborious process, particularly according to common practice where only a few people do the work, with little support tools and educational materials available. Incentives for creating meta-analyses are low, as public recognition is tied to a single publication. The benefits of meta-analyses for the field, for instance the possibility to conduct power analyses, are often neither evident nor accessible to individual researchers, as the data are not shared and traditional meta-analyses remain static after publication, aging quickly as new results emerge [@Tsuji2014]. 

To support the improvement current practices, we propose to make meta-analyses available in the form of ready-to-use online tools, dynamic reports, and as raw data. These different levels allow researchers with varying interest and expertise interests to make the best use of the extant record on language development, including study planning by choosing robust methods and appropriate sample sizes. There are additional advantages for interpreting single results as well as for theory building that emerge from our collection of meta-analyses: On one hand, researchers can easily check whether their study result falls within the expected range of outcomes for their research question -- indicating whether or not a potential moderator influenced the result. On the other hand, aggregating over many data points allows for the tracing of emerging abilities over time, quantifying their growth, and identifying possible trajectories and dependencies across phenomena [for a demonstration see @SynthesisPaper]. Finally, by making our data and source code open, we also invite contributions and can update our data, be it by adding new results, file-drawer studies, or new datasets. Our implementation of this proposal is freely online available at http://metalab.stanford.edu.



## Cumulative evidence to decide whether skills are "absent" or not

Developmental research often relies on interpreting both significant and non-significant findings, particularly to establish a developmental time-line tracing when skills emerge. This approach is problematic for multiple reasons, as we mentioned in the introduction. Disentangling whether a non-significant finding indicates the absence of a skill, random measurement noise, or the lack of experimental power to detect this skill reliably and with statistical support is in fact impossible based on *p* values. Further, we want to caution researchers against interpreting the difference between significant and non-significant findings without statistically assessing it first [@gelman2006difference]. 

Concretely, we recommend the use of meta-analytic tools as demonstrated in this paper as well as in the work by @SynthesisPaper. The use of meta-analyses precisely to demonstrate the absence of an effect was also recently demonstrated by @vadillo2016underpowered. In this study, null results that were taken as evidence for an absent effect were pooled to yield an effect size estimate of Cohen's *d* = .3, an effect larger than some pertaining to the literature we survey here. This striking result thus must prompt re-evaluation of long-standing theoretical models. 

Aggregating over multiple studies allows not only for a more reliable estimate of an effect and conclusions about its absence (because any single finding might either be a false positive or a false negative) but also makes it possible to trace developmental trajectories. A demonstration of such a procedure is given in the work of @InPhonDB for native and non-native vowel discrimination. The results match well with the standard assumption that infants begin to tune into their native language at around six months of age. For a contrasting example, see @InWordDB, where the typically assumed developmental trajectory for word segmentation from native speech could not be confirmed, as across all included age groups infants seem to be able to detect words in the speech stream -- the effect size of this skill is simply comparatively small and thus it is difficult to detect [see also @TopDownBottomUp for a more recent discussion of both meta-analyses]. As a consequence, meta-analytic investigations can yield more refined, or even restructured theoretical accounts of child development, bolstered with a better estimate of the timeline for phenomena of interest. 


## Future directions

The present analyses can be expanded and improved in a number of ways. First, the present collection of meta-analyses does not represent an exhaustive survey of phenomena in language acquisition, let alone child development research. Particularly, topics typically investigated in younger children are over-represented. However, we sampled in an opportunistic, and thus to some degree random fashion, which lends some credibility to our approach. It would nonetheless be advisable to follow up on this report with a larger sample. To this end, we made all source materials along with extensive documentation available online. 

Second, it would be important to further investigate the role of participant age in child development research. It is possible that developmental psychologists working with older age groups might focus on different issues or find that power and experimental design choices are less problematic; for instance, it may be easier to recruit larger samples via institutional testing in schools, and older children may be more reliable and consistent in their responses [@roberts2000rank]. We thus hope particularly to analyze more studies of older children to test this assumption. 



## Conclusion

We have demonstrated the use of standardized collections of meta-analyses for a diagnosis of (potential) issues in developmental research. Our results point to an overall lack of consideration of meta-analytic effect size in experiment planning, leading to habitually under-powered studies. In addition, method choice and participant age play an important role in the to be expected outcome; we here provide first estimates of the importance of either factor in experiment design. Assessing data quality, we find no evidence for questionable research practices and conclude that most phenomena considered here have evidential value. To ensure that developmental research is robust and that theories of child development are built on solid and reliable results, we strongly recommend an increased use of effect sizes and meta-analytic tools, including prospective power calculations. 


\newpage



# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}


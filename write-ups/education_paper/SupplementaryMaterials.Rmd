---
title             : "Supplementary materials for the paper Promoting replicability in developmental research through meta-analyses: Insights from language acquisition research"
shorttitle        : "SupMat Reprodocuible developmental research"


author: 
  - name          : "Christina Bergmann"
    affiliation   : "1, 2"
    corresponding : yes    # Define only one corresponding author
    address       : "Max Planck Institute for Psycholinguistics, Language Development Department, Wundtlaan 1, 6525XD Nijmegen, The Netherlands"
    email         : "chbergma@gmail.com"
  - name          : "Sho Tsuji"
    affiliation   : "3,1"
  - name          : "Page E. Piccinini"
    affiliation   : "4"
  - name          : "Molly L. Lewis"
    affiliation   : "5"
  - name          : "Mika Braginsky"
    affiliation   : "6"
  - name          : "Michael C. Frank"
    affiliation   : "7"
  - name          : "Alejandrina Cristia"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Ecole Normale Superieure, PSL Research University, Departement d'Etudes Cognitives, Laboratoire de Sciences Cognitives et Psycholinguistique (ENS, EHESS, CNRS)"
  - id            : "2"
    institution   : "Max Planck Institute for Psycholinguistics, Language Development Department"
  - id            : "3"
    institution   : "University of Pennsylvania, Department of Psychology"
  - id            : "4"
    institution   : "Ecole Normale Superieure, PSL Research University, Departement d'Etudes Cognitives, Neuropsychologie Interventionnelle (ENS, EHESS, CNRS)"
  - id            : "5"
    institution   : "University of Chicago, Computation Institute/University of Wisconsin-Madison, Department of Psychology"
  - id            : "6"
    institution   : "Massachusetts Institute of Technology, Department of Brain and Cognitive Sciences"
  - id            : "7"
    institution   : "Stanford University, Department of Psychology, Language and Cognition Lab"

author_note: >
  

abstract: >


keywords          : 
wordcount         : 

bibliography      : ["metalab_education.bib"]

figsintext        : yes
figurelist        : yes
tablelist         : yes
footnotelist      : yes
lineno            : no

lang              : "english"
class             : "paper"
output            : papaja::apa6_word
---

```{r Preamble, echo = FALSE, warning = FALSE, message = FALSE}
# These packages are need in the here for some in document commands
library(knitr)
library(tidyverse)
library(pwr)
library(RColorBrewer)

knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE, fig.pos = "T")
options(scipen=1, digits=2)

#source("save_analyses.R") # if changes have been made to any analyses run this, comment below
load("educationpaper_environment.RData") # if no changes to analyses run this, comment above
```

\newpage



# Instructions to reproduce the analyses 

To expand the analyses we show here and in the reproducible manuscript, the following steps are necessary:  

1. Clone from github the metalab repository: https://github.com/langcog/metalab
2. Download the scripts for this paper and put them in a subfolder. 
3. Adjust the location of `global.R` in `initial_data.R` to read in the correct file. 
4. Run the scripts. To load data, first run `initial_data.R`. 


When only aiming to reproduce the same manuscript and to inspect the data we used, these steps are necessary:

1. Download the .Rmd files (like the raw version of this file).
2. Download the environment file `educationpaper_environment.Rdata` (generated by `save_analyses.R`).
3. Place all files in the same folder.

All required packages are loaded at the beginning of each script.

# Data preprocessing

We preprocess the data to exclude data testing special populations, non-critical conditions, and to remove outliers. We also exclude a longitudinal dataset from these analyses, because it is not suitable for our purposes. In addition, we remove a dataset with adult participants. 

```{r Preprocessing, eval = FALSE}

## CLEAN DATA ####
all_data = all_data %>%
  filter(is.na(condition_type) | condition_type == "critical") %>%
  filter(dataset!="Pointing and vocabulary (longitudinal)") %>%
  filter(dataset!="Categorization Bias") %>%
  filter(infant_type == "typical") 


#Remove outliers

clean_data = all_data %>%
  group_by(dataset) %>%
  mutate(mean_es = median(d_calc)) %>%
  mutate(sd_es = sd(d_calc)) %>%
  ungroup() %>%
  mutate(no_outlier = ifelse(d_calc < mean_es+3*sd_es, ifelse(d_calc > mean_es-3*sd_es, TRUE, FALSE), FALSE))  %>%
  filter(no_outlier)

#Comment out if you do not want to remove outliers
all_data = clean_data

```


# Effect size calculation

Effect sizes were calculated based on standard formulae; below we show commented code from the MetaLab platform at http://metalab.stanford.edu and the associated github repository https://github.com/langcog/metalab. In the source script `compute_es.R` a number of special cases are also computed, we leave them out of this document for brevity. The computations are based on the following references: @Lipsey; @morris2002combining; @dunlap1996meta; @howell2012statistical; @metafor.

```{r ESCalc, eval = FALSE}

  #start of decision tree where effect sizes are calculated differently based on participant design
  #depending on which data is available, effect sizes are calculated differently
  if (participant_design == "between") {
    es_method  <- "between"
    #effect size calculation
    if (complete(x_1, x_2, SD_1, SD_2)) {
      pooled_SD <- sqrt(((n_1 - 1) * SD_1 ^ 2 + (n_2 - 1) * SD_2 ^ 2) / (n_1 + n_2 - 2)) # Lipsey & Wilson, 3.14
      d_calc <- (x_1 - x_2) / pooled_SD # Lipsey & Wilson (2001)
    } else if (complete(t)) {
      d_calc <- t * sqrt((n_1 + n_2) / (n_1 * n_2)) # Lipsey & Wilson, (2001)
    } else if (complete(f)) {
      d_calc <- sqrt(f * (n_1 + n_2) / (n_1 * n_2)) # Lipsey & Wilson, (2001)
    }
    if (complete(n_1, n_2, d_calc)) {
      #now that effect size are calculated, effect size variance is calculated
      d_var_calc <- ((n_1 + n_2) / (n_1 * n_2)) + (d_calc ^ 2 / (2 * (n_1 + n_2)))
    } else if (complete(r)) {
      #if r instead of d is reported, transform for standardization
      d_calc <- 2 * r / sqrt(1 - r ^ 2)
      d_var_calc <- 4 * r_var / ((1 - r ^ 2) ^ 3)
    } else if (complete(d, d_var)) {
      #if d and d_var were already reported, use those values
      d_calc <- d
      d_var_calc <- d_var
    }

  } else if (participant_design == "within_two") {
    if (is.na(corr)) {
      #if correlation between two measures is not reported, use an imputed correlation value
      corr <- corr_imputed
    }
    #effect size calculation
    if (complete(x_1, x_2, SD_1, SD_2)) { 
      pooled_SD <- sqrt((SD_1 ^ 2 + SD_2 ^ 2) / 2) # Lipsey & Wilson (2001)
      d_calc <- (x_1 - x_2) / pooled_SD # Lipsey & Wilson (2001)
      es_method  <- "group_means_two"
    } else if (complete(x_1, x_2, SD_dif)) {
      within_SD <- SD_dif / sqrt(2 * (1 - corr)) # Lipsey & Wilson (2001); Morris & DeShon (2002)
      d_calc <- (x_1 - x_2) / within_SD # Lipsey & Wilson (2001)
      es_method  <- "group_means_two"
    } else if (complete(x_dif, SD_1, SD_2)) {
      pooled_SD <- sqrt((SD_1 ^ 2 + SD_2 ^ 2) / 2) # Lipsey & Wilson (2001)
      d_calc <- x_dif / pooled_SD # Lipsey & Wilson (2001)
      es_method  <- "subj_diff_two"
    } else if (complete(x_dif, SD_dif)) {
      wc <- sqrt(2 * (1 - corr))
      d_calc <- (x_dif / SD_dif) * wc #Morris & DeShon (2002)
      es_method  <- "subj_diff_two"
    } else if (complete(t)) {
      wc <- sqrt(2 * (1 - corr))
      d_calc <- (t / sqrt(n_1)) * wc #Dunlap et al., 1996, p.171
      es_method  <- "t_two"
    } else if (complete(f)) {
      wc <- sqrt(2 * (1 - corr))
      d_calc <- sqrt(f / n_1) * wc
      es_method  <- "f_two"
    }
    if (complete(n_1, d_calc)) {
      #now that effect size are calculated, effect size variance is calculated
      d_var_calc <- (2 * (1 - corr)/ n_1) + (d_calc ^ 2 / (2 * n_1)) # Lipsey & Wilson (2001)
    } else  if (complete(r)) {
      #if r instead of d is reported, transform for standardization
      d_calc <- 2 * r / sqrt(1 - r ^ 2)
      d_var_calc <- 4 * r_var / ((1 - r ^ 2) ^ 3)
      es_method  <- "r_two"
    } else if (complete(d, d_var)) {
      #if d and d_var were already reported, use those values
      d_calc <- d
      d_var_calc <- d_var
      es_method  <- "d_two"
    }

  } else if (participant_design == "within_one") {
    if (complete(x_1, x_2, SD_1)) {
      d_calc <- (x_1 - x_2) / SD_1
      es_method  <- "group_means_one"
    } else if (complete(t)) {
      d_calc <- t / sqrt(n_1)
      es_method  <- "t_one"
    } else if (complete(f)) {
      d_calc <- sqrt(f / n_1)
      es_method  <- "f_one"
    }
    if (complete(n_1, d_calc)) {
      d_var_calc <- (1 / n_1) + (d_calc ^ 2 / (2 * n_1)) #this models what is done in metafor package, escalc(measure="SMCR"() (Viechtbauer, 2010)
      
    } else if (complete(r, n_1)){ 
      # this deals with pointing and vocabulary 
      # Get variance of transformed r (z; fisher's tranformation)
      SE_z = 1 / sqrt(n_1 - 3) # from Howell (2010; "Statistical methods for Psychology", pg 275)
      var_z = SE_z ^ 2
      
      # Transform z variance to r variance
      var_r = tanh(var_z)  # from wikipedia (https://en.wikipedia.org/wiki/Fisher_transformation) for convert z -> r, consistent with Howell
      
      # Transform r to d
      d_calc = 2 * r / (sqrt(1 - r ^ 2)) # from (Hunter and Schmidt, pg. 279)
      d_var_calc = (4 * var_r)/(1 - r ^ 2) ^ 3 # from https://www.meta-analysis.com/downloads/Meta-analysis%20Converting%20among%20effect%20sizes.pdf (pg. 4)
      
      es_method  <- "r_one"
      
    } else if (complete(r)) {
      #if r instead of d is reported, transform for standardization
      d_calc <- 2 * r / sqrt(1 - r ^ 2)
      d_var_calc <- 4 * r_var / ((1 - r ^ 2) ^ 3)
     
       es_method  <- "r_one"
      
    } else  if (complete(d, d_var)) {
      #if d and d_var were already reported, use those values
      d_calc <- d
      d_var_calc <- d_var
      es_method  <- "d_one"

```

Readers interested in calculating effect sizes for single papers might want to try out the shiny app or spreadsheet based on @lakens2013   
https://katherinemwood.shinyapps.io/lakens_effect_sizes/   
https://osf.io/ixGcd/ 

## Additional visualizations

Both analyses on method choice took into account participant age; for clarity we did not visualize the interaction between method, exclusion rate / effect size, and age. Here we provide the respective plots for the interested reader. 

```{r DropoutPlot, echo = FALSE, ig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "Percent dropout as explained by different methods and participant age."}
exclude_age.plot <- method_exclude_age.plot + ylab("Percent Excluded") 
exclude_age.plot
```

```{r MethodPlot, echo = FALSE, ig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "Effect size by different methods and age."}
method_age.plot
```


\newpage

We can also visualize typical effect sizes by mean participant age (odered on the x-axis), to better visualize the data in Table 1 of the main paper.

```{r ESagePlot, echo = FALSE, ig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "Effect size per phenomenon, ordered by mean participant age."}
ES_MA.plot = ggplot(all_data , aes(x = reorder(dataset, mean_age, FUN=mean), y = d_calc, color = dataset)) +
  geom_hline(yintercept = 0) +
  geom_boxplot(lwd=1) +
  geom_jitter(size = .5, alpha = .35) +
  scale_color_manual(values=colorRampPalette(brewer.pal(11,"Paired"))(length(unique(all_data$short_name))))+
  xlab("Meta-Analysis") +
  ylab("Effect Size") +
  ylim(c(-1, 2.5)) +
  theme_classic() +
  theme(axis.line.x = element_line(), axis.line.y = element_line(), axis.text.x=element_blank(),
        axis.ticks.x=element_blank(), legend.position = "top")
ES_MA.plot
```





\newpage 

# Additional analyses


### How does effect size relate to *p*-values?

```{r SigPlot, echo = FALSE, fig.width=4, fig.height=3.5, fig.cap="Comparison of a study's effect size and the according *p*-values. Point size reflects sample size. The typical significance threshold of .05 is indicated by a vertical line."}
sig_plot
```

Single experiments are often evaluated by their associated *p*-value, despite the frequent criticisms and well-documented shortcomings of that measure [@Ioannidis2005]. One of them is particularly relevant here: In the Pearson-Neyman model which is (implicitly) underlying most current empirical research, *p*-values should be used to inform a binary decision, namely to either reject the null hypothesis or fail to do so; in contrast, effect sizes are a continuous measure. Researchers sometimes believe that significant *p*-values are equivalent to very high effect sizes, and that non-significant *p*-values are due to very low effect sizes near zero lead; and more generally that *p*-values have a continuous interpretation such that very low *p*-values necessarily indicate very strong evidence for differences, whereas values near .05 indicate weaker evidence for a difference.

This figure addresses these intutions by showing *p* value and effect size data within MetaLab. The vertical line marks the usual binary decision threshold for *p*-values at .05. We see that, for extreme values, namely effect sizes near zero and above 1.5, the intuition that *p*-values will be respectively large and small is borne out. However, the majority of effect sizes we observe falls in a range that with sufficient power, in other words a sufficiently large sample, can lead to a significant outcome. Underpowered studies, in contrast, might tap into a similar sized effect but fail to reach significance. The grey horizontal band illustrates such a region where both significant and non-significant results are observed. 



\newpage


### Power distribution in meta-analyses


How representative is our power estimate based on median sample size compared to single studies? We calculated power on the study level and visualize the results here, ordered by mean power. 

```{r Powerplot, echo = FALSE, fig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap="Summary plot of power across meta-analyses"}
power_study.plot
```



\newpage



### Correlation between oldest paper effect size and meta-analytic effect size
```{r dDiffplot, echo = FALSE, fig.pos = "T!", fig.cap="Correlation of largest d from oldest paper and difference between oldest d and meta-analytic d."}
d_comparison_diff.plot
```

To illustrate the disparity between the oldest effect size and the meta-analytic effect, and consequently the difference in power, we plot the difference between both against the oldest effect. This difference is larger as oldest effect size increases, with an average of `r mean(d_comparison_full$diff_d)` compared with an average effect size of `r mean(abs(d_comparison_full$d))` (note that we based this on the absolute value). The plot showcases that researchers might want to be wary of large effects, as they are more likely to be non-representative of the true phenomenon compared to smaller initial effects being reported. Especially when making decisions about sample sizes, large effect might thus not be the best guide. Taking the above-mentioned mean values as example, a realistic sample size to ensure 80% power would be `r round(pwr.t.test(n = NULL,d = mean(abs(d_comparison_full$d)), sig.level = .05, power=.80)$n, digits = 0)` participants, instead of `r round(pwr.t.test(n = NULL,d = mean(abs(d_comparison_full$largest_d)), sig.level = .05, power=.80)$n, digits = 0)` participants suggested by the first paper. While these numbers average over research questions and methods, which all influence the specific number of participants necessary, this example showcases that experimenters should take into account as much evidence as available to be able to plan for robust and reproducible studies. 



### Power over time

The comparison of initial and meta-analytic effect size has a number of caveats, for example, as we will lay out in the next section, methods might be different between initial reports and our overall sample; the availability of methods changes over time, as new approaches are being developed and automated procedures become more common. Further, the largest effect size from a seminal paper might have been spurious, and the research community could well be aware of that. In additional, as infant research becomes more common, recruitment and obtaining funds might both become easier, thereby increasing typical sample size over the years. 
For a more continuous approach, we thus investigate power (which is determined by effect size and sample size) as follows. We first generate a meta-analytic model for each dataset that takes into account infant age and method and then derive the respective to be expected effect size base on those data for each entry in this dataset. Power is then estimated based on the sample size actually tested. 
Across datasets we observe a general negative trend, with varied steepness. The only positive trends occur in the upper ranges of estimated power and for older children. 

```{r PowerTime, echo = FALSE, fig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap="Summary plot of power across years and meta-analyses"}
power_year.plot
```


\newpage




# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}



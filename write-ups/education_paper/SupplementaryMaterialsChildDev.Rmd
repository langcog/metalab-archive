---
title: "MetaLab: A platform for cumulative meta-meta-analyses"
author: Christina Bergmann, Page Piccinini, Sho Tsuji,Molly Lewis, Mika Braginsky, Michael C. Frank, and Alejandrina Cristia 
header-includes:
  - \usepackage{longtable}
  - \usepackage{caption}
date: '`r Sys.Date()`'
output:
  pdf_document:
  html_document:
    code_folding: hide
    number_sections: no
    theme: cerulean
subtitle: Supplementary Information
toc: yes
---

```{r Preamble, echo = FALSE, warning = FALSE, message = FALSE}
# These packages are need in the here for some in document commands
library(knitr)
library(dplyr)
library(pwr)

knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE, fig.pos = "T")

#source("save_analyses.R") # if changes have been made to any analyses run this, comment below
load("educationpaper_environment.RData") # if no changes to analyses run this, comment above
```

# Effect size calculation

- Formulae used
- Frequency
- Data availability per dataset

# Supplementary information to the analyses in the paper


```{r Plot of Difference of d Values, echo = FALSE, fig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap="Correlation of largest d from oldest paper and difference between oldest d and meta-analytic d."}
d_comparison_diff.plot
```

To illustrate the disparity between the oldest effect size and the meta-analytic effect, and consequently the difference in power, we plot the difference between both against the oldest effect. This difference is larger as oldest effect size increases, with an average of `r mean(d_comparison_full$diff_d)` compared with an average effect size of `r mean(abs(d_comparison_full$d))` (note that we based this on the absolute value). The plot showcases that researchers might want to be wary of large effects, as they are more likely to be non-representative of the true phenomenon compared to smaller initial effects being reported. Especially when making decisions about sample sizes, large effect might thus not be the best guide. Taking the above-mentioned mean values as example, a realistic sample size to ensure 80% power would be `r round(pwr.t.test(n = NULL,d = mean(abs(d_comparison_full$d)), sig.level = .05, power=.80)$n, digits = 0)` participants, instead of `r round(pwr.t.test(n = NULL,d = mean(abs(d_comparison_full$largest_d)), sig.level = .05, power=.80)$n, digits = 0)` participants suggested by the first paper. While these numbers average over research questions and methods, which all influence the specific number of participants necessary, this example showcases that experimenters should take into account as much evidence as available to be able to plan for robust and reproducible studies. 






# Additional analyses

### Power over time

The comparison of initial and meta-analytic effect size has a number of caveats, for example, as we will lay out in the next section, methods might be different between initial reports and our overall sample; the availability of methods changes over time, as new approaches are being developed and automated procedures become more common. Further, the largest effect size from a seminal paper might have been spurious, and the research community could well be aware of that. In additional, as infant research becomes more common, recruitment and obtaining funds might both become easier, thereby increasing typical sample size over the years. 
For a more continuous approach, we thus investigate power (which is determined by effect size and sample size) as follows. We first generate a meta-analytic model for each dataset that takes into account infant age and method and then derive the respective to be expected effect size base on those data for each entry in this dataset. Power is then estimated based on the sample size actually tested. 
Across datasets we observe a general negative trend, with varied steepness. The only positive trends occur in the upper ranges of estimated power and for older children. 

```{r Plot of power over time, echo = FALSE, fig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap="To BE UPDATED: Summary plot of power across years and meta-analyses"}
power_year.plot
```

## Publication biases


```{r Funnel data, echo = FALSE}
kable(data_funnel, col.names = c("Meta-analysis (MA)", "Kendall's Tau", "p-value"), digits = 2, caption = "Non-parametric test for funnel plot assymmetry. A significant value indicates bias.")
```


```{r Bias data, echo = FALSE}
kable(data_bias, col.names = c("Meta-analysis (MA)", "Kendall's Tau", "p-value"), digits = 2, caption = "Non-parametric correlations between sample sizes and effect sizes for each dataset. A significant value indicates bias.")
```
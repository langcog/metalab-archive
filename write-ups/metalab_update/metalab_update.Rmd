---
title: "A Quantitative Synthesis of Early Language Acquisition Using Meta-Analysis"
shorttitle: "A Quantitative Synthesis"
author: 
  - name:  Molly Lewis*
    affiliation: 1
  - name: Mika Braginsky*
    affiliation: 1
  - name: Sho Tsuji
    affiliation: 2
  - name: Christina Bergmann
    affiliation: 2
  - name: Page Piccinini
    affiliation: 2
  - name: Alejandrina Cristia
    affiliation: 2
  - name: Michael C. Frank
    affiliation: 1
affiliation:
  - id: 1
    institution: Department Psychology, Stanford University
  - id: 2
    institution: Laboratoire de Sciences Cognitives et Psycholinguistique, ENS
    
abstract: |
  replicability, etc.
  
note: |  
  Correspondence concerning this article should be addressed to Molly Lewis, Psychology Department, Stanford University. 450 Serra Mall, Stanford, CA 94305. E-mail: mll@stanford.edu.
  
  \*Contributed equally to this work.
  
keywords: "replicability, reproducibility, meta-analysis, developmental psychology, language acquisition"

wordcount: "XXX"

class: man
lang: american
figsintext: yes
lineno: no
bibliography:
  - metalab_update.bib

output: papaja::apa6_pdf
---

```{r message = FALSE, warning = FALSE}
require("papaja")
apa_prepare_doc() # Prepare document for rendering
```

# Introduction
Psychologists hope to build  generalizable theories about human behavior---theories that hold true beyond particulars of an individual study. The field has grown concerned as a result in the face of recent high-profile evidence that an effect observed in one study may not be the same in another (``replicability crisis''; Ioannidis, 2005; Nosek, 2012, 2015). Some of this variability is to be expected, however---the question we should instead be asking is, do the data provide support for the theory, even if they are noisy? Furthermore, to build parsimonious theories of human behavior, we should seek to explain not just individual phenemenon, but entire literatures of research. What is needed, then, is a tool for aggregating noisy data across studies within a phenomenon, as well as a common language for comparing effects across phenomenona.

Meta-analytic methods provide a powerful tool for doing just this. The basic unit of meta-analysis---the effect size---provides an estimate of the *size* of an effect, as well as a measure of uncertainty around this point estimate. With  such a continuous measure of success, we can apply the same reasoning we use to aggregate  noisy measurements over  participants in a single study: By assuming each *study*, rather than participant, is sampled from a population, we can appeal to the classical statistical framework to combine estimates of the effect size for a given phenomenon.

This quantitative approach provides a rich tool kit for synthesizing across literatures. By describing different phenomena using the same unit of measurement, we are able to compare effects in different domains. Rather than simply concluding that two effects are both ``real,'' we can ask more fine-grained questions: Is effect *X* bigger than effect *Y*? Does a moderator influence effect *X* in the same way as effect *Y*? This type of continuous analysis supports building quantitative models, and specifying theories that are more precise and constraining.

In addition to these theoretical motivations, there are practical reasons for conducting a quantitative synthesis. When planning an experiment, an estimate of the size of an effect on the basis of prior literature can inform the sample size needed to achieve a desired level of power. Meta-analytic estimates of effect sizes can also aid in design choices: If a certain paradigm tends to have overall larger effect sizes than another, the strategic researcher might select this paradigm in order to maximize the power of a study.

In practice, however, the feasability of this meta-analytic approach relies on the field’s commitment to practices that facilitate cumulative science. These practices apply to all stages of the research process. At the stage of experimental planning, researchers must pre-specify analytical descision to limit ``researcher'' degrees of freedom (Simmons, 2011; Simonsohn, 2014a, 2014b, 2014c). At the stage of completion, researchers should share a result regardless of its significance (Rosenthal, 1979; Fanelli 2012). And, at the stage of sharing, researchers must provide enough information about the method for another lab to conduct a close replication. Critically,r eports must also contain complete descriptions of both data and analytical decisions so that effect sizes can be calculcated for the purposes of meta-analysis,

In the present paper, we use meta-analytic methods to provide a quantitative synthesis of an entire field of psychological research: language acquisition.  We think this field is a particularly informative case study. It may be particularly vulnerable to false findings because running children is expensive (Ioanndis, 2005), and thus:

+ sample sizes are small 
+ replications difficult and rare
+ Recent attention about practices in developmental research @Peterson:2016

We have two goals:

+ Describe the state of the field in terms of its participation in practices that are prerequisites to cumulative science, and ultimately, a theoretical synthesis
+ Provide a preliminary theoretical synthesis of the field

Towards this end, we introduce [Metalab](http://metalab.stanford.edu/).


# Method
```{r load data, eval = F}
source("../../dashboard/global.R", chdir = TRUE)
#We calculated estimates of effect sizes for `r nrow(datasets)` different phenomenena in language acquisition. We selected these phenomena in order to describe development at many different levels of the language hierarchy, from the acquistion of prosody and phonemic contrasts, to gaze following in linguistic interaction. This wide range of phenomena allowed us to compare the course of development across different domains, as well as explore questions about the interactive nature of language acquisition. 

#Estimates of effect size were based on journal reports of experimental data.  In total, our sample includes estimates from `r datasets$num_papers`, `r datasets$num_experiments` different experiments and, `r datasets$num_subject` participants. 

#citations - [e.g., @bauer_2014; @bem_2011] 
#citations - @bauer_2014 → Baumer et al. (2014).
#italics - *R* 
#html - [RMarkdown](http://rmarkdown.rstudio.com/)


```

We calculated estimates of effect sizes for XX different phenomena in language acquisition. We selected these phenomena in order to describe development at many different levels of the language hierarchy, from the acquistion of prosody and phonemic contrasts, to gaze following in linguistic interaction. This wide range of phenomena allowed us to compare the course of development across different domains, as well as explore questions about the interactive nature of language acquisition. 

Estimates of effect size were based on journal reports of experimental data.  In total, our sample includes estimates from XX, XX different experiments and, XX participants. 

The process for selecting papers from the literature differed by domain, with some individual meta-analyses using more systematic approaches than others. [Simulations here?]

TABLE

## Statistical approach


# Replicability of the field
Effect size can vary between studies for reasons unrelated to a theoretical construct. One reason for this variability is the precision of the effect size, which we can model based on the sample size of the study. A remaining source variability, however, are biases introduced directly by the experimenter, via publication bias [@rothstein2006publication; @rosenthal1979file; @Fanelli:2010kf], p-hacking through analytical flexibility [@Simmons:2011iw; @Simonsohn:2014ch; @simonsohn2015better; @simonsohn2014p], or even fraud. These biases are much more difficult to model, and may therefore lead to large but unknown errors in estimates of the effect size. If these types of practices are present in the literature, estimates of effect size may be poor estimates of the true underlying effect size, making it extremely difficult to make theoretical progress. Below we present analyses of publicaiton bias and p-hacking that reveal no evidence of systematic experimenter bias in the language acquistion literature.

-> if we find bias, could be selection bias in meta-analysis

## Publication bias
Across studies we should expect some variability in effect size due to measurement and sampling noise alone. But this variability in effect size should be *systematic*: there should be less variability for more precise studies, as measured by sample size. The presence of variability in effect sizes that is not accounted for sample size may suggest publication bias in a literature.

* Egger's regression test
* Begg and Mazumdar’s rank correlation test - low power
* many sources of assymetry, eg. selection level
* are there ways to estimate selection model based on systmatic MAs? [need ps?]
* Ferguson-- reports of publication bias in literature
* Francis (2014) --TES method for significance in a paper (Maybe do this with ME?); do this with effect size?

## P-hacking
(p-curves)

## Power
Ioannidis and Trikalinos (2007) 


# Theoretical Synthesis


# Discussion


#### Author Contributions

#### Acknowledgments


#### References


```{r create_r-references}
r_refs(file = "metalab_update.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{8pt}

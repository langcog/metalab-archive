---
title: "A Quantitative Synthesis of Early Language Acquisition Using Meta-Analysis"
shorttitle: "A Quantitative Synthesis"

author: 
  - name: Molly Lewis*
    affiliation: 1
  - name: Mika Braginsky
    affiliation: 2
  - name: Sho Tsuji
    affiliation: 3
  - name: Christina Bergmann
    affiliation: 3, 4
  - name: Page Piccinini
    affiliation: 5
  - name: Alejandrina Cristia
    affiliation: 3
  - name: Michael C. Frank
    affiliation: 6
affiliation:
  - id: 1
    institution: Computation Institute, University of Chicago
  - id: 2
    institution: Department of Brain and Cognitive Sciences, MIT
  - id: 3
    institution: Ecole Normale Superieure, PSL Research University, Departement d'Etudes Cognitives, Laboratoire de Sciences Cognitives et Psycholinguistique (ENS, EHESS, CNRS) 
  - id: 4
    institution: Max Planck Institute for Psycholinguistics, Language Development Department
  - id: 5
    institution: NeuroPsychologie Interventionnelle, ENS
  - id: 6
    institution: Department of Psychology, Stanford University
    
abstract: To acquire a language, children must learn a range of skills, from the sounds of their language to the meanings of words. These skills are typically studied in isolation in separate research programs, but a growing body of evidence points to interdependencies across skills in the acquisition process. Here, we suggest that the meta-analytic method can support the process of building systems-level theories, as well as provide a tool for detecting bias in a literature. We present meta-analyses of 12 phenomena in language acquisition with over 700 effect sizes. We find that the language acquisition literature overall has a high degree of evidential value. We then present a quantitative synthesis of language acquisition phenomena that is consistent with interactivity in skills across the system.

note: |  
  $^*$To whom correspondence should be addressed. E-mail: mollylewis@uchicago.edu
  
keywords: "developmental psychology, language acquisition, quantitative theories, meta-analysis"

wordcount: "4784"

class: man
lang: english
figsintext: yes
lineno: no
bibliography:
  - metalab_synthesis.bib
header-includes:
  - \usepackage{setspace}
  - \usepackage{float}
  - \usepackage{graphicx}
  - \AtBeginEnvironment{tabular}{\singlespacing}
  - \usepackage{pbox}

output: papaja::apa6_pdf

---

```{r message = FALSE, warning = FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      cache = FALSE, 
                      fig.pos = "T!")
require("papaja")
```

```{r analysis setup}
rm(list = ls())

# load functions
source("../paper_scripts/pcurve.R")
source("../paper_scripts/synthesis_helpers.R")

# load libraries
library(knitr)
library(purrr)
library(langcog)
library(metafor)
library(forcats)
library(feather)
library(tidyverse)

# load data (cached by ../paper_scripts/cache_script_data.R)
synthesis_meta_data <- read_feather("../paper_data/synthesis_paper_datasets.feather")
synthesis_all_data <- read_feather("../paper_data/synthesis_paper_all_data.feather")
```

# Introduction
Children beginning to acquire a language must learn its sounds, its word forms, and their meanings, and a number of other component skills of language understanding and use. A synthetic theory that explains the inputs, mechanisms, and timeline of this process is an aspirational goal for the field of early language learning. One important aspect of such a theory is an account of how the acquisition of individual skills depends on others. For example, to what extent must the sounds of a language be mastered prior to learning word meanings? Although a huge body of research addresses individual aspects of early language learning  [see e.g., @kuhl2004early for review], only a small amount of work addresses the question of relationships between different skills [e.g., @feldman2013word; @johnson2010synergies; @shukla2011prosody]. Yet if meaningful relationships exist, they should play a central role in our theories. 

The effort to build synthetic theories is further complicated by the fact that there is often uncertainty about the developmental trajectory of individual skills. Developmental trajectories are typically communicated via verbal (often categorical) summaries of a set of variable experimental findings (e.g., "by eight months, infants can segment words from fluent speech"). In the case of contradictory findings, theorists may have uncertainty about which experimental findings can be used to constrain the theory, and often must resort to verbal discounting of one finding or the other based on methodological or theoretical factors. Resolving this issue requires a method for synthesizing findings in a more systematic and principled fashion. 

We suggest that a solution to both of these challenges---building integrative whole-system theories and evaluating evidential strength in a field of scientific research---is to describe experimental findings in quantitative, rather than qualitative, terms. Quantitative descriptions allow for the use of quantitative methods for aggregating experimental findings in order to evaluate evidential strength. In addition, describing experimental findings as quantitative estimates provides a common language for comparing across phenomena, and a way to make more precise predictions. In this paper, we consider the domain of language acquisition and demonstrate how the quantitative tools of meta-analysis can support theory building in psychological research. 

Meta-analysis is a quantitative method for aggregating across experimental findings [@hedges2014statistical;  @glass1976primary]. The fundamental unit of meta-analysis is the *effect size*: a scale-free, quantitative measure of "success" in a phenomenon. Importantly, an effect size provides an estimate of the size of an effect, as well as a measure of uncertainty around this point estimate. With  this quantitative measure, we can apply the same reasoning we use to aggregate  noisy measurements over participants in a single study: By assuming each study, rather than participant, is sampled from a population, we can appeal to a statistical framework to combine estimates of the effect size for a given phenomenon. While literature reviews can also function to combine findings across papers, meta-analytic methods are likely to be less biased than qualitative methods for aggregating even the smallest sample of studies [@valentine2010many].

Meta-analytic methods can support theory building in several ways. First, they provide a way to evaluate which effects in a literature are most likely to be robust: that is, how consistently an effect is found and the extent to which publication bias and other questionable research practices are present in the literature. This analysis in turn informs which effects should constrain the theory of a phenomenon. Having a method for determining the robustness of an effect is particularly important in light of recent evidence that an effect observed in one study may be unlikely to replicate in another [@ebersole2015many; @open2012open; @open2015estimating]. Failed replications are difficult to interpret, however, because they may result from a wide variety of causes, including an initial false positive, a subsequent false negative, or differences between initial and replication studies, such that making causal attributions in a situation with two conflicting studies is often difficult [@Gilbert1037; @anderson2016response].  By aggregating evidence across studies and assuming that there is some variability in true effect size from study to study, meta-analytic methods can provide a more veridical description of the empirical landscape, which in turn leads to better theory-building.

Second, meta-analysis supports theory building by providing higher fidelity descriptions of phenomena. Given an effect size estimate, meta-analytic methods provide a method for quantifying the amount variability around this point estimate. Furthermore, the quantitative framework allows researchers to measure potential moderators in effect size. This ability is crucial for developmental phenomena because building a theory requires a precise description of changes in effect size across development. Individual papers typically describe an effect size for 1-2 age groups, but the ultimate goal for the theorist is to detect a moderator---age---in this effect. Given that moderators typically require more power to detect than main effects [@button2013power], it may be quite difficult to estimate effect size from individual papers. By aggregating across papers using meta-analytic methods, however, we may be better able to detect these changes, leading to more precise description of the empirical phenomena.

Finally, effect size estimates provide a common language for comparing across phenomena, which facilitates building system-level theories of phenomena.  In the current work, this common language allows us to  consider the relationship between different phenomena in the language acquisition domain ("meta-meta-analysis"). Through cross-phenomenon comparisons, we can understand not only the trajectory of a particular phenomenon, such as word learning, but also how the trajectory of each phenomenon might relate to other skills, such as sound learning, gaze following, and many others. This more holistic description of the empirical landscape can inform theories about the extent to which there is interdependence between the acquisition of different linguistic skills.

Although the meta-analytic-driven method of theory building can be applied to any literature, the method is particularly useful in the case of language acquisition research. One reason is that developmental studies may be uniquely vulnerable to false findings because collecting data from children is expensive, and thus sample sizes are often small and studies are underpowered. In addition, the high cost and practical difficulties associated with collecting large developmental datasets means that replications are relatively rare in the field. Meta-analysis provides a method for addressing these issues by harnessing existing data to estimate effect sizes and developmental trends.

We take as our ultimate goal a broad theory of language acquisition that can explain and predict the range of linguistic skills a child acquires. As a first step toward this end, we collected a dataset of effect sizes in the language acquisition literature across 12 phenomena (Metalab; [ http://metalab.stanford.edu](http://metalab.stanford.edu); see Table 1 for description of phenomena). We use this dataset to demonstrate how meta-analysis supports building theory in two ways. We first use meta-analytic techniques to evaluate the evidential value of the empirical landscape in language acquisition research. We find broadly that this literature has strong evidential value, and thus that the effects reported in the literature should constrain our theorizing of language acquisition. We then turn toward the task of synthesizing these findings across phenomena and offer a preliminary, quantitative synthesis.


```{r, ma metadata}
id_meta <- filter(synthesis_meta_data, short_name == "idspref")
ph_meta <- filter(synthesis_meta_data, short_name == "phonotactics")
vn_meta <- filter(synthesis_meta_data, short_name == "inphondb-native")
vx_meta <- filter(synthesis_meta_data, short_name == "inphondb-nonnative")
sd_meta <- filter(synthesis_meta_data, short_name == "sounds")
wd_meta <- filter(synthesis_meta_data, short_name == "inworddb")
mx_meta <- filter(synthesis_meta_data, short_name == "mutex")
ss_meta <- filter(synthesis_meta_data, short_name == "symbolism")
la_meta <- filter(synthesis_meta_data, short_name == "labadv")
wr_meta <- filter(synthesis_meta_data, short_name == "word_recognition")
gf_meta <- filter(synthesis_meta_data, short_name == "gaze_following")
pc_meta <- filter(synthesis_meta_data, short_name == "pointing_concurrent")
```

\renewcommand{\arraystretch}{1.5}
\begin{table}[h!]
	\footnotesize
	\begin{tabular}{lp{4cm} p{5cm}r}
		\toprule
		\textbf{Level} & \textbf{Phenomenon} & \textbf{Description} & \textbf{N papers (conditions)} \\
		\midrule
		
		Prosody       & IDS  preference  \newline  {\scriptsize (Dunst, Gorman, \& Hamby, 2012)} & 
	              	{\scriptsize  `r id_meta[,"short_desc"][[1]][1]`} & `r id_meta[,"num_papers"][[1]][1]` 
		              (`r id_meta[,"num_experiments"][[1]][1]`) \\
		Sounds        & Phonotactic learning  \newline {\scriptsize (Cristia, 2017)} & 
	              	{\scriptsize  `r ph_meta[,"short_desc"][[1]][1]`} & `r ph_meta[,"num_papers"][[1]][1]` 
		              (`r ph_meta[,"num_experiments"][[1]][1]`) \\
		~             & Vowel discrimination (native) \newline {\scriptsize (Tsuji \& Cristia, 2014)} & 
		              {\scriptsize  `r vn_meta[,"short_desc"][[1]][1]`} & `r vn_meta[,"num_papers"][[1]][1]` 
		              (`r vn_meta[,"num_experiments"][[1]][1]`)   \\ 
		~             & Vowel discrimination (non-native) \newline {\scriptsize (Tsuji \& Cristia, 2014)} & 
		              {\scriptsize  `r vx_meta[,"short_desc"][[1]][1]`} & `r vx_meta[,"num_papers"][[1]][1]` 
		              (`r vx_meta[,"num_experiments"][[1]][1]`)  \\
		~             & Statistical sound learning  \newline {\scriptsize (Cristia, 2017)} & 
		              {\scriptsize  `r sd_meta[,"short_desc"][[1]][1]`} & `r sd_meta[,"num_papers"][[1]][1]` 
		              (`r sd_meta[,"num_experiments"][[1]][1]`)  \\                         
		~             & Word segmentation \newline {\scriptsize  (Bergmann \& Cristia, 2015) } & 
		              {\scriptsize  `r wd_meta[,"short_desc"][[1]][1]`} & `r wd_meta[,"num_papers"][[1]][1]` 
		              (`r wd_meta[,"num_experiments"][[1]][1]`)  \\                     
	 Words          & Mutual exclusivity \newline {\scriptsize (Lewis \& Frank, in prep.)} & 
		              {\scriptsize  `r mx_meta[,"short_desc"][[1]][1]`} & `r mx_meta[,"num_papers"][[1]][1]` 
		              (`r mx_meta[,"num_experiments"][[1]][1]`)  \\  
		~             & Sound Symbolism \newline {\scriptsize (Lammertink et al., 2016)} &
                  {\scriptsize  `r ss_meta[,"short_desc"][[1]][1]`} & `r ss_meta[,"num_papers"][[1]][1]` 
		              (`r ss_meta[,"num_experiments"][[1]][1]`)  \\  
		~             & Concept-label advantage   \newline {\scriptsize (Lewis \& Long, unpublished)} &    
		              {\scriptsize  `r la_meta[,"short_desc"][[1]][1]`} & `r la_meta[,"num_papers"][[1]][1]` 
		              (`r la_meta[,"num_experiments"][[1]][1]`)  \\                   
		~             & Online word recognition \newline {\scriptsize (Frank, Lewis, \& MacDonald, 2016)} &                               {\scriptsize  `r wr_meta[,"short_desc"][[1]][1]`} & `r wr_meta[,"num_papers"][[1]][1]` 
		              (`r wr_meta[,"num_experiments"][[1]][1]`)  \\ \\
 Communication    & Gaze following   \newline {\scriptsize  (Frank, Lewis, \& MacDonald, 2016)}       &                               {\scriptsize  `r gf_meta[,"short_desc"][[1]][1]`} & `r gf_meta[,"num_papers"][[1]][1]` 
		              (`r gf_meta[,"num_experiments"][[1]][1]`)  \\   \\
	  ~             & Pointing and vocabulary  \newline {\scriptsize (Colonnesi et al., 2010)} & 
		              {\scriptsize  `r pc_meta[,"short_desc"][[1]][1]`} & `r pc_meta[,"num_papers"][[1]][1]` 
		              (`r pc_meta[,"num_experiments"][[1]][1]`)  \\
		\bottomrule
	\end{tabular}
	\caption{Overview of meta-analyses in dataset.}
\end{table}
	

# The evidential value of the field
To assess the evidential value of language acquisition research, we conducted several diagnostic analyses: Meta-analytic estimates of effect size, fail-safe-Ns [@orwin1983fail], funnel plots, and p-curves [@simonsohn2014p; @simonsohn2014power; @simonsohn2015better]. These analytical approaches each have limitations, but taken together, they provide converging evidence about whether an effect is likely to exist, and the extent to which publication bias and other questionable research practices are present in the literature. Overall, we find most phenomena in the language acquisition literature have evidential value, and can therefore provide the basis for theoretical development. We also find evidence for some bias, as well as evidence that one experimental paradigm---phonotactic learning---leads to null or near-null effects.

## Meta-Analytic Effect Size
```{r, overall_d}
all_ds <- synthesis_all_data %>%
  split(.$short_name) %>%
  map_df(function(ma_data) overall_es(ma_data, multilevel = TRUE)) %>%
  mutate_at(c("overall.d", "ci_lower", "ci_upper"), 
            funs(formatC(round(., 2), format = 'f', digits = 2))) %>%
  mutate(d_string = paste0(overall.d, " [", ci_lower, ", ",ci_upper, "]"),
         short_name = dataset)
```

To estimate the overall effect size of a literature, effect sizes are pooled across papers to obtain a single meta-analytic estimate. This meta-analytic effect-size can be thought of as the "best estimate" of the effect size for a phenomenon given all the available data in the literature. Table 2, column 2 presents meta-analytic effect size estimates for each of our phenomena. We find evidence for a non-zero effect size in 11 out of 12 of the phenomena in our dataset. In the case of phonotactic learning, however, we find that the meta-analytic effect size estimate does not differ from zero, indicating that this literature does not describe robust effects (as first reported in Cristia, 2017). 

We next turn to methods of assessing evidential value that describe the degree to which a literature has evidential value, and thus the degree to which it should constrain our theory building. In the following three analyses---fail-safe-N, funnel plots, and p-curves---we attempt to quantify the evidential value of these literatures.

\begin{table}[t]
\footnotesize
\begin{tabular}{lrrrr}
\toprule
\textbf{Phenomenon} & \textbf{\textit{d}} & \textbf{fail-safe-N} & \textbf{funnel skew} & \textbf{p-curve skew}\\
\midrule
IDS preference & 0.74 [0.47, 1.01] & 3507 & 1.50 & -10.40*\\
Phonotactic learning & 0.12 [-0.02, 0.25] & 45 & -1.36 & -1.52\\
Vowel discrim.\ (native) & 0.64 [0.48, 0.80] & 8866 & 10.30* & -9.96*\\
Vowel discrim.\ (non-native) & 0.89 [0.35, 1.43] & 3393 & 8.19* & -8.89*\\
Statistical sound learning & 0.29 [0.01, 0.57] & 465 & -0.13 & -2.96*\\
Word segmentation & 0.16 [0.11, 0.21] & 5326 & 2.90* & -9.40*\\
Mutual exclusivity & 0.77 [0.52, 1.03] & 6443 & 10.86* & -12.87*\\
Sound symbolism & 0.23 [0.01, 0.45] & 526 & 0.78 & -5.56*\\
Concept-label advantage & 0.38 [0.23, 0.53] & 2337 & 2.80* & -4.79*\\
Online word recognition & 1.12 [0.60, 1.65] & 1934 & 4.47* & -14.51*\\
Gaze following & 1.06 [0.71, 1.42] & 4277 & 6.11* & -18.66*\\
Pointing and vocabulary & 0.98 [0.62, 1.34] & 1617 & 1.25 & -6.33*\\
\bottomrule
\end{tabular}
\caption{Summary of replicability analyses. \textit{d} = Effect size (Cohen's {\it d}) estimated from a random-effect model; fail-safe-N = number of missing studies that would have to exist in order for the overall effect size to be {\it d} = 0; funnel skew = test of asymmetry in funnel plot using the random-effect Egger's test (Sterne \& Egger, 2005); p-curve skew = test of the right skew of the p-curve using the Stouffer method (Simonsohn, Simmons, \& Nelson, 2015). Brackets give 95\% confidence intervals and asterisks indicate {\it p} < .05.}
\end{table}


## Fail-safe-N
```{r, fail_safe_N}
CRIT_ES_VALUE <- .01 # must be > 0 

fsn.package.data <- synthesis_all_data %>%
  nest(-dataset, .key = "ma_data") %>%
  mutate(dataset = as.character(dataset)) %>%
  pmap_df(get_fail_safe_N, CRIT_ES_VALUE) %>%
  left_join(synthesis_meta_data %>% select(name, short_name), by = c("dataset" = "name"))
```

One approach for quantifying the reliability of a literature is to ask, How many missing studies with null effects would have to exist in the "file drawer" in order for the overall effect size to be zero? This is called the "fail-safe" number of studies [@orwin1983fail]. This number provides an  estimate of the size and variance of an effect using the intuitive unit of number of studies. To calculate this effect, we estimated the overall effect size for each phenomenon (Table 2, column 2), and then used this value to estimate the fail-safe-N (Table 2, column 3). 

Because of the large number of positive studies in many of the meta-analyses we assessed, this analysis suggests a very large number of studies would have to be "missing" in each literature ($M$ = `r format(round(mean(fsn.package.data$fsn_string[is.finite(fsn.package.data$fsn_string)])), big.mark=",", scientific=FALSE)`) in order for the overall effect sizes to be 0. Thus, while it is possible that some reporting bias is present in the literature, the overall large fail-safe-N suggests that the literature nonetheless likely describes robust effects. 

This analysis provides a quantitative estimate of the size of an effect in an intuitive unit, but it does not assess analytical or publication bias [@scargle1999publication]. Importantly, if experimenters are selectively reporting results, then the number and magnitude of observed true effects in the literature may be greatly inflated. In the next analysis, we assess the presence of bias through funnel plots.

## Funnel Plots 

```{r,  fig.pos = 'T!', fig.width=8, fig.height=5.5, fig.cap = "Funnel plots for each meta-analysis. Each effect size estimate is represented by a point, and the mean effect size is shown as a red dashed line. The grey dashed line shows an effect size of zero. The funnel corresponds to a 95%  CI around this mean (with zero at the top). In the absence of true heterogeneity in effect sizes (no moderators) and bias, we should expect all points to be equally distributed within the funnel."}

CRIT_95 <- 1.96 

funnel.es.data <- synthesis_all_data %>%
    mutate(dataset = fct_relevel(dataset, 
                                  "Infant directed speech preference","Phonotactic learning", "Vowel discrimination (native)", 
                                 "Vowel discrimination (non-native)","Statistical sound category learning", "Word segmentation",
                                 "Mutual exclusivity", "Sound symbolism", "Label advantage in concept learning","Online word recognition",
                                 "Gaze following",  "Pointing and vocabulary"),
           dataset = fct_recode(dataset,
                                "IDS preference" = "Infant directed speech preference",
                                "Statistical sound learning" = "Statistical sound category learning", 
                                "Concept-label advantage" = "Label advantage in concept learning",
                                "Vowel discrimination\n(native)" = "Vowel discrimination (native)",
                                "Vowel discrimination\n(non-native)" = "Vowel discrimination (non-native)")) %>%
  group_by(dataset) %>%
  mutate(se = sqrt(d_var_calc), 
         es = d_calc, 
         center = mean(d_calc), 
         lower_lim = max(se) + .05 * max(se))


# separate df for 95 CI funnel shape
funnel95.data.wide <- funnel.es.data %>%
                select(center, lower_lim, dataset) %>%
                group_by(dataset) %>%
                summarise(x1 = (center-lower_lim * CRIT_95)[1], 
                          x2 = center[1],
                          x3 = center[1] + lower_lim[1] * CRIT_95,
                          y1 = -lower_lim[1],
                          y2 =  0, 
                          y3 = -lower_lim[1]) 

funnel95.data.x <- funnel95.data.wide  %>%
                  select(dataset,  dplyr::contains("x")) %>%
                  gather("coordx", "x", 2:4) %>%
                  arrange(dataset, coordx) %>%
                  select(-coordx)

funnel95.data.y <- funnel95.data.wide  %>%
                  select(dataset, dplyr::contains("y")) %>%
                  gather("coordy", "y", 2:4) %>%
                  arrange(dataset, coordy) %>%
                  select(-coordy)

funnel95.data <- bind_cols(funnel95.data.x, funnel95.data.y)

ggplot(funnel.es.data, aes(x = es, y = -se)) +
  facet_wrap(~dataset, scales = "free") +
  xlab("Effect Size")  +
  ylab("Standard Error\n")  +
  scale_colour_solarized(name = "") +
  geom_polygon(aes(x = x, y = y), 
               data = funnel95.data,
               fill = "white") +
  geom_vline(aes(xintercept=x2), 
             linetype = "dashed", color = "red", size = .8, data = funnel95.data.wide) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "grey44",  size = .8) +
  scale_y_continuous(labels = function(x){abs(x)}) +
  geom_point(size = .5) +
  theme(panel.grid.major = element_line(colour = "grey", size = 0.2),
        panel.grid.minor = element_line(colour = "grey", size = 0.5),
        strip.text.x = element_text(size = 9),
        strip.background = element_rect(fill="grey")) 
```


Funnel plots provide a visual method for evaluating whether variability in effect sizes is due  only to differences in sample size. A funnel plot shows effect sizes versus a metric of sample size, standard error. If there is no bias in a literature, we should expect studies to be randomly sampled around the mean, with more variability for less precise studies. 

Figure 1 presents funnel plots for each of our `r nrow(synthesis_meta_data)` meta-analyses. These plots show evidence of asymmetry (bias) for many of our phenomena (Table 2, column 4). An important limitation of this method, however, is that it is difficult to determine the source of this bias. One possibility is that this asymmetry reflects true heterogeneity in phenomena affecting both effect size and precision (e.g., a test of older infants may require fewer participants because their performance is more stable and yields a larger effect).\footnote{The role of moderators such as age can be interactively explored on the Metalab website (http://metalab.stanford.edu).} P-curve analyses provide one method for addressing this issue, which we turn to next.

```{r, funnel_skew}
eggers.data <- synthesis_all_data %>%
  ungroup() %>%
  split(.$short_name) %>%
  map_df(function(ma_data) eggers_tests(ma_data)) %>%
  mutate(egg.random.z = formatC(round(egg.random.z, 2), 
                                format='f', digits=2),
         egg.random.p = round(egg.random.p, digits = 2),
         egg_string = ifelse(egg.random.p < .05, 
                             paste0(egg.random.z, "*"),paste0(egg.random.z, " ")))  %>%
  rename(short_name = dataset) %>%
  select(short_name, egg_string)
```

```{r get_pcurves}
ALPHA <- .05
P_INCREMENT <- .01 

pc.data <- get_all_pc_data(synthesis_all_data, ALPHA, P_INCREMENT, transform = TRUE) 
# transform flag determines whether p-values are calculated from descriptive statistics 

p.source <- pc.data %>%
  select(f.transform, f.value, dataset, study_ID, p_round) %>%
  group_by(dataset) %>%
  summarise(n.total = n(),
            n.transform = length(which(!is.na(f.transform))),
            sig.p = length(which(p_round < ALPHA))) %>%
  mutate(dataset = fct_relevel(dataset,
                               "Infant directed speech preference",
                               "Phonotactic learning", 
                               "Vowel discrimination (native)",
                               "Vowel discrimination (non-native)",
                               "Statistical sound category learning", 
                               "Word segmentation", 
                               "Mutual exclusivity",
                               "Sound symbolism", 
                               "Label advantage in concept learning",
                               "Online word recognition",
                               "Gaze following",  
                               "Pointing and vocabulary"),
         dataset = fct_recode(dataset,
                              "IDS preference" = "Infant directed speech preference",
                              "Statistical sound learning" = "Statistical sound category learning", 
                              "Concept-label advantage" = "Label advantage in concept learning", 
                              "Vowel discrimination\n(native)" = "Vowel discrimination (native)",
                              "Vowel discrimination\n(non-native)" = "Vowel discrimination (non-native)"))  %>%
  mutate(stat_only = ifelse(n.total > n.transform, 1, 0),
         prop.ts = 1-n.transform/n.total,
         prop.ts.string = formatC(round(prop.ts, 2), 
                                format='f', digits = 2)) %>%
  arrange(-stat_only) %>%
  as.data.frame()
```

```{r pcurve_CIS}
ci.data <- pc.data %>%
  split(.$dataset) %>%
  map_df(function(data) get_all_CIS_multi(data, ALPHA, P_INCREMENT)) %>%
  mutate(dataset = fct_recode(dataset,"IDS preference" = "Infant directed speech preference",
                                   "Statistical sound learning" = "Statistical sound category learning", 
                                   "Concept-label advantage" = "Label advantage in concept learning",
                                   "Vowel discrimination\n(native)" = "Vowel discrimination (native)",
                                   "Vowel discrimination\n(non-native)" = "Vowel discrimination (non-native)"))
```

## P-curves

A p-curve is the distribution of p-values for the statistical test of the main hypothesis across a literature [@simonsohn2014p; @simonsohn2014power; @simonsohn2015better]. Critically, if there is a robust effect in the literature, the shape of the p-curve should reflect this. In particular, we should expect the p-curve to be right-skewed with more small values (e.g., .01) than large values (e.g., .04). An important property of this analysis is that we should expect this skew independent of any true heterogeneity in the data, such as age. Evidence that the curve is in fact right-skewed would suggest that the literature is not biased, and that it provides evidential value for theory building. 

P-values for each condition were calculated based on the reported test statistic. However, test statistics were not available for many conditions, either because they were not reported or because they were not coded. To remedy this, we also calculated p-values indirectly based on descriptive statistics (means and standard deviations; see SI for details). 

```{r, fig.pos = "t!",  fig.width = 7.5, fig.height = 6, fig.cap = "P-curve for each meta-analysis (Simonsohn, Nelson, & Simmons, 2014). In the absence of p-hacking, we should expect the observed p-curve (blue) to be right-skewed (more small values). The red dashed line shows the expected distribution of p-values when the effect is non-existent (the null is true). The green dashed line shows the expected distribution if the effect is real, but studies only have 33% power. Grey ribbons show 95% confidence intervals estimated from a multinomial distribution. Text on each plot shows the number of p-values for each dataset that are less than .05 and thus are represented in each p-curve (\"sig.\ ps\"), relative to the total number of conditions for that phenomenon. Each plot also shows the proportion of p-values that were derived from test statistics reported in the paper (\"prop.\ test stat.\"); all others were derived by conducting analyses on the descriptive statistics or transforming reported effect sizes."}

pc_plot_data <- pc.data %>%
  group_by(dataset) %>%
  do(get_p_curve_df(., ALPHA, P_INCREMENT)) %>%
  ungroup() %>%
  mutate(dataset = fct_relevel(dataset, "Infant directed speech preference", "Phonotactic learning",
                               "Vowel discrimination (native)", "Vowel discrimination (non-native)",
                               "Statistical sound category learning", "Word segmentation", "Mutual exclusivity", 
                               "Sound symbolism", "Label advantage in concept learning",
                               "Online word recognition", "Gaze following",  "Pointing and vocabulary"),
         dataset = fct_recode(dataset,"IDS preference" = "Infant directed speech preference",
                                   "Statistical sound learning" = "Statistical sound category learning", 
                                   "Concept-label advantage" = "Label advantage in concept learning",
                                   "Vowel discrimination\n(native)" = "Vowel discrimination (native)",
                                   "Vowel discrimination\n(non-native)" = "Vowel discrimination (non-native)")) 

ggplot(pc_plot_data) + 
  facet_wrap(~ dataset, nrow = 3) +
  geom_ribbon(aes(ymin = ci.lower, ymax = ci.upper, x = p), 
              fill = "grey87", data = ci.data) +
  geom_line(size = 1, aes(x = p, y = value, linetype = measure,
                          color = measure)) +
  scale_colour_manual(name = "", values = c("red", "green", "blue"), 
                      labels=c("Null of no effect", 
                               "Null of 33% power", "Observed")) +
  scale_linetype_manual(values = c("dashed", "dashed", "solid"), 
                        guide = FALSE)  +
  ylab("Proportion p-values\n") +
  xlab("p-value") +
  geom_text(aes(label = paste(sig.p, "sig. ps /", 
                              n.total, "conditions\nprop. test stat. = ", prop.ts.string),
                x = .021, y = .8), data = p.source, 
            colour = "black", size = 2, hjust = 0) +
  theme_bw() + 
  theme(legend.position = "top",
        legend.key = element_blank(),
        legend.background = element_rect(fill = "transparent"),
        strip.text.x = element_text(size = 9),
        axis.title = element_text(colour = "black", size = 12),
        panel.margin = unit(.65, "lines"),
        strip.background = element_rect(fill = "grey"))
```

Figure 2 shows p-curves for each of our `r nrow(synthesis_meta_data)` meta-analyses. All p-curves show evidence of right skew, except for phonotactic learning (Table 2, column 5). This pattern did not differ when only reported test-statistics were used to calculate p-curves (see SI).

```{r, p_curve_skew}
stouffer.data <- pc.data %>%
  group_by(dataset) %>%
  do(data.frame(stouffer = stouffer_test(., ALPHA))) %>%
  filter(stouffer.pp.measure == "ppr.full") %>%
  full_join(synthesis_meta_data %>% select(name, short_name),
            by = c("dataset" = "name")) %>%
  select(short_name,stouffer.Z.pp, stouffer.p.Z.pp) %>%
  mutate_at(funs(formatC(round(., 2), format = 'f', digits = 2)), 
            .vars = c("stouffer.p.Z.pp", "stouffer.Z.pp")) %>%
  mutate(stouff_string = ifelse(is.na(as.character(stouffer.Z.pp)), "",
                                ifelse(stouffer.p.Z.pp < .05, 
                                       paste0(as.character(stouffer.Z.pp), "*"), paste0(as.character(stouffer.Z.pp), " ")))) %>%
  select(short_name, stouff_string)
```


```{r, make_table}
table_data <- left_join(select(all_ds, c(short_name, d_string)), fsn.package.data) %>%
              left_join(eggers.data) %>%
              left_join(stouffer.data) %>%
              select(dataset, dplyr::contains("string")) %>%
              ungroup() %>%
              .[c(2,8,3,4,10,5,7,11,6,12,1,9),]  %>% # reorder rows 
              mutate(dataset = fct_recode(dataset,
                                         "IDS preference" = "Infant directed speech preference",
                                         "Statistical sound learning" = "Statistical sound category learning", 
                                         "Concept-label advantage" = "Label advantage in concept learning",
                                         "Vowel discrim.\ (native)" = "Vowel discrimination (native)",
                                         "Vowel discrim.\ (non-native)" = "Vowel discrimination (non-native)")) 

# paste above (so can edit latex - remove \addlinespace, fix header and row formatting):
# kable(table_data, format = "latex", booktabs = TRUE) 
```

In sum, then, meta-analytic methods, along with our dataset of effect sizes, provide an opportunity to assess the replicability of the field of language acquisition. Across a range of analyses, we find that this literature shows some evidence for bias, but overall, it is quite robust.

# Quantitative Synthesis and Theory Development

Next, we turn to how these data can be used to constrain and develop theories of language acquisition.

Meta-analytic methods provide a precise, quantitative description of the developmental trajectory of individual phenomena. Figure 3 presents the developmental trajectories of the phenomena in our dataset at each level in the linguistic hierarchy. By describing how effect sizes change as a function of age,  we can begin to understand what factors might moderate the trajectory, such as aspects of a child's experience or maturation.  For example, the meta-analysis on mutual exclusivity [the bias for children to select a novel object, given a novel word; @markman1988] suggests a steep developmental trajectory of this skill. We then can use these data to build quantitative models to understand how aspects of experience (e.g., vocabulary development) or maturational constraints may be related to this trajectory [e.g., @frank2009using; @mcmurray2012word]. 

```{r, level.plot.data}
# make levels df
ld.df <- data.frame(dataset = synthesis_meta_data$name,
                    domain = c("Prosody", "Words", "Communication", "Sounds",
                              "Sounds", "Sounds", "Sounds", "Sounds", "Words",
                              "Words", "Communication", "Words"))

ld.df$domain = factor(ld.df$domain, levels = c("Prosody","Sounds", "Words", "Communication"))

single_method_datasets = synthesis_all_data %>%
  group_by(dataset) %>%
  summarise(n_methods = length(levels(as.factor(method)))) %>%
  filter(n_methods == 1) %>%
  .[["dataset"]]

# get model fits
all_data.resid <- data.frame()
for (i in 1:length(synthesis_meta_data$name)) {
    d <- filter(synthesis_all_data, dataset == synthesis_meta_data$name[i])
    if (synthesis_meta_data$name[i] %in% single_method_datasets) {
      full.model = rma(d_calc, vi = d_var_calc, data = d)
    } else {
      full.model = rma(d_calc ~ method, vi = d_var_calc, data = d)
    }
  
  d <- as.data.frame(rstandard(full.model)$resid) %>%
       cbind(d) %>%
       rename(residual.d = `rstandard(full.model)$resid`) %>%
       mutate(residual.d = residual.d + full.model$b[1]) %>% # add in intercept term
       inner_join(synthesis_all_data) 
  
  all_data.resid <- rbind(all_data.resid,d)
}

# merge in levels
residualized.es <- all_data.resid %>%
          left_join(ld.df) %>%
          mutate(age.years = mean_age/365) %>%
          dplyr::filter(dataset != "Phonotactic learning",
                 age.years < 3)  %>%
          mutate(dataset = droplevels.factor(dataset))
```

```{r, eval = F}
# Figs. 3 and 4 are edited in illustrator for the final version
fs <- 16

level.plot <- ggplot(residualized.es, aes(x = age.years, y = residual.d, col = dataset)) +
  facet_grid(~ domain) +
  geom_point(aes(size = n), alpha = .1, 
             data = filter(residualized.es, residual.d > -.5 & residual.d < 2.5 )) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_line(stat="smooth", method="lm", se = FALSE, size = 1, formula = y ~ log(x)) +
  coord_cartesian(ylim = c(-.5, 2.5), xlim = c(0, 3.1)) +  # doesn't remove data from geom_smooth
  xlab("Age (years)") +
  ylab("Method-residualized\n effect size") +
  theme_bw() +
  scale_color_solarized() +
  theme(legend.position = "none",
        legend.key = element_blank(),
        #axis.line= element_line(size = .6),
        axis.text = element_text(colour = "black", size = fs),
        strip.text.x = element_text(size = 14),
        axis.title = element_text(colour = "black", size = fs),
        strip.background = element_rect(fill="grey"))

pdf("figs/fig3.pdf", width=8, height=3.5)
level.plot
dev.off()

#legend text (for illustrator)
legend.desc1 <- residualized.es %>%
  group_by(dataset) %>%
  slice(1) %>%
  ungroup() %>%
  mutate(x = 1, y = seq(1,44,4)) %>%
  select(dataset,x,y) %>%
  ggplot(aes(x = x, y = y, col = dataset)) +
  geom_text(aes(label = dataset)) +
  #geom_point() +
  theme_bw() +
  scale_color_solarized() +
  theme(legend.position = "none",
        legend.key = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank())

pdf("figs/fig3legend.pdf")
legend.desc1
dev.off()
```

In addition, meta-analytic methods provide an approach for synthesizing across different linguistic skills via the common metric of effect sizes. The ultimate goal is to use meta-analytic data to build a single, quantitative model of the language acquisition system, much like those developed for individual language acquisition phenomena, like word learning. Developing a single quantitative model is a lofty goal, however, and will likely require much more precise description of the phenomena than is available in our dataset. Nevertheless, we can use our data to distinguish between broad classes of theories about the interdependency of skills.

```{r, fig.width=8, fig.height=3.5,  fig.pos = 'T!', fig.cap = "Method-residualized effect size plotted as a function of age across the 11 meta-analyses in our dataset shown to have evidential value (excluding phonotactic learning). Lines show logarithmic model fits. Each point corresponds to a condition, with the size of the point indicating the number of participants."}

include_graphics("figs/fig3_lab.pdf")
```

We first consider two broad theories of learning dependencies that have been articulated in a number of forms, and which we will separate for the purposes of exposition. At one extreme, there is the stage-like theory which posits that linguistic skills are acquired in a strictly sequential manner, beginning with skills at the lowest level of the linguistic hierarchy and working their way up. Under this theory, once a skill is mastered, and only then, can it be used to support the acquisition of skills higher in the linguistic hierarchy. In this way, a child sequentially acquires the skills of language, "bootstrapping" from existing knowledge at lower levels to new knowledge at higher levels. Consistent with this view, there is evidence that the general prosodic and rhythmic characteristics of language are accessible to infants early in development [@ramus2000language], and that these characteristics support the acquisition of sound categories [e.g., @werker2007infant], word boundaries [e.g., @jusczyk1999beginnings], grammatical categories [e.g., @shi1999newborn], and even word learning [e.g., @shukla2011prosody]. 

```{r, synthesis.plot.data, eval = F}
## real data
all_data.f <- residualized.es %>%
  select(age.years, dataset, residual.d, n) %>%
  mutate(model = "Observed")

## simulated data
# bottom-up
x <- seq(0, 3, .01)
slope <- 1.1
y1 <- ifelse((log(x - 1) * slope) < 0, NA, log(x - 1) * slope)
y2 <- ifelse((log(x) * slope) < 0, NA, log(x) * slope)
y3 <- ifelse((log(x + 1) * slope) < 0, NA, log(x + 1) * slope)

# interactive 
y4 <- log((x + 1)) * .2
y5 <- log((x + 1)) * .5
y6 <- log((x + 1)) * 1.1

# ad hoc 
y7 <- .2 * x
y8 <- dnorm(x, mean = 1.7, sd = .5) 
y9 <- log(x + .0001, base = .02) + .3
y9[1 <- 1.47 

# merge together simulated data
simulated.development <- data.frame(age.years = c(x,x,x,x,x,x,x,x,x),
                                   residual.d = c(y3, y2, y1, y6, y5, y4, y9, y8, y7),
                                   dataset = c(rep(1:3, each = length(x)), rep(1:3, each = length(x)),
                                               rep(1:3, each = length(x))),
                                   model = rep(c("Sequential", "Simultaneous", "Ad hoc"), 
                                               each = length(x) * 3),
                                   n = 1)

## merge together simulated and real
simulated.development.all <- mutate(simulated.development, dataset = as.factor(dataset)) %>%
  rbind(all_data.f) %>%
  mutate(model = ordered(model, levels = c("Sequential", "Simultaneous", "Ad hoc", "Observed")),
         level = ifelse(model != "Observed", dataset, 
                        ifelse(dataset == "Infant directed speech preference", 1,
                               ifelse(dataset == "Pointing and vocabulary" | dataset == "Gaze following", 4, 
                                      ifelse(dataset == "Word segmentation" | dataset == "Vowel discrimination (native)" | dataset == "Vowel discrimination (non-native)", 2, 3)))))

real.levels <- levels(simulated.development.all$dataset)
fake.levels <- c("1", "2", "3") 

simulated.development.all <- simulated.development.all %>%
  mutate(dataset.label = as.factor(dataset)) %>%
  mutate(dataset.label = plyr::mapvalues(dataset.label, from = c("1", "2", "3", "Gaze following",  "Infant directed speech preference", "Label advantage in concept learning", "Mutual exclusivity", "Vowel discrimination (native)", "Vowel discrimination (non-native)", "Sound symbolism" , "Online word recognition" , "Word segmentation", "Pointing and vocabulary", "Statistical sound category learning"), to = c("","","", "GF", "IDS", "LA", "ME", "VD-N", "VD-NN", "SS", "WR", "WS", "PV", "SC")))
```

```{r, eval = F}
## plot
synthesis.plot <- ggplot(simulated.development.all,
       aes(x = age.years, y = residual.d, col = dataset)) + 
  geom_line(data = filter(simulated.development.all, model != "Observed"), 
            size = 1) +
  geom_line(stat="smooth", 
            data = filter(simulated.development.all, model == "Observed"), 
            size = 1,
            method = "lm",
            formula = y~log(x),
            se = FALSE) + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_cartesian(ylim = c(-.5, 2.5), xlim = c(0, 3.1)) +  # doesn't remove data from geom_smooth
  facet_grid(.~model) +
  scale_colour_manual(values=c( "grey70", "grey35", "grey20", "#d33682", 
                                "#dc322f" ,"#cb4b16", "#b58900", "#859900" ,"#2aa198",
                                "#268bd2" ,"#6c71c4", "#993399" ,"#268bd2", "#657b83"),
                      breaks = real.levels[real.levels!=fake.levels]) +
  xlab("Age (years)") +
  ylab("Method-residualized\neffect size") + 
  theme_bw() + 
  theme(legend.position = "none",
        legend.key = element_blank(),
        legend.background = element_rect(fill = "transparent"),
        legend.title = element_blank(),
        #axis.line= element_line(size = 3),
        axis.text = element_text(colour = "black", size = fs),
        strip.text.x = element_text(size = 14),
        axis.title = element_text(colour = "black", size = fs),
        strip.background = element_rect(fill = "grey"))

pdf("figs/fig4.pdf", width = 8, height = 3.5)
synthesis.plot
dev.off()

# legend text (for illustrator)
legend.desc2  <-  simulated.development.all %>%
  filter(model == "Observed") %>%
  group_by(dataset.label) %>%
  slice(1) %>%
  ungroup() %>%
  mutate(x = 1, y = seq(1,22,2)) %>%
  select(dataset.label,x,y) %>%
  ggplot(aes(x = x, y = y, col = dataset.label)) +
  geom_text(aes(label = dataset.label)) +
  theme_bw() +
  scale_color_solarized() +
  theme(legend.position = "none",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank())
### NOTE! I manually changed two colors that were out of order in illustrator (SSL and VW-NN)

pdf("figs/fig4legend.pdf")
legend.desc2
dev.off()
```

```{r, fig.width=8, fig.height=3.5,  fig.pos = 'T!', fig.cap="The left two panels show the developmental trajectories predicted under different meta-theories of language acquisition. The stage-like theory predicts that a child will not begin learning the next skill in the linguistic hierarchy until the previous skill has been mastered. The interactive theory predicts that multiple skills may be simultaneously acquired. The third panel shows other possible developmental trajectories (decreasing, linear, and non-monotonic). The fourth panel shows the observed meta-analytic data. Effect size is plotted as a function of age from 0-3 years, across 11 different phenomena (excluding phonotactic learning). Model fits are the same as in Figure 3. These developmental curves suggest there is interactivity across language skills, rather than stage-like learning of the linguistic hierarchy. GF: Gaze following; IDS: IDS preference; LA: Concept -label advantage; ME: Mutual exclusivity; VD-(N)N: Vowel discrimination (non-)native; PV: Pointing-vocabulary correlations; SS: Sound symbolism; WR: Word recognition; SSL: Statistical sound category learning."}

include_graphics("figs/fig4_lab.pdf")
```

Alternatively, multiple skills may be learned simultaneously across the system regardless of their place in the hierarchy, and potentially with top-down effects. There is evidence that higher-level skills like word learning may be acquired relatively early in development, likely before phonological learning has been completed [e.g., @bergelson2016; @tincoff1999some].  The possibility that higher levels may be learned concurrently or even before lower levels is consistent with predictions of a class of hierarchical Bayesian models that suggest that more abstract knowledge may be acquired quickly, before lower-level information, and may in turn support the acquisition of lower information ["blessing of abstraction," @goodman2011learning]. There is evidence for this proposal from work that suggests word learning supports the acquisition of lower-level information like phonemes [@feldman2013word].

These two theories make different predictions about the relative trajectories of skills across development. Within the meta-analytic framework,  we can represent  these different trajectories schematically by plotting the effect sizes for different skills across development. In particular, the bottom-up theory predicts serial acquisition of skills (Figure 4; left) while the interactive theory predicts simultaneous acquisition (left center). We can also specify many other possible trajectories by varying the functional form and parameters of the model. Figure 4 ("Ad hoc"; right center) shows several other possible trajectories. For example, a skill might have a non-monotonic trajectory, increasing with age, and then decreasing. By specifying the shape of these developmental trajectories and the age at which acquisition begins, we can consider many patterns of developmental trajectories, and how these different patterns, in turn, constrain our broad theories of development. 

Our data allow us to begin to differentiate between this space of theories. Figure 4 (right) presents a synthetic representation of the developmental trajectories of the skills in our dataset with literatures shown to have evidential value (all but  phonotactic learning). We find strong evidence for the simultaneous acquisition of skills---children begin learning even high-level skills, like the meanings of words, early in development, and even low-level skills like sound categories show a protracted period of development. This pattern is less consistent with stage-like theories than with parallel or top-down theories of language acquisition. Notably, however, while there is clearly a degree of simultaneous acquisition across skills, higher-level skills---gaze-following, word recognition, and mutual exclusivity---appear to show a more stage-like pattern of development. In future research, we can use this approach to distinguish between a larger, more nuanced space of meta-theories and, ultimately, refine our way towards a single quantitative theory of language acquisition.

# Discussion
Building a theory of a complex psychological phenomenon requires making good inductive inferences from the available data. Meta-analysis can support this process by providing a toolkit for quantitative description of individual behaviors and their relationship to important moderators (e.g., age, in our case). Here, we apply the  meta-analytic toolkit to the domain of language acquisition---a domain where there are concerns of replicability, and where high-fidelity data are needed for theory building. We find that the existing literature in this domain describes mostly robust phenomena and thus could form the basis of theory development. We then aggregate across phenomena to offer the first quantitative synthesis of the field. We find evidence that linguistic skills are acquired simultaneously rather than in a stage-like fashion.

In this paper, we focused on theoretical motivations for building meta-analysis, but naturally, there are many other practical reasons for conducting a quantitative synthesis. For example, when planning an experiment, an estimate of the size of an effect on the basis of prior literature can inform the sample size needed to achieve a desired level of power. Meta-analytic estimates of effect sizes can also aid in design choices: If a certain paradigm or measure tends to yield overall larger effect sizes than another, the strategic researcher might select this paradigm in order to maximize the power achieved with a given sample size. These and other advantages, illustrated with the same database used here, are explained in Bergmann et al.\ (in press). 

Despite its potential, there are a number of important limitations to the meta-analytic method as a tool for theory building in psychological research. One challenging issue is that in many cases experimental method (e.g., habituation, behavioral choice, etc.) and phenomenon are confounded. This is problematic because a method with less noise than another will produce a bigger effect size for the same phenomenon. As a result, it is difficult to determine the extent to which a difference in effect size between two phenomena is due to an underlying difference in the phenomena, or merely to a difference in the way each was tested. While method may account for some variability in our dataset, we find that method does not have a large impact on effect size for phenomena relative to other moderators like age (see SI). Nevertheless, covariance between method and phenomenon, as found in our dataset and probably many other fields of study, limits our ability to directly compare effect sizes across phenomena.

Second, meta-analysis, like all analytical methods, requires the researcher to make analytical decisions, and these decisions may be subject to the biases of the researcher. We believe that a virtue of the current approach is that we have applied the same analytical method across all phenomena we examined, thus limiting our “degrees of freedom” in the analysis. However, in some cases this uniform approach to data analysis means that we are unable to take into consideration aspects of a particular phenomenon that might be relevant. For example, in a study using the vowel discrimination and word segmentation datasets to adjudicate bottom-up versus top-down theories, Bergmann, Tsuji, and Cristia (2017) concluded this was effectively studied only when subsetting to papers that tested at least two different age groups as a way of focusing on age differences while controlling for other possible differences between experiments. Here, we have followed a different route, by normalizing effect sizes across methods. We believe that the systematic, uniform analytical approach used here is the most appropriate for minimizing bias by the meta-analyst.  Notably, this analytical decision has consequences for interpretation: Bergmann, Tsuji, and Cristia (2017) found a moderate decrease in effect size with age for non-native vowel discrimination, while the current analysis suggests a moderate increase. We thus recommend future researchers to consider this question carefully, particularly in meta-analyses with high heterogeneity.

While meta-analysis uniquely provides a high-level view of the empirical landscape, we consider the meta-analytic method as synergistic with other methodological approaches. Critically, a complete meta-theory of language acquisition would include not just a description of the temporal overlap of different skills, but also a theory of their causal relationship. For example, the interactive theory suggests that skills at higher levels *support* the acquisition at lower levels, even before skills at lower levels are mastered. In the meta-analytic framework, this predicts that there should be simultaneous development of skills across the language hierarchy---as we observe in the current work. Importantly, however, this analysis is inherently correlational, entailing that we cannot directly infer a causal relationship between acquisition at lower levels and acquisition at higher levels. That is, while the observed pattern is consistent with the interactive theory, it is also possible that there is no causal relationship between skills across the language hierarchy, merely parallel trajectories of acquisition. For this reason, experimental work must go hand-in-hand with meta-analysis to address causal questions.

Finally, there are a number of important limitations to the meta-analytic method more broadly. One issue is that the method relies on researchers conducting replications of the same study across a range of ages and, critically, reporting these data so that they can be used in meta-analyses. To the extent that researchers do not conduct these studies, or report the necessary statistics in their write-ups (e.g., means and standard deviations), the meta-analytic method cannot be applied. In addition, the meta-analytic method, as in the case of qualitative forms of synthesis (e.g., literature review), is limited by the potential presence of bias, which can come from a range of sources including non-representative participant populations, failure to publish null findings, and analytical degrees-of-freedom. To the extent these biases are present in the literature, methods of synthesizing these findings will also be biased.

In sum, understanding the psychological mechanisms underlying complex phenomena is a difficult inferential task: The researcher must develop a predictive and explanatory theory on the basis of limited and noisy experimental data. Here we have focused on language acquisition as a case study of how meta-analytic methods can be productively leveraged as a tool for theory building. Meta-analytic methods allow the researcher to  determine whether phenomena are robust, synthesize across contradictory findings, and ultimately, build an integrative theory across phenomena. Moving forward, we see meta-analysis as a powerful tool in the researcher’s toolkit for developing quantitative theories to account for complex psychological phenomena.

# Methods
We analyzed 12 different phenomena in language acquisition. We selected these particular phenomena because of their theoretical importance or because a previously-published meta-analysis already existed.

To obtain estimates of effect size, we either coded or adapted others' coding of papers reporting experimental data (see SI for details). Within each paper, we calculated a separate effect size estimate for each experiment and age group (we refer to each measurement separated by age as a \enquote{condition}). In total, our sample includes estimates from `r sum(synthesis_meta_data$num_papers)` papers, `r sum(synthesis_meta_data$num_experiments)` different conditions and `r format(floor(sum(synthesis_meta_data$num_subjects)), big.mark=",", scientific=FALSE)` participants. The process for selecting papers from the literature differed by domain, with some individual meta-analyses using more systematic approaches than others (see SI for specific search strategies). 

#### Data and Code Availability
The data and code reported in this paper have been deposited in GitHub, a web-based repository hosting service, 
[https://github.com/langcog/metalab/](https://github.com/langcog/metalab/).

#### Supplementary Information
This article contains supporting information online at [http://rpubs.com/mll/synthesis_SI](http://rpubs.com/mll/synthesis_SI)

\newpage

### References 

---
nocite: | 
  @lfprep
  @dunst2012preference
  @frank2016performance
  @bergmann2015development
  @bergmann2017
  @sterne2005regression
  @bergmanneducational
  @lammertink2016
  @lewisunpublished
  @colonnesi2010relation
  @cristia2017
...


```{r create_r-references}

r_refs(file = "metalab_synthesis.bib")

```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{8pt}

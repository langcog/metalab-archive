---
title: "Reproducibility Analyses for Synthesis Paper"
author: "Molly Lewis, Christina Bergmann, and Mike Frank"
output: html_document
---

```{r, setup, include = FALSE}
rm(list=ls())
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE)
ggplot2::theme_set(langcog::theme_mikabr(base_family = "Ubuntu"))
source("../../../dashboard/global.R", chdir = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
library(langcog)
library(metafor)
library(pwr)
library(metafor)
library(knitr)
```
# Average effect size
# get mean power by dataset
```{r}
overall_es <- function(ma_data){
  model = rma(ma_data$d_calc, ma_data$d_var_calc, method = "REML",
               control = list(maxiter = 1000, stepadj = 0.5))
    data.frame(dataset = ma_data$short_name[1],
               overall.d = model$b,
               ci_lower = model$ci.lb,
               ci_upper = model$ci.ub)
}

all_ds = all_data %>%
  split(.$short_name) %>%
  map(function(ma_data) overall_es(ma_data)) %>%
  bind_rows() %>%
  group_by(dataset) %>%
  mutate(overall.d = round(overall.d, digits = 2),
         ci_lower  = round(ci_lower, digits = 2),
         ci_upper = round(ci_upper, digits = 2)) 

kable(all_ds)
```

# P-curves

P-curves have been proposed as one (of many) ways to measure publication biases, because so-called p-hacking (conducting multiple statistical analyses, excluding data points, etc. until a p-value below the significance threshold [typically .05] is reached) can lead to an inflation of high p-values that are nonetheless significant. The exected distribution of p-values in the absence of prevalent p-hacking of a null effect is biased towards smaller values. The present script is based on the p-curve app version 4.0.

One of the main problems with p-curve is the use of text mining to generate input, and the presence of heterogeneous tests (https://peerj.com/articles/1715/). In the case of MetaLab, however, all test statistics were entered by hand and concern a homogeneous research literature (within each dataset). Further, we only enter the *main* test statistic which would also be used to calculate effect sizes in our meta-analysis. Therefore, we believe that p-curve is an appropriate analysis for the dataset. 


## Data preparation

When reported or necessary to calculate effect sizes, we note the t-value or F-score for each record (a single study). We do not consider studies for which this information was unavailable in the following analyses. 

To calculate p-values from $t$-values and $F$-scores, we also need the degrees of freedom. When one group of participants was tested, we use $N-1$, for a two group comparison design we use $N-2$. $F$-scores require two degrees of freedom, the first one however is always one in the main analysis of interest (we do not enter or consider e.g., interactions of the main effect of interest with covariates such as age, gender, etc.).

We need to source code from the p-curve app. This p-curve report is based on the p-curve app 4.0 via code from the [app](http://p-curve.com/app4/App%204.0%20p-curve%202016%2001%2011.r). We have heavily adapted this code, however. 

```{r}
source("pcurve.R")
```

Set global variables
```{r}
ALPHA = .05
P_INCREMENT = .01 
```

Get all data necessary to do p-curve calculations
```{r}
pc.data <- get_all_pc_data(all_data, ALPHA, P_INCREMENT)
```

## P-curve for each individual dataset
```{r}
pc.data %>%
  group_by(dataset) %>%
  do(get_p_curve_df(., ALPHA, P_INCREMENT)) %>%
  ggplot(aes(x = p, y = value, lty = measure, color = measure)) + 
  geom_line(size = .7) +
  scale_colour_manual(values = c("red", "green", "blue")) +
  scale_linetype_manual(values = c("dashed", "dashed", "solid"))  +
  facet_wrap(~ dataset) 
```

## Test for right skew.
We employ inferential statistics to test whether the p-curves are right skewed, i.e. whether they have eveidential value. Following Simonsohn, Simmons and Nelson (2015) we use Stouffer method. This method is less sensitive to outlier, relative to Fischer method (in the 2014a paper).

* full + half
* 33 


```{r}
stouffer.data = pc.data %>%
  group_by(dataset) %>%
  do(data.frame(stouffer = stouffer_test(., ALPHA))) 

ggplot(stouffer.data, aes(x = dataset, y = stouffer.pp.measure, 
                          fill = stouffer.sig)) +
  geom_tile() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(values = c("blue","pink"))
  
```

## Get power, based on p-curve
```{r}
power.data = pc.data %>%
  group_by(dataset) %>%
  do(data.frame(power = get_pc_power(., ALPHA))) 

ggplot(power.data, aes(x = dataset, y = power, fill = dataset)) +
  geom_hline(yintercept = .8, lty = 2) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0,1) +
  theme(legend.position = "none")
```

## Raw power
Just for comparision, let's also get an estimate of the power using pwr.t.test.
```{r}
powers = all_data %>%
  left_join(all_ds) %>%
  select(dataset, short_name, overall.d, n_1, n_2,participant_design) %>%
  rowwise %>%
  mutate(power = unlist(ifelse(participant_design == "within_one",
                         pwr.t.test(d = overall.d, n = n_1, type = "one.sample")["power"],
                            ifelse(participant_design == "within_two",
                            pwr.t.test(d = overall.d, n = n_1, type = "two.sample")["power"],
                            pwr.t.test(d = overall.d, n = sum(n_1, n_2)/2, type = "paired")["power"]))))

powers = powers %>%
  group_by(dataset) %>%
  multi_boot_standard(column = "power", na.rm = T) %>%
  rename(mean_power = mean) %>%
  left_join(powers) %>%
  filter(!is.na(mean_power))
  
ggplot(powers, aes(x = power, fill = short_name)) +
  geom_density() +
  geom_vline(aes(xintercept = mean_power), size = 1) +
  facet_wrap(~short_name, scales =  "free_y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")
```


## Get effect size, based on p-curve.
P-curve effect size dramatically inflates effect size in our set of meta-analyses. This is because an assumption of this calculation is that the test statistic is greater than 0, and thus we inflate the effect size by including only positive effects. For this reason, we are not including P-curve based effect sizes in the paper. 
```{r, eval = F}
pc_es = pc.data %>%
  group_by(dataset) %>%
  do(data.frame(get_pc_es(., ALPHA))) 

obs_es =  all_data %>%
  group_by(dataset) %>%
  summarise(d = summary(rma(d_calc,d_var_calc,
             slab = short_cite, 
             data = .))$b[1],
            ci_lb = summary(rma(d_calc,d_var_calc,
             slab = short_cite, 
             data = .))$ci.lb[1],
            ci_ub = summary(rma(d_calc,d_var_calc,
             slab = short_cite, 
             data = .))$ci.ub[1]) %>%
  mutate(method = "observed")

all_es = rbind(pc_es, obs_es)

ggplot(all_es, aes(y = d, x = dataset, group = method, 
                   color = method)) +
  geom_pointrange(aes(ymin = ci_lb, ymax = ci_ub ), 
                  position = position_dodge(width = .1)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# TO DO:
# check code!!
# ncp parameter for f? Can't interpret effect sizes.
# why are some datasets missing? -> we only have test statistics for 6/11- is there a way to recover this?
```

# Orwin's Fail-Safe-N
From Metafor package documentation: "The Orwin method calculates the number of studies averaging null results that would have to be added to the given set of observed outcomes to reduce the (unweighted) average effect size to a target (unweighted) average effect size. The method is described in Orwin (1983)."

Analysis of different measures of FSN. With no target, package assumes mean of unweighted average. Book and metafor the same. Weighted average of ds (from rma) only slight less. Conclusion: Just use the package, unless we think the weighted average is better.

```{r}
fsn.package.data = all_data %>%
  group_by(dataset) %>%
  summarise(fsn_package_small_T = fsn(d_calc, d_var_calc, data = all_data, 
                         target = .01, type="Orwin")$fsnum,
            fsn_package_noT = fsn(d_calc, d_var_calc, data = all_data, type="Orwin")$fsnum
            ) %>%
  left_join(datasets %>% select(name, short_name), by= c("dataset" = "name"))

fsn.manual.data = all_data %>%
  group_by(short_name) %>%
  summarise(n = n(),
            mean_d = mean(d_calc),
            target =  .01) %>%
  select(short_name, mean_d, n, target)  %>%
  left_join(all_ds %>% rename(short_name = dataset)) %>%
  select(short_name, mean_d, n, target, overall.d) %>%
  mutate(fsn_book1_small_T = round(n * (mean_d /target-1),0),
         fsn_book2_noT = round(n*mean_d, 0),
         fsn_raw_package = round(n * ((mean_d - target)/target),0),
         fsn_book1_MA_small_T  = round(n * (overall.d /target-1),0),
         fsn_book2_MA_noT  = round(n*overall.d, 0),
         fsn_raw_package_MA = round(n * ((overall.d - target)/target),0)) 

fsn.data = left_join(fsn.manual.data, fsn.package.data) %>%
          select(-n, -mean_d, -target, -dataset, -overall.d) %>%
          select(-fsn_raw_package, -fsn_raw_package_MA) %>%
          gather("fsn_type", "fsn", -1)

ggplot(fsn.data, aes(y = fsn, x = short_name, color = fsn_type)) +
  geom_point(stat = "identity") +
  ylab("Fail-Safe-N") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(fsn.data, aes(y = fsn, x = fsn_type, color = short_name)) +
  geom_point() +
  ylab("Fail-Safe-N") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
fsn.package.data = all_data %>%
  group_by(dataset) %>%
  summarise(fsn_package= fsn(d_calc, d_var_calc, data = all_data, 
                         target = .01, type="Orwin")$fsnum) %>%
  left_join(datasets %>% select(name, short_name), by= c("dataset" = "name")) %>%
  select(-dataset)

ggplot(fsn.package.data, aes(y = fsn_package, x = dataset, fill = dataset)) +
  geom_bar(stat = "identity") +
  ylab("Fail-Safe-N") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")
```

# Funnel Plot
```{r}

```

## Test for funnel plot asymmetry

From Anzures-Cabrera & Higgins (2010): "

Mathematically, a Galbraith plot is a scatter plot of standardized effect estimates (estimates divided by their standard errors, or yi / si) against inverse standard error (1/ si). Imprecise estimates of effect lie near the origin and precise estimates further away, so that distance from the origin conveys the relative amounts of information in the different studies. An unweighted regression line constrained through the origin has a slope equal to  the fixed-effectmeta-analysis point estimate. Similarly, lines from the origin to each individual point have slope equal to the (unstandardized) point estimate in each study. A radial (arc of a circle) scale can be used to illustrate these slopes, allowing immediate derivation of point estimates from the studies and the meta-analysis. When the radial scale is added, Galbraith plots are sometimes called ‘radial plots’. A statistical test for non-zero intercept in an unconstrained line through the plotted points is equivalent to a particular test for funnel plot asymmetry [33]."

Eggers and Sterne (2005) - regression test

Trying to understand the different tests.
```{r, eval = F}
me = filter(all_data, dataset == "Mutual exclusivity")
# [1] No moderators
# (1a) classical egger test (random effect model) 
re_model <- rma(d_calc, d_var_calc, data=me)
regtest(re_model, model="lm") # regtest implements as: lm(yi ~ sqrt(vi) - 1, weights = 1/vi)

# (1b) classical egger test, by hand
summary(lm(d_calc ~ sqrt(d_var_calc),weights = 1/d_var_calc, data = me))

# (1c) classical egger test different version. Eggers and Sterne (2005) say that  1a/1b should be the same as the below ...but they're not. Need this to do Egger's plot. Maybe because doesnt include overdispersion parameter?
summary(lm(d_calc/sqrt(d_var_calc) ~ 1/sqrt(d_var_calc), data = me))

# (1d) random/mixed-effects version of the Egger test
regtest(re_model)
 
# Egger plot of 1c
ggplot(all_data, aes(y = d_calc/sqrt(d_var_calc), x = 1/sqrt(d_var_calc))) +
  geom_point() +
  facet_wrap(~ dataset, scales = "free_x") +
  ylab("Standarized effect size") +
  xlab("Precision") +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 0)


# [2] Age moderator
res_mixed <- rma(d_calc ~ mean_age_1, d_var_calc, data=me)

# (2a) classic eggers
regtest(res_mixed, model="lm")

# (2b) random/mixed-effects version of the Egger test
regtest(res_mixed)

## Egger plot of 2a -- not sure how do do this.
resid_df = data.frame(d_resid = resid(res_mixed), d_var_calc = me$d_var_calc)
ggplot(resid_df, aes(x = 1/sqrt(d_resid), y = d_resid/sqrt(d_var_calc))) +
  geom_point() +
  #facet_wrap(~ dataset, scales = "free_x") +
  ylab("Standarized effect size") +
  xlab("Precision") +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 0)
```

```{r}
eggers_tests <- function(ma_data){
    # models
    model.basic = metafor::rma(ma_data$d_calc, ma_data$d_var_calc, method = "REML",
               control = list(maxiter = 1000, stepadj = 0.5))
    model.mod = metafor::rma(ma_data$d_calc ~ma_data$mean_age_1, ma_data$d_var_calc, method = "REML",
               control = list(maxiter = 1000, stepadj = 0.5))
    # Eggers tests
    egg.basic = regtest(model.basic, model="lm") 
    egg.basic.random = regtest(model.basic) 
    egg.mod = regtest(model.mod, model="lm") 
    egg.mod.random = regtest(model.mod) 
    
    data.frame(dataset = ma_data$short_name[1],
               egg.basic.z = egg.basic$zval,
               egg.basic.p = egg.basic$pval,
               egg.basic.random.z =  egg.basic.random$zval,
               egg.basic.random.p =  egg.basic.random$pval,
               egg.mod.z = egg.mod$zval,
               egg.mod.p = egg.mod$pval,
               egg.mod.random.z = egg.mod.random$zval,
               egg.mod.random.p = egg.mod.random$pval)
}

eggers = all_data %>%
  split(.$short_name) %>%
  map(function(ma_data) eggers_tests(ma_data)) %>%
  bind_rows() %>%
  gather("p.type","p", contains(".p")) %>%
  mutate(sig = ifelse(p<.05,"skewed", "not skewed")) %>%
  select(p.type, sig, dataset)

ggplot(eggers, aes(x = dataset, y =  p.type , 
                          fill = sig)) +
  geom_tile() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(values = c("blue","pink"))
```
It matters for inworddb and ids pref which measure we use. My vote is for random with age moderator.



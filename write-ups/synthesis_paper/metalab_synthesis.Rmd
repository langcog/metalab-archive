---
title: "A Quantitative Synthesis of Early Language Acquisition Using Meta-Analysis"
shorttitle: "A Quantitative Synthesis"

author: 
  - name:  Molly Lewis
    affiliation: 1
  - name: Mika Braginsky
    affiliation: 1
  - name: Sho Tsuji
    affiliation: 2
  - name: Christina Bergmann
    affiliation: 2
  - name: Page Piccinini
    affiliation: 2
  - name: Alejandrina Cristia
    affiliation: 2
  - name: Michael C. Frank
    affiliation: 1
affiliation:
  - id: 1
    institution: Department Psychology, Stanford University
  - id: 2
    institution: Laboratoire de Sciences Cognitives et Psycholinguistique, ENS
    
abstract: |
  replicability, etc.
  
note: |  
  Correspondence concerning this article should be addressed to Molly Lewis, Psychology Department, Stanford University. 450 Serra Mall, Stanford, CA 94305. E-mail: mll@stanford.edu.
  
keywords: "replicability, reproducibility, meta-analysis, developmental psychology, language acquisition"

wordcount: "XXXX"

class: man
lang: american
figsintext: yes
lineno: no
bibliography:
  - metalab_synthesis.bib
header-includes:
  - \usepackage{setspace}
  - \AtBeginEnvironment{tabular}{\singlespacing}
  - \usepackage{pbox}

output: papaja::apa6_pdf
---

```{r message = FALSE, warning = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE)
require("papaja")
apa_prepare_doc() # Prepare document for rendering
```

# Introduction
Psychologists hope to build  generalizable theories about human behavior---theories that hold true beyond particulars of an individual study. The field has grown concerned as a result in the face of recent high-profile evidence that an effect observed in one study may not be the same in another (``replicability crisis''; Ioannidis, 2005; Nosek, 2012, 2015). Some of this variability is to be expected, however---the question we should instead be asking is, do the data provide support for the theory, even if they are noisy? Furthermore, to build parsimonious theories of human behavior, we should seek to explain not just individual phenemenon, but entire literatures of research. What is needed, then, is a tool for aggregating noisy data across studies within a phenomenon, as well as a common language for comparing effects across phenomenona.

Meta-analytic methods provide a powerful tool for doing just this. The basic unit of meta-analysis---the effect size---provides an estimate of the *size* of an effect, as well as a measure of uncertainty around this point estimate. With  such a continuous measure of success, we can apply the same reasoning we use to aggregate  noisy measurements over  participants in a single study: By assuming each *study*, rather than participant, is sampled from a population, we can appeal to the classical statistical framework to combine estimates of the effect size for a given phenomenon.

This quantitative approach provides a rich tool kit for synthesizing across literatures. By describing different phenomena using the same unit of measurement, we are able to compare effects in different domains. Rather than simply concluding that two effects are both ``real,'' we can ask more fine-grained questions: Is effect *X* bigger than effect *Y*? Does a moderator influence effect *X* in the same way as effect *Y*? This type of continuous analysis supports building quantitative models, and specifying theories that are more precise and constraining.

In addition to these theoretical motivations, there are practical reasons for conducting a quantitative synthesis. When planning an experiment, an estimate of the size of an effect on the basis of prior literature can inform the sample size needed to achieve a desired level of power. Meta-analytic estimates of effect sizes can also aid in design choices: If a certain paradigm tends to have overall larger effect sizes than another, the strategic researcher might select this paradigm in order to maximize the power of a study.

In practice, however, the feasability of this meta-analytic approach relies on the field’s commitment to practices that facilitate cumulative science. These practices apply to all stages of the research process. At the stage of experimental planning, researchers must pre-specify analytical descision to limit ``researcher'' degrees of freedom (Simmons, 2011; Simonsohn, 2014a, 2014b, 2014c). At the stage of completion, researchers should share a result regardless of its significance (Rosenthal, 1979; Fanelli 2012). And, at the stage of sharing, researchers must provide enough information about the method for another lab to conduct a close replication. Critically,r eports must also contain complete descriptions of both data and analytical decisions so that effect sizes can be calculcated for the purposes of meta-analysis,

In the present paper, we use meta-analytic methods to provide a quantitative synthesis of an entire field of psychological research: language acquisition.  We think this field is a particularly informative case study. It may be particularly vulnerable to false findings because running children is expensive (Ioanndis, 2005), and thus:

+ sample sizes are small 
+ replications difficult and rare
+ Recent attention about practices in developmental research @Peterson:2016

We have two goals:

+ Describe the state of the field in terms of its participation in practices that are prerequisites to cumulative science, and ultimately, a theoretical synthesis
+ Provide a preliminary theoretical synthesis of the field

Towards this end, we introduce [Metalab](http://metalab.stanford.edu/).


# Method
```{r load data}
source("../../dashboard/global.R", chdir = T)
library(dplyr)

#citations - [e.g., @bauer_2014; @bem_2011] 
#citations - @bauer_2014 → Baumer et al. (2014).
#italics - *R* 
#html - [RMarkdown](http://rmarkdown.rstudio.com/)

```

We calculated estimates of effect sizes for `r nrow(datasets)` different phenomenena in language acquisition. We selected these phenomena in order to describe development at many different levels of the language hierarchy, from the acquistion of prosody and phonemic contrasts, to gaze following in linguistic interaction. This wide range of phenomena allowed us to compare the course of development across different domains, as well as explore questions about the interactive nature of language acquisition. 

Estimates of effect size were based on journal reports of experimental data.  In total, our sample includes estimates from `r sum(datasets$num_papers)` papers, `r sum(datasets$num_experiments)` different conditions and `r format(floor(sum(datasets$num_subject)), big.mark=",", scientific=FALSE)` participants. 

The process for selecting papers from the literature differed by domain, with some individual meta-analyses using more systematic approaches than others. [Simulations here?]
\renewcommand{\arraystretch}{1.5}
\begin{table}[h!]
		\footnotesize
		\begin{tabular}{lp{4cm} p{5cm}r}
			\textbf{Level} & \textbf{Phenomenon}                                                               & \textbf{Description}                                                                                 & \textbf{N papers (conditions)}                                                                                                                                               \\
			\hline
			Prosody        & IDS  preference  \newline  {\scriptsize (Dunst, Gorman, \& Hamby, 2012)}          & {\scriptsize  `r datasets[datasets$name == "Infant directed speech preference", "short_desc"]`}      & `r datasets[datasets$name == "Infant directed speech preference", "num_papers"]` (`r datasets[datasets$name == "Infant directed speech preference", "num_experiments"]`)     \\
			Sounds         & Phonotactic learning  \newline {\scriptsize (Cristia, in prep)}                   & {\scriptsize `r datasets[datasets$name == "Phonotactic learning", "short_desc"]`  }                  & `r datasets[datasets$name == "Phonotactic learning", "num_papers"]` (`r datasets[datasets$name == "Phonotactic learning", "num_experiments"]`)                               \\
			~              & Vowel discrimination (native) \newline {\scriptsize (Tsuji \& Cristia, 2014)}     & {\scriptsize `r datasets[datasets$name == "Vowel discrimination (native)", "short_desc"]`  }         & `r datasets[datasets$name == "Vowel discrimination (native)", "num_papers"]` (`r datasets[datasets$name == "Vowel discrimination (native)", "num_experiments"]`)             \\ 
			~              & Vowel discrimination (non-native) \newline {\scriptsize (Tsuji \& Cristia, 2014)} & {\scriptsize `r datasets[datasets$name == "Vowel discrimination (non-native)", "short_desc"]`  }     & `r datasets[datasets$name == "Vowel discrimination (non-native)", "num_papers"]` (`r datasets[datasets$name == "Vowel discrimination (non-native)", "num_experiments"]`)     \\
			Phonotactics   & Statistical sound learning  \newline {\scriptsize (Cristia, in prep)}             & {\scriptsize `r datasets[datasets$name == "Statistical sound category learning", "short_desc"]`   }  & `r datasets[datasets$name == "Statistical sound category learning", "num_papers"]` (`r datasets[datasets$name == "Statistical sound category learning", "num_experiments"]`) \\ 
			Proto-words    & Word segmentation \newline {\scriptsize  (Bergmann \& Cristia, 2015) }            & {\scriptsize `r datasets[datasets$name == "Word segmentation", "short_desc"]`  }                     & `r datasets[datasets$name == "Word segmentation", "num_papers"]` (`r datasets[datasets$name == "Word segmentation", "num_experiments"]`)                                     \\
			Words     &   Mutual exclusivity \newline {\scriptsize (Lewis \& Frank, in prep)} &{\scriptsize  `r datasets[datasets$name == "Mutual exclusivity", "short_desc"]`}
			& `r datasets[datasets$name == "Mutual exclusivity", "num_papers"]` (`r datasets[datasets$name == "Mutual exclusivity", "num_experiments"]`)             \\
			~              & Concept-label advantage   \newline {\scriptsize (Lewis \& Long, unpublished)}     & {\scriptsize `r datasets[datasets$name == "Label advantage in concept learning", "short_desc"]`    } & `r datasets[datasets$name == "Label advantage in concept learning", "num_papers"]` (`r datasets[datasets$name == "Label advantage in concept learning", "num_experiments"]`) \\
			~              & Online word recognition \newline {\scriptsize (Frank, Lewis, \& MacDonald, 2016)} & {\scriptsize `r datasets[datasets$name == "Online word recognition", "short_desc"]`   }              & `r datasets[datasets$name == "Online word recognition", "num_papers"]` (`r datasets[datasets$name == "Online word recognition", "num_experiments"]`)                         \\
			Communication  & Gaze following  \newline {\scriptsize  (Frank, Lewis, \& MacDonald, 2016)}        & {\scriptsize `r datasets[datasets$name == "Gaze following", "short_desc"]`   }                       & `r datasets[datasets$name == "Gaze following", "num_papers"]` (`r datasets[datasets$name == "Gaze following", "num_experiments"]`)                                           \\
			~              & Pointing and vocabulary  \newline {\scriptsize (Colonnesi et al., 2010)}          & {\scriptsize `r datasets[datasets$name == "Pointing and vocabulary", "short_desc"]`  }               & `r datasets[datasets$name == "Pointing and vocabulary", "num_papers"]` (`r datasets[datasets$name == "Pointing and vocabulary", "num_experiments"]`)                         \\ 
		\end{tabular}
	\end{table}
# Replicability of the field
Effect size can vary between studies for reasons unrelated to a theoretical construct. One reason for this variability is the precision of the effect size, which we can model based on the sample size of the study. A remaining source variability, however, are biases introduced directly by the experimenter, via publication bias [@rothstein2006publication; @rosenthal1979file; @Fanelli:2010kf], analytical flexibility [@Simmons:2011iw], reporting errors, or even fraud. These biases are much more difficult to model, and may therefore lead to large but unknown errors in estimates of the effect size. If these types of practices are present in the literature, estimates of effect size may be poor estimates of the true underlying effect size, making it difficult to make theoretical progress. Below we present analyses examining whether signatures of publication bias and analytical flexibility are present in the language acquistion literature. We find little evivdence of these biases.


```{r, overall_d}
overall_es <- function(ma_data){
  model = metafor::rma(ma_data$d_calc, ma_data$d_var_calc, method = "REML",
               control = list(maxiter = 1000, stepadj = 0.5))
    data.frame(dataset = ma_data$short_name[1],
               overall.d = model$b,
               ci_lower = model$ci.lb,
               ci_upper = model$ci.ub)
}

all_ds = all_data %>%
  split(.$short_name) %>%
  map(function(ma_data) overall_es(ma_data)) %>%
  bind_rows() %>%
  mutate_each_(funs(round(., digits = 2)), vars = c("overall.d", "ci_lower", "ci_upper")) %>%
  mutate(d_string = paste0(overall.d, " [", ci_lower, ", ",ci_upper, "]")) %>%
  mutate(short_name = dataset)
```

```{r, pcurve_power}
source("paper_analyses/pcurve.R")

ALPHA = .05
P_INCREMENT = .01 

pc.data <- get_all_pc_data(all_data, ALPHA, P_INCREMENT)

power.data = pc.data %>%
  group_by(dataset) %>%
  do(data.frame(power = get_pc_power(., ALPHA)))  %>%
  full_join(datasets %>% select(name, short_name), by= c("dataset" = "name")) %>%
  mutate_each_(funs(round(., digits = 2)), vars = c("power")) %>%
  mutate(power_string = ifelse(is.na(as.character(power)), "", as.character(power)))
```

```{r, p_curve_skew}
stouffer.data = pc.data %>%
  group_by(dataset) %>%
  do(data.frame(stouffer = stouffer_test(., ALPHA))) %>%
  filter(stouffer.pp.measure == "ppr.full") %>%
  full_join(datasets %>% select(name, short_name), by= c("dataset" = "name")) %>%
  select(short_name,stouffer.Z.pp, stouffer.p.Z.pp) %>%
  mutate_each_(funs(round(., digits = 2)), vars = c("stouffer.p.Z.pp", "stouffer.Z.pp")) %>%
  mutate(stouff_string = ifelse(is.na(as.character(stouffer.Z.pp)), "", paste0(stouffer.Z.pp, " (", stouffer.p.Z.pp,")"))) %>%
  select(short_name, stouff_string)
```

```{r, funnel_skew}
eggers_tests <- function(ma_data){
    # model
    model.mod = metafor::rma(ma_data$d_calc ~ma_data$mean_age_1, ma_data$d_var_calc, method = "REML",
               control = list(maxiter = 1000, stepadj = 0.5))
    # Eggers test
    egg.mod.random = regtest(model.mod) 
    
    data.frame(dataset = ma_data$short_name[1],
               egg.mod.random.z = egg.mod.random$zval,
               egg.mod.random.p = egg.mod.random$pval)
}

eggers.data = all_data %>%
  split(.$short_name) %>%
  map(function(ma_data) eggers_tests(ma_data)) %>%
  bind_rows() %>%
  mutate_each_(funs(round(., digits = 2)), vars = c("egg.mod.random.z", "egg.mod.random.p")) %>%
  mutate(egg_string = paste0(egg.mod.random.z, " (",egg.mod.random.p, ")"))  %>%
  rename(short_name = dataset)
```

```{r, fail_safe_N}
fsn.package.data = all_data %>%
  group_by(dataset) %>%
  summarise(fsn_package= fsn(d_calc, d_var_calc, data = all_data, 
                         target = .01, type="Orwin")$fsnum) %>%
  left_join(datasets %>% select(name, short_name), by= c("dataset" = "name")) %>%
  rename(fsn_string = fsn_package)
```

```{r, make_table}
table.data = left_join(select(all_ds, -dataset), power.data) %>%
              left_join(stouffer.data) %>%
              left_join(eggers.data) %>%
              left_join(fsn.package.data) %>%
          select(datset, contains("string"))
  
```



\begin{table}[h!]
	\footnotesize
	\begin{tabular}{lrrrrrr}
		\textbf{Phenomenon}                                 & $\mathbf{\bar{d}}$ & \textbf{power} & \textbf{p-curve skew} & \textbf{funnel skew} & \textbf{fail-safe-N} \\                   
		\hline
		IDS  preference                  & `r all_ds[all_ds$dataset=="idspref", "d_string"]`         & ~         & `r stouffer.data[stouffer.data$dataset=="idspref", "string"]`     & ~            & ~   \\ 
		Phonotactic learning              & `r all_ds[all_ds$dataset=="phonotactics", "d_string"]`          & ~         & `r stouffer.data[stouffer.data$dataset=="phonotactics", "string"]`     & ~            & ~         \\ 
		Vowel discrimintation (native)    & `r all_ds[all_ds$dataset=="inphondb-native", "d_string"]`           & ~         & `r stouffer.data[stouffer.data$dataset==inphondb-native", "string"]`     & ~            & ~             \\ 
		Vowel discrimination (non-native) & `r all_ds[all_ds$dataset=="inphondb-nonnative", "d_string"]`           & ~         & `r stouffer.data[stouffer.data$dataset=="inphondb-nonnative", "string"]`     & ~            & ~           \\ 
		Statistical sound learning        & `r all_ds[all_ds$dataset=="sounds", "d_string"]`           & ~         & `r stouffer.data[stouffer.data$dataset=="sounds", "string"]`     & ~            & ~             \\ 
		Word segmentation                 & `r all_ds[all_ds$dataset=="inworddb", "d_string"]`           & ~         & `r stouffer.data[stouffer.data$dataset=="inworddb", "string"]`     & ~            & ~             \\ 
		Mutual exclusivity               & `r all_ds[all_ds$dataset=="mutex", "d_string"]`           & ~         & `r stouffer.data[stouffer.data$dataset=="mutex", "string"]`    & ~            & ~             \\ 
		Concept-label advantage           & `r all_ds[all_ds$dataset=="labadv", "d_string"]`           & ~         & `r stouffer.data[stouffer.data$dataset=="labdv", "string"]`     & ~            & ~              \\ 
		Gaze following                    & `r all_ds[all_ds$dataset=="gaze_following", "d_string"]`          & ~         & `r stouffer.data[stouffer.data$dataset=="gaze_following", "string"]`     & ~            & ~             \\ 
		Pointing and vocabulary           & `r all_ds[all_ds$dataset=="pointing", "d_string"]`           & ~         & `r stouffer.data[stouffer.data$dataset=="pointing", "string"]`     & ~            & ~               \\ 
	\end{tabular}
\end{table}
## p-curves
[@Simonsohn:2014ch; @simonsohn2015better; @simonsohn2014p]
Across studies we should expect some variability in effect size due to sampling error alone. But this variability in effect size should be *systematic*: There should be less variability around the mean for more precise studies, as measured by sample size. The presence of variability in effect sizes that is not accounted for sample size may suggest publication bias in a literature.

bias introduced by meta-analysis in selection (second-order selection bias)

## Funnel
* Egger's regression test

## Orwin

## Power
Ioannidis and Trikalinos (2007) 

TABLE WITH: Eggers regression, p-curve (stouffer), regular power, p-power, orwin


# Theoretical Synthesis
OUTLINE

## Statistical Approach

METAMETAPLOT


# Discussion


#### Author Contributions

#### Acknowledgments

\newpage

### References


---
nocite: | 
  @lfprep
  @dunst2012preference
  @frank2016performance
  @tsuji2014perceptual
  @bergmann2015development
...

```{r create_r-references}
r_refs(file = "metalab_synthesis.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{8pt}

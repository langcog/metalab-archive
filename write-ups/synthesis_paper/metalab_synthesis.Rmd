---
title: "A Quantitative Synthesis of Early Language Acquisition Using Meta-Analysis"
shorttitle: "A Quantitative Synthesis"

author: 
  - name:  Molly Lewis
    affiliation: 1
  - name: Mika Braginsky
    affiliation: 1
  - name: Sho Tsuji
    affiliation: 2
  - name: Christina Bergmann
    affiliation: 2
  - name: Page Piccinini
    affiliation: 2
  - name: Alejandrina Cristia
    affiliation: 2
  - name: Michael C. Frank
    affiliation: 1
affiliation:
  - id: 1
    institution: Department Psychology, Stanford University
  - id: 2
    institution: Laboratoire de Sciences Cognitives et Psycholinguistique, ENS
    
abstract: |
  replicability, etc.
  
note: |  
  Correspondence concerning this article should be addressed to Molly Lewis, Psychology Department, Stanford University. 450 Serra Mall, Stanford, CA 94305. E-mail: mll@stanford.edu.
  
keywords: "replicability, reproducibility, meta-analysis, developmental psychology, language acquisition"

wordcount: "XXXX"

class: man
lang: english
figsintext: yes
lineno: no
bibliography:
  - metalab_synthesis.bib
header-includes:
  - \usepackage{setspace}
  - \AtBeginEnvironment{tabular}{\singlespacing}
  - \usepackage{pbox}

output: papaja::apa6_pdf
---

```{r message = FALSE, warning = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE)
require("papaja")
apa_prepare_doc() # prepare document for rendering
```

# Introduction
To learn to speak a language, a child must acquire a wide range knowledge and skills:   the sounds of the language, the word forms, and how words map to meanings, to name only a few. How does this process unfold? Our goal as psychologists is to build a theory that can explain and predict this process in a way that is both precise but also highly generalizable. The challenge we face, however, is that we must build this theory on the basis of very limited data, and, as a consequence, we must rely on error-prone inductive reasoning strategies to build our theories. In this paper, we consider the domain of language acquisition and demonstrate how meta-analytic methods can support the inductive theory building process.

Meta-analysis is a quantitative method for aggregating across experimental findings. The fundamental unit of meta-analysis is the *effect size*: a scale-free, quantitative measure of "success" in a phenomenon. Importantly, an effect size provides an estimate of the *size* of an effect, as well as a measure of uncertainty around this point estimate. With  such a quantitative measure of success, we can apply the same reasoning we use to aggregate  noisy measurements over  participants in a single study: By assuming each *study*, rather than participant, is sampled from a population, we can appeal to the classical statistical framework to combine estimates of the effect size for a given phenomenon.

Meta-analytic methods support theory building in several ways. First, they provide a way of evaluating which effects in a literature are "real," and thus should constrain the theory. This is particulary important in  light of recent high-profile evidence in the field that an effect observed in one study may not replicate in another ["replication crisis",  @ioannidis2005most; @open2015estimating; @open2012open]. Such cases of failed replications are difficult to interpret, however. One possibility is that the failure was observed because the effect is in fact not real. This can occur when there was bias introduced by the experimenter in the original study. But another possibility is that an effect *is* real, but the failure to replicate is due to sampling error. Just as we would not conclude that a phenomenon does not exist because one or two participants do not show an effect, we should not conclude that an effect does not exist on the basis of one or two failed replications. Meta-analytic methods provide a principled approach for distinguising between these two sources of failed studies. This supports a more veridical description of the empirical landscape, which in turn leads to better theory-building.

Second, meta-analysis support theory building by providing higher fidelity descriptions of phenomenona. Rather than simply concluding that an effect exists, effect sizes allow us  to ask finer grain questions: How much variability is there in the effect? How does the effect change over development? To what extent does a moderator of theoretetic influence the effect? This type of continuous analysis supports building quantitative models, and specifying theories that are more precise and constraining.

Furthermore, effect sizes provide a common language for comparing effects across phenomenona. This allows us to understand not only what the trajectory of a particular phenomenon is, like word learning, for example, but also how this phenomenon might depend on other skills (e.g. sound learning, gaze following, etc.). With this more complete picture of langauge development--- and the common language of effect sizes--- we can begin to build a more synthetic theory of language acquisition.

Finally, in addition to these theoretical motivations, there are practical reasons for conducting a quantitative synthesis. When planning an experiment, an estimate of the size of an effect on the basis of prior literature can inform the sample size needed to achieve a desired level of power. Meta-analytic estimates of effect sizes can also aid in design choices: If a certain paradigm tends to have overall larger effect sizes than another, the strategic researcher might select this paradigm in order to maximize the power of a study.

While meta-analytic methods are likely helpful for many psychological literatures, we believe language acquisition is a particularly informative application for this tool. One reason is that language acquistion may be uniquely vulnerable to false findings because running children is expensive, and thus sample sizes are small and studies are underpowered [@ioannidis2005most]. In addition, the difficulty in running participants means that replications are relatively rare in the field. Finally, there has been attention to developmental psychology research practices more broadly, suggesting evidence of experimenter bias [@Peterson:2016].  

With these motivations, we developed a dataset of effect sizes in the language acquistion literature across 12 different phenomenona ([Metalab; http://metalab.stanford.edu/](http://metalab.stanford.edu/)). We take as our ultimate goal a single overarching theory of language acquisition that can explain and predict all the relevant pheonemona. We demonstrate how meta-analysis supports building this theory in two ways. We first use meta-analytic techniques to evaluate the evidential value of the empirical landscape in language acquistion research. We find broadly that this literature has strong evidential value, and thus that the effects report in the literature should constrain our theory of language acquisition. We then turn to synthesizing these findings across phenenomena and offer a preliminary theoretical synthesis of the field.

# Method
```{r load data}
source("../../dashboard/global.R", chdir = TRUE)
source("paper_analyses/pcurve.R")
library(metafor)
library(knitr)
# Tried installling packrat, but got error: Error in hash(descFile) : 
#No DESCRIPTION file at path #'/Documents/GRADUATE_SCHOOL/Projects/metalab/metalab/write-ups/synthesis_paper/packrat#/lib/x86_64-apple-darwin13.4.0/3.2.1/BH/DESCRIPTION'!

#citations - [e.g., @bauer_2014; @bem_2011] 
#citations - @bauer_2014 â†’ Baumer et al. (2014).
#italics - *R* 
#html - [RMarkdown](http://rmarkdown.rstudio.com/)
```

We analyzed `r nrow(datasets)` different phenomenena in language acquisition. We selected these phenomena in order to describe development at many different levels of the language hierarchy, from the acquistion of prosody and phonemic contrasts, to gaze following in linguistic interaction. This wide range of phenomena allowed us to compare the course of development across different domains, as well as explore questions about the interactive nature of language acquisition (Table 1). 

To obtain estimates of effect size, we coded papers reporting experimental data. Within each paper, we calculated a separate effect size estimate for each experiment and age group ("condition"). In total, our sample includes estimates from `r sum(datasets$num_papers)` papers, `r sum(datasets$num_experiments)` different conditions and `r format(floor(sum(datasets$num_subject)), big.mark=",", scientific=FALSE)` participants. The process for selecting papers from the literature differed by domain, with some individual meta-analyses using more systematic approaches than others (see SI).
\renewcommand{\arraystretch}{1.5}
\begin{table}[h!]
		\footnotesize
		\begin{tabular}{lp{4cm} p{5cm}r}
			\toprule
			\textbf{Level} & \textbf{Phenomenon}                                                               & \textbf{Description}                                                                                 & \textbf{N papers (conditions)}                                                                                                                                               \\
						\midrule

			Prosody        & IDS  preference  \newline  {\scriptsize (Dunst, Gorman, \& Hamby, 2012)}          & {\scriptsize  `r datasets[datasets$name == "Infant directed speech preference", "short_desc"]`}      & `r datasets[datasets$name == "Infant directed speech preference", "num_papers"]` (`r datasets[datasets$name == "Infant directed speech preference", "num_experiments"]`)     \\
			Sounds         & Phonotactic learning  \newline {\scriptsize (Cristia, in prep.)}                   & {\scriptsize `r datasets[datasets$name == "Phonotactic learning", "short_desc"]`  }                  & `r datasets[datasets$name == "Phonotactic learning", "num_papers"]` (`r datasets[datasets$name == "Phonotactic learning", "num_experiments"]`)                               \\
			~              & Vowel discrimination (native) \newline {\scriptsize (Tsuji \& Cristia, 2014)}     & {\scriptsize `r datasets[datasets$name == "Vowel discrimination (native)", "short_desc"]`  }         & `r datasets[datasets$name == "Vowel discrimination (native)", "num_papers"]` (`r datasets[datasets$name == "Vowel discrimination (native)", "num_experiments"]`)             \\ 
			~              & Vowel discrimination (non-native) \newline {\scriptsize (Tsuji \& Cristia, 2014)} & {\scriptsize `r datasets[datasets$name == "Vowel discrimination (non-native)", "short_desc"]`  }     & `r datasets[datasets$name == "Vowel discrimination (non-native)", "num_papers"]` (`r datasets[datasets$name == "Vowel discrimination (non-native)", "num_experiments"]`)     \\
			   & Statistical sound learning  \newline {\scriptsize (Cristia, in prep.)}             & {\scriptsize `r datasets[datasets$name == "Statistical sound category learning", "short_desc"]`   }  & `r datasets[datasets$name == "Statistical sound category learning", "num_papers"]` (`r datasets[datasets$name == "Statistical sound category learning", "num_experiments"]`) \\ 
			& Word segmentation \newline {\scriptsize  (Bergmann \& Cristia, 2015) }            & {\scriptsize `r datasets[datasets$name == "Word segmentation", "short_desc"]`  }                     & `r datasets[datasets$name == "Word segmentation", "num_papers"]` (`r datasets[datasets$name == "Word segmentation", "num_experiments"]`)                                     \\
			Words     &   Mutual exclusivity \newline {\scriptsize (Lewis \& Frank, in prep.)} &{\scriptsize  `r datasets[datasets$name == "Mutual exclusivity", "short_desc"]`}
			& `r datasets[datasets$name == "Mutual exclusivity", "num_papers"]` (`r datasets[datasets$name == "Mutual exclusivity", "num_experiments"]`)             \\
			~ &   Sound Symbolism \newline {\scriptsize (Lammertink et al., in prep.)} &{\scriptsize  `r datasets[datasets$name == "Sound symbolism", "short_desc"]`}
			& `r datasets[datasets$name == "Sound symbolism", "num_papers"]` (`r datasets[datasets$name == "Sound symbolism", "num_experiments"]`)             \\
			~              & Concept-label advantage   \newline {\scriptsize (Lewis \& Long, unpublished)}     & {\scriptsize `r datasets[datasets$name == "Label advantage in concept learning", "short_desc"]`    } & `r datasets[datasets$name == "Label advantage in concept learning", "num_papers"]` (`r datasets[datasets$name == "Label advantage in concept learning", "num_experiments"]`) \\
			~              & Online word recognition \newline {\scriptsize (Frank, Lewis, \& MacDonald, 2016)} & {\scriptsize `r datasets[datasets$name == "Online word recognition", "short_desc"]`   }              & `r datasets[datasets$name == "Online word recognition", "num_papers"]` (`r datasets[datasets$name == "Online word recognition", "num_experiments"]`)                         \\
			Communication  & Gaze following  \newline {\scriptsize  (Frank, Lewis, \& MacDonald, 2016)}        & {\scriptsize `r datasets[datasets$name == "Gaze following", "short_desc"]`   }                       & `r datasets[datasets$name == "Gaze following", "num_papers"]` (`r datasets[datasets$name == "Gaze following", "num_experiments"]`)                                           \\
			~              & Pointing and vocabulary  \newline {\scriptsize (Colonnesi et al., 2010)}          & {\scriptsize `r datasets[datasets$name == "Pointing and vocabulary", "short_desc"]`  }               & `r datasets[datasets$name == "Pointing and vocabulary", "num_papers"]` (`r datasets[datasets$name == "Pointing and vocabulary", "num_experiments"]`)                         \\ 
			\bottomrule
		\end{tabular}
		\caption{Overview of meta-analyses in dataset.}
	\end{table}
	


                  
# Replicability of the field
A literature is more likely to describe a real effect if studies are randomly sampled from the population of all possible  studies that researchers could in principle conduct. This assumption does not mean, however,  that there should be *no* variability in effect size across studies: We should expect random variation around the true mean effect size, with smaller studies showing more variability around this mean. 

Variability in effect sizes will be biased when this assumption of  random study sampling  does not hold. Bias may be introduced by the experimenter in a number of ways, including  failure to publish null findings ["publication bias", @rothstein2006publication; @rosenthal1979file; @Fanelli:2010kf], analytical flexibility [e.g., "p-hacking," @Simmons:2011iw; @simonsohn2014p], reporting errors, or even fraud. These biases are problematic for theoretical development because they  lead to large but often unknown errors in estimates of the effect size. If bias is present in the literature, estimates of effect size may be poor estimates of the true underlying effect size and thus be of limited evidential value. To make theoretical progress, we must therefore distinguish variability in effect sizes due to sample size from varability due to bias.

To assess the replicability of language acquisition phenomena, we conducted several diagnostic analyses: Meta-analytic estimates of effect size, fail-safe-N [@orwin1983fail], funnel plots, and p-curve [@simonsohn2014p; @simonsohn2014power; @simonsohn2015better]. These analytical approaches each have limitations, but taken together, they provide converging evidence about the replicability of a literature. Overall, we find little evidence of bias in our meta-analyses, suggesting that the language acqusition literature likely describes real psychological phenomenona and should therefore provide the basis for theoretical development. 

## Meta-Analytic Effect Size
```{r, overall_d}
overall_es <- function(ma_data){
  model = metafor::rma(ma_data$d_calc, ma_data$d_var_calc, method = "REML",
               control = list(maxiter = 1000, stepadj = 0.5))
    data.frame(dataset = ma_data$short_name[1],
               overall.d = model$b,
               ci_lower = model$ci.lb,
               ci_upper = model$ci.ub)
}

all_ds = all_data %>%
  split(.$short_name) %>%
  map(function(ma_data) overall_es(ma_data)) %>%
  bind_rows() %>%
  mutate_each_(funs(round(., digits = 2)), 
               vars = c("overall.d", "ci_lower", "ci_upper")) %>%
  mutate(d_string = paste0(overall.d, " [", ci_lower, ", ",ci_upper, "]")) %>%
  mutate(short_name = dataset)
```
Meta-analysis provides a quantitative method for aggregating across studies. To estimate the overall effect size of a literature, effect sizes are pooled across papers to obtain a single meta-analytic estimate. Importantly, meta-analysis allows us to model variability in effect sizes due to differences in sample sizes by weighting studies with more participants more heavily in the overall estimate. This meta-analytic effect-size can be thought of as the ``best estimate" of the effect size for a phenomenon given all the available data in the literature.

Table 2, column 2 presents meta-analytic effect size estimates for each of our phenomenona. We find evidence for a non-zero effect size in 11 out of 12 of our phenomena, suggesting these literature provide evidential value. In the case of phonotatic learning, however, we find that the meta-analytic effect size estimate does not differ from zero, suggest that this literature does not describe a real effect. [Remove it from analyses below?].

While the meaure of effect size is itself quantitative, meta-analytic estimates of effect size provide only categorical information about the evidential value of a literature: the effect is real, or not. But, a more powerful method of assessing evidential value would tell us the *degree* to which a literature has evidential value, and thus the degree to which it should constrain our theory building. In the following three analyses---fail-safe-N, funnel plots, and p-curves---we describe through analyses that quantify the evidential value of these literatures.

## Fail-safe-N
```{r, fail_safe_N}
fsn.package.data = all_data %>%
  group_by(dataset) %>%
  summarise(fsn_package = fsn(d_calc, d_var_calc, data = all_data, 
                         target = .01, type="Orwin")$fsnum) %>%
  left_join(datasets %>% select(name, short_name), by= c("dataset" = "name")) %>%
  rename(fsn_string = fsn_package)
```

One approach for quantifying the reliability of a literature is to ask, How many missing studies with null effects would have to exist in the "file drawer" in order for the overall effect size to be zero? This is called the "fail-safe" number of studies [@orwin1983fail]. To answer this question, we estimated the overall effect size for each phenomenenon (Table 2, column 2), and then used this to estimate the fail-safe-N (Table 2, column 3). 

This analysis suggests a very large number of studies would have to be "missing" in each literature ($M$ = `r round(mean(fsn.package.data$fsn_string[is.finite(fsn.package.data$fsn_string)]))`) in order for the overall effect sizes to be 0. Thus, while it is possible that some reporting bias is present in the literature, the large fail-safe-N suggests that the literature nonetheless likely describes a real effect. 

One limitation of this analysis, however, is that it assumes that all reported effect sizes are obtained in the absence of analytical flexibility: If experimenters are exercising analytical flexibility through practices like p-hacking, then the number and magnitude of observed true effects in the literature may be inflated. In the next analysis, we examine this possibility through funnel plots.

## Funnel Plots 
Funnel plots provide a visual method for evaluating whether variability in effect sizes is due  only to differences in sample size. A funnel plot shows effect sizes versus a metric of sample size, standard error. If there is no bias in a literature, we should expect studies to be randomly sampled around the mean, with more variability for less precise studies. 

Figure 1 presents funnel plots for each of our `r nrow(datasets)` meta-analyses. These plots show evidence of asymmetry (bias) for several of our phenomenon (Table 2, column 4). However, an important limitation of this method is that it is difficult to determine the source of this bias. One possibility is that this bias is due not to experimenter, but to true heterogenity in phenomena (e.g. different ages). P-curve analyses provide one method for addressing this issue, which we turn to next.

```{r,  fig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "Funnel plots for each meta-analysis. Each effect size estimate is represented by a point, and the mean effect size is shown as a red dashed line. The funnel corresponds to a 95% (narrow) and 99% (wide) CI around this mean. In the absense of true heterogenity in effect sizes (no moderators) and bias, we should expect all points to fall inside the funnel."}

CRIT_95 = 1.96 
CRIT_99 = 2.58

funnel.es.data = all_data %>%
  mutate(dataset = as.factor(dataset),
         dataset= gdata::reorder.factor(dataset, 
                                    new.order = c(2,6,10,11,9,12,4,8,3,5,1,7)),
         dataset = plyr::revalue(dataset, 
                                 c("Infant directed speech preference"="IDS preference",
                                   "Statistical sound category learning"="Statistical sound learning", 
                                   "Label advantage in concept learning"="Concept-label advantage",
                                    "Vowel discrimination (native)"="Vowel discrimination\n(native)",
                                    "Vowel discrimination (non-native)"="Vowel discrimination\n(non-native)"))) %>%
  group_by(dataset) %>%
  mutate(se = sqrt(d_var_calc), 
         es = d_calc, 
         center = mean(d_calc), 
         lower_lim = max(se) + .05 * max(se))


# separate df for 95 CI funnel shape
funnel95.data.wide <- funnel.es.data %>%
                select(center, lower_lim, dataset) %>%
                group_by(dataset) %>%
                summarise(x1 = (center-lower_lim * CRIT_95)[1], 
                          x2 = center[1],
                          x3 = center[1] + lower_lim[1] * CRIT_95,
                          y1 = -lower_lim[1],
                          y2 =  0, 
                          y3 = -lower_lim[1]) 

funnel95.data.x = funnel95.data.wide  %>%
                  select(dataset, contains("x")) %>%
                  gather("coordx", "x", 2:4) %>%
                  arrange(dataset, coordx) %>%
                  select(-coordx)

funnel95.data.y = funnel95.data.wide  %>%
                  select(dataset, contains("y")) %>%
                  gather("coordy", "y", 2:4) %>%
                  arrange(dataset, coordy) %>%
                  select(-coordy)

funnel95.data = bind_cols(funnel95.data.x, funnel95.data.y)

# separate df for 99 CI funnel shape
funnel99.data.wide <- funnel.es.data %>%
                select(center, lower_lim, dataset) %>%
                group_by(dataset) %>%
                summarise(x1 = (center-lower_lim* CRIT_99)[1], 
                          x2 = center[1],
                          x3 = center[1] + lower_lim[1] * CRIT_99,
                          y1 = -lower_lim[1],
                          y2 =  0, 
                          y3 = -lower_lim[1]) 

funnel99.data.x = funnel99.data.wide  %>%
                  select(dataset, contains("x")) %>%
                  gather("coordx", "x", 2:4) %>%
                  arrange(dataset, coordx) %>%
                  select(-coordx)

funnel99.data.y = funnel99.data.wide  %>%
                  select(dataset, contains("y")) %>%
                  gather("coordy", "y", 2:4) %>%
                  arrange(dataset, coordy) %>%
                  select(-coordy)

funnel99.data = bind_cols(funnel99.data.x, funnel99.data.y)

ggplot(funnel.es.data, aes(x = es, y = -se)) +
  facet_wrap(~dataset, scales = "free") +
  xlab("Effect Size")  +
  ylab("Standard Error\n")  +
  scale_colour_solarized(name = "") +
  geom_polygon(aes(x = x, y = y), 
               data = funnel95.data, alpha = .5,
               fill = "white") +
  geom_polygon(aes(x = x, y = y), 
               data = funnel99.data, alpha = .5,
               fill = "white") +
  geom_vline(aes(xintercept=x2), 
             linetype = "dashed", color = "red", size = .8, data = funnel99.data.wide) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "darkgrey",  size = .8) +
  geom_point(size = .5) +
  theme(panel.background = element_rect(fill = "lightgrey"),
        panel.grid.major = element_line(colour = "grey", size = 0.2),
        panel.grid.minor = element_line(colour = "grey", size = 0.5))
```

```{r, funnel_skew}
eggers_tests <- function(ma_data){
    # model
    model.mod = metafor::rma(ma_data$d_calc ~ma_data$mean_age_1, ma_data$d_var_calc,
                             method = "REML",
                control = list(maxiter = 1000, stepadj = 0.5))
    
    # Eggers test
    egg.mod.random = regtest(model.mod) 
    
    data.frame(dataset = ma_data$short_name[1],
               egg.mod.random.z = egg.mod.random$zval,
               egg.mod.random.p = egg.mod.random$pval)
}

eggers.data = all_data %>%
  split(.$short_name) %>%
  map(function(ma_data) eggers_tests(ma_data)) %>%
  bind_rows() %>%
  mutate(egg.mod.random.z = round(egg.mod.random.z, digits = 2)) %>%
  mutate(egg.mod.random.p = round(egg.mod.random.p, digits = 2)) %>%
  mutate(egg_string = paste0(egg.mod.random.z, " (",egg.mod.random.p, ")"))  %>%
  rename(short_name = dataset) %>%
  select(short_name, egg_string)
```

## P-curves
```{r p_curve_plots,  fig.pos = "tb", fig.width=7, fig.height=6, fig.cap = "P-curve for each meta-analysis (Simonsohn, Nelson, & Simmons, 2014). In the absense of p-hacking, we should expect the observed p-curve (blue) to be right-skewed (more small values). The red dashed line shows the expected distribution of p-values when the effect is non-existent (the null is true). The green dashed line shows the expected distribution if the effect is real, but studies only have 33% power."}

ALPHA = .05
P_INCREMENT = .01 

pc.data <- get_all_pc_data(all_data, ALPHA, P_INCREMENT)

pc.data %>%
  group_by(dataset) %>%
  do(get_p_curve_df(., ALPHA, P_INCREMENT)) %>%
  ungroup() %>%
  mutate(dataset = as.factor(dataset),
                 dataset = plyr::revalue(dataset, 
                                         c("Label advantage in concept learning" = "Concept-label advantage")),
         dataset = gdata::reorder.factor(dataset, 
                                    new.order = c(3,5,6,7,2,4,1))) %>%
  ggplot(aes(x = p, y = value, linetype = measure, color = measure)) + 
  geom_line(size = 1) +
  scale_colour_manual(name = "", values = c("red", "green", "blue"), 
                      labels=c("Null of no effect", "Null of 30% power", "Observed")) +
  scale_linetype_manual(values = c("dashed", "dashed", "solid"), guide = FALSE)  +
  ylab("Proportion p-values") +
  xlab("p-value") +
  #scale_colour_discrete(labels=c("baseline", "expected", "observed")) +
  facet_wrap(~ dataset) +
  theme(legend.position = "top",
        legend.key = element_blank(),
        legend.background = element_rect(fill = "transparent"))
```

```{r p_curve_power}
power.data = pc.data %>%
  group_by(dataset) %>%
  do(data.frame(power = get_pc_power(., ALPHA)))  %>%
  full_join(datasets %>% select(name, short_name), by= c("dataset" = "name")) %>%
  mutate(power = round(power, digits = 2)) %>%
  mutate(power_string = ifelse(is.na(as.character(power)),"", as.character(power))) %>%
  select(-power)
```

```{r, p_curve_skew}
stouffer.data = pc.data %>%
  group_by(dataset) %>%
  do(data.frame(stouffer = stouffer_test(., ALPHA))) %>%
  filter(stouffer.pp.measure == "ppr.full") %>%
  full_join(datasets %>% select(name, short_name), by= c("dataset" = "name")) %>%
  select(short_name,stouffer.Z.pp, stouffer.p.Z.pp) %>%
  mutate_each_(funs(round(., digits = 2)), vars = c("stouffer.p.Z.pp",
                                                    "stouffer.Z.pp")) %>%
  mutate(stouff_string = ifelse(is.na(as.character(stouffer.Z.pp)), "",
                                paste0(stouffer.Z.pp, " (", stouffer.p.Z.pp,")"))) %>%
  select(short_name, stouff_string)
```

A p-curve is the distribution of p-values for the statistical test of the main hypothesis across a literature [@simonsohn2014p; @simonsohn2014power; @simonsohn2015better]. Critically, if there is a real effect in the literature, the shape of the p-curve should reflect this. In particular, we should expect the p-curve to be right skewed with more small values (e.g., .01) than large values (e.g., .04). An important property of this analysis is that we should expect this skew independent of any true heterogenity in the data, such as age. Evidence that the curve is in fact right-skewed would suggest that the literature is not biased, and that it provides evidential value for theory building. 

```{r, make_table}
table.data = left_join(select(all_ds, c(short_name, d_string)), fsn.package.data) %>%
             left_join(eggers.data) %>%
             left_join(stouffer.data) %>%
             left_join(power.data) %>%
             select(dataset, contains("string")) %>%
             ungroup() %>%
             .[c(2,8,3,4,10,5,7,11,6,12,1,9),]  %>% # reorder rows 
             mutate(dataset = as.factor(dataset),
                 dataset = plyr::revalue(dataset, c("Infant directed speech preference" ="IDS preference", # shorten dataset names
                                                   "Statistical sound category learning"= "Statistical sound learning",
                                                   "Label advantage in concept learning" =  "Concept-label advantage")))

# kable(table.data, format = "latex", booktabs = TRUE) # paste below (so can edit latex)
# header:	\textbf{Phenomenon}& \textbf{\textit{d}} & \textbf{fail-safe-N} & \textbf{funnel skew} & \textbf{p-curve skew} & \textbf{power}  \\
```

\begin{table}[t]
\footnotesize
\begin{tabular}{lrrrrr}
\toprule
\textbf{Phenomenon}& \textbf{\textit{d}} & \textbf{fail-safe-N} & \textbf{funnel skew} & \textbf{p-curve skew} & \textbf{power}\\
\midrule
IDS preference & 0.71 [0.53, 0.89] & 3762 & 1.88 (0.06) &  & \\
Phonotactic learning & 0.04 [-0.09, 0.16] & 45 & -1.08 (0.28) & -1.52 (0.06) & 0.14\\
Vowel discrimination (native) & 0.6 [0.5, 0.71] & 9536 & 8.98 (0) & -5.42 (0) & 0.67\\
Vowel discrimination (non-native) & 0.66 [0.42, 0.9] & 3391 & 4.13 (0) & -3.24 (0) & 0.78\\
Statistical sound learning & -0.14 [-0.27, -0.02] & Inf & -1.87 (0.06) &  & \\
Word segmentation & 0.2 [0.15, 0.25] & 5645 & 1.54 (0.12) & -9.67 (0) & 0.56\\
Mutual exclusivity & 1.01 [0.68, 1.33] & 6443 & 6.25 (0) &  & \\
Sound symbolism & 0.15 [0.04, 0.26] & 538 & -1.32 (0.19) & -2.16 (0.02) & 0.96\\
Concept-label advantage & 0.4 [0.29, 0.51] & 3928 & 0.31 (0.76) & -6.15 (0) & 0.69\\
Online word recognition & 1.89 [0.81, 2.96] & 2843 & 2.92 (0) &  & \\
Gaze following & 0.84 [0.26, 1.42] & 2641 & -1.69 (0.09) &  & \\
Pointing and vocabulary & 0.41 [0.32, 0.49] & 1202 & 0.59 (0.55) &  & \\
\bottomrule
\end{tabular}
\caption{Summary of replicability analyses. \textit{d} = Effect size (Cohen's {\it d}) estimated from a random-effect model; fail-safe-N = number of missing studies that would have to exist in order for the overall effect size to be {\it d} = 0; funnel skew = test of asymmetry in funnel plot using the random-effect Egger's test (Stern \& Eggers, 2005); p-curve skew = test of the right skew of the p-curve using the Stouffer method (Simonsohn, Simmons, \& Nelson, 2015); power = power to reject the null hypothesis at the 5\% significance level based on the p-curve (Simonsohn, Nelson, \& Simmons, 2014);  Brackets give 95\% confidence intervals, and parentheses show p-values.}
\end{table}

Figure 2 shows p-curves for `r  length(unique(pc.data$dataset))` of our `r nrow(datasets)` meta-analyses.\footnote{We were unable to do this analysis on all meta-analyses because some were missing key statistical tests (e.g. gaze following) or the test statistic was not available (e.g. pointing and vocabulary).} With the exception of phonotactic learning, all p-curves show evidence of right skew. This is confirmed by formal analyses (Table 2, column 5).

P-curves also provide a method for calculcating the overall power of a literature, based on the shape of the p-curve [@simonsohn2014power]. Intuitively, when power is high and effect is real, we should be more likely to observe an effect size "further" from the null. This means that we will observe more small effect sizes. Thus, the higher the power, the more right skewed the p-curve will be.  Table 2 (column 6) presents estimates of power for each meta-analysis based on p-curve. With the exception of phonotactic learning (*power* = .14), literatures appear to have acceptable power.

```{r, eval = F}

 This bias can come from a number of sources, such as  data-contigent analytical decisions  (Simmons, 2011; Simonsohn, 2014a, 2014b, 2014c), data-contigent data reporting  (Rosenthal, 1979; Fanelli 2012), or even fraud. 

# filter(table.data, dataset == "Phonotactic learning")$power_string
And, without veridical data, it is difficult to make theoretical progress on a phenomenon.
Across studies we should expect some variability in effect size due to sampling error alone. But this variability in effect size should be *systematic*: There should be less variability around the mean for more precise studies, as measured by sample size. The presence of variability in effect sizes that is not accounted for sample size may suggest publication bias in a literature. A limitation of this approach:only accounts for XXXX.

if researchers are using power analysis appropriately to plan their effect sizes, then true large effects will be studied with small samples and true small effects will be studied with large samples, leading to an asymmetrical funnel plot and the illusion of research bias.

First, when true effect sizes differ across studies, as they inevitably do, the funnel plot and the excessive significance approaches risk falsely concluding publica- tion bias is present when in fact it is not 

Funnel plots should be seen as a generic means of examining small-study effects (the tendency for the smaller studies in a meta-analysis to show larger treatment effects) 

Strictly speaking, funnel plots probe whether studies with little precision (small studies) give different results from studies with greater precision (larger studies). Asymmetry in the funnel plot may therefore result not from a systematic under-reporting of negative trials but from an essential difference between smaller and larger studies that arises from inherent between-study heterogeneity.

* could be quality,  measure of precision affects precision

language bias (selective inclusion of studies published in English); availability bias (selective inclusion of studies that are easily accessible to the researcher); cost bias (selective inclusion of studies that are available free or at low cost); familiarity bias (selective inclusion of studies only from oneâ€™s own discipline, and outcome bias (selective reporting by the author of a primary study of some outcomes but not others, depending on the direction and statistical significance of the results). 
```


# Theoretical Synthesis
METAMETAPLOT


# Discussion
Limitations


#### Author Contributions

#### Acknowledgments

\newpage

### References 

---
nocite: | 
  @lfprep
  @dunst2012preference
  @frank2016performance
  @tsuji2014perceptual
  @bergmann2015development
  @sterne2005regression
...

```{r create_r-references}
r_refs(file = "metalab_synthesis.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{8pt}

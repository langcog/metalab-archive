---
title: "A Quantitative Synthesis of Early Language Acquisition Using Meta-Analysis"
shorttitle: "A Quantitative Synthesis"

author: 
  - name:  Molly Lewis
    affiliation: 1
  - name: Mika Braginsky
    affiliation: 1
  - name: Sho Tsuji
    affiliation: 2
  - name: Christina Bergmann
    affiliation: 2
  - name: Page Piccinini
    affiliation: 2
  - name: Alejandrina Cristia
    affiliation: 2
  - name: Michael C. Frank
    affiliation: 1
affiliation:
  - id: 1
    institution: Department Psychology, Stanford University
  - id: 2
    institution: Laboratoire de Sciences Cognitives et Psycholinguistique, ENS
    
abstract: |
  replicability, etc.
  
note: |  
  Correspondence concerning this article should be addressed to Molly Lewis, Psychology Department, Stanford University. 450 Serra Mall, Stanford, CA 94305. E-mail: mll@stanford.edu.
  
keywords: "developmental psychology,
language acquisition, quantitative theories, meta-analysis"

wordcount: "XXXX"

class: man
lang: english
figsintext: yes
lineno: no
bibliography:
  - metalab_synthesis.bib
header-includes:
  - \usepackage{setspace}
  - \usepackage{float}
  - \usepackage{graphicx}
  - \AtBeginEnvironment{tabular}{\singlespacing}
  - \usepackage{pbox}

output: papaja::apa6_pdf
---

```{r message = FALSE, warning = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE, fig.pos = "T")
require("papaja")
apa_prepare_doc() # prepare document for rendering
```

# Introduction
To learn to speak a language, a child must acquire a wide range of knowledge and skills: the sounds of the language, the word forms, and the mappings of words to meanings, to name only a few. How does this process unfold? Our goal as psychologists is to build a theory that can explain and predict this process. The most constraining aspect of this theory is an account of how the acquistion of individual skills depends on other skills---a sort of "meta-theory" of language development. For example, the theory must describe to what extent a child must solidify knoweldge of language sounds before begining the process of learning the meanings of words. To develop this theory, a pragmatic research strategy has been to study these skills primarily in isolation, describing the developmental trajectories of individual phenomena in separate research programs. This research shows that, while the onset of these skills can often be found in the first year of life, most follow a protracted pattern of development, with important changes occuring in the second year and beyond. Importantly, however, if it is indeed the case that linguistic skills are interdependent, this both casts some doubt on results based on the isolationist method and suggests that we will not be able to understand one skill without a more precise understanding of the broader system. 

```{r}

#But, acquiring a language requires not just learning these skills in isolation; it requires the integration of a range of skills across the language hierarchy. Consider, for example, a child learning the word "dog:" If the child is unable to segment the word from natural speech, learning the meaning of this word is impossible. As a result, it would behoove the research community to take into account the development of the whole system at once.


#Critically, an important aspect feature of this theory must account for how the acquistion of individual skills depends on other others skills.


#One of the most important aspects of this theory is describing the relationship between individuals skills: How does the acquisition of one skill depend on other skills?


#language acq is cool
#many people have provided reviews of how this works
#a mainstream view was based on serial model
#but then findings of early onset + long dev
#overviews got complicated and it got difficult to disambiguate between competing verbal theories - to what extent do results fit better one than the other?
#specially since noisy - see below, so people could pick & choose
```
The theory building effort is further complicated by the fact that we typically have some degree of uncertainty about the trajectory of individual skills. Developmental trajectories are often communicated in the form a verbal description summarizing a body of limited, noisy experimental findings. However, in actual fact, there is often one study finding an effect, but another failing to do so. These contradictions leave the theorist with uncertainty about	which experimental findings should constrain the theory, and are sometimes solved verbally, discounting one or the other finding depending on the viewer's expertise (and potentially their theoretical penchant). What is needed then is a method for resolving these contradictions in a more systematic and principled fashion.

We suggest a solution to both of these challenges---building integrative whole-system views and evaluating evidential strength in a field of scientific research---is to reframe experimental findings in terms of quantitative, rather than qualitative, descriptions. Quantitative descriptions allow for the use of quantitative methods for aggregating experimental findings in order to evaluate evidential strength. In addition, describing experimental findings as quantitative estimates provides a common language for comparing across phenomenon, and a way to make more precise predictions. In this paper, we consider the domain of language acquisition and demonstrate how a set of quantitative tools---meta-analysis---can support these two theory-building goals. 

Meta-analysis is a quantitative method for aggregating across experimental findings. The fundamental unit of meta-analysis is the *effect size*: a scale-free, quantitative measure of "success" in a phenomenon. Importantly, an effect size provides an estimate of the *size* of an effect, as well as a measure of uncertainty around this point estimate. With  such a quantitative measure of success, we can apply the same reasoning we use to aggregate  noisy measurements over  participants in a single study: By assuming each *study*, rather than participant, is sampled from a population, we can appeal to the classical statistical framework to combine estimates of the effect size for a given phenomenon.

Meta-analytic methods support theory building in several ways. First, they provide a way to evaluate which effects in a literature are most likely to be observed consistently, and thus should constrain the theory. This issue is particularly important in light of recent high-profile evidence that an effect observed in one study may not replicate in another ["replication crisis,"  @ioannidis2005most; @open2015estimating; @open2012open]. Failed replications are difficult to interpret, however, because they may result from a wide variety of causes, including an initial false positive, a subsequent false negative, or differences between initial and replication studies, such that making causal attributions in a situation with two conflicting studies is often difficult [@Gilbert1037; @anderson2016response].  By aggregating evidence across studies and assuming that there is some variability in true effect size from study to study, meta-analytic methods can provide more veridical description of the empirical landscape, which in turn leads to better theory-building.

Second, meta-analysis supports theory building by providing higher fidelity descriptions of phenomena. Given an effect size estimate, meta-analytic methods provide a method for quantifying the amount variability around this point estimate. In addition to providing a quantitative measure of success, effect size estimates also provide a common language for comparing \emph{across} phenomena. Indeed, the quantitative framework allows researchers to detect potential moderators in effect size. This ability is particularly important for developmental phenomena because testing a theory requires precise measurement of changes in effect size across development. Individual papers typically describe an effect size for 1-2 age groups, but the ultimate goal is to detect a moderator---age---in this effect. Given that moderators always require more power to detect [@button2013power], it may be quite difficult to detect developmental trends in effect sizes from individual papers. By aggregating across papers through meta-analytic methods, however, we may be able to detect these changes, leading to a more precise description of the empirical phenomena.

In the current work, the common language of effect size allows us to meaningfully consider the relationship between different phenomena in the language acquisition domain ("meta-meta-analysis"). Through cross-phenomena comparisons, we can understand not only the trajectory of a particular phenomenon, like word learning for example, but also how this phenomenon might relate to other skills, such as sound learning, gaze following, and many others. 

Although these tools are broadly applicable to psychological literatures,
language acquisition may be a particularly informative case study. One reason is that language acquisition may be uniquely vulnerable to false findings because running children is expensive, and thus sample sizes are small and studies are underpowered [@ioannidis2005most]. In addition, the high cost and practical difficulties associated with collecting large developmental datasets means that replications are relatively rare in the field. Finally, there has been attention to research practices in developmental psychology broadly, suggesting evidence of experimenter bias [@Peterson:2016].  

We take as our ultimate goal a broad theory of language acquisition that can explain and predict the range of linguistic skills a child acquires. Toward this end, we developed a dataset of effect sizes in the language acquisition literature across 12 core phenomena ([Metalab; http://metalab.stanford.edu/](http://metalab.stanford.edu)). We demonstrate how meta-analysis supports building this theory in three ways. We first use meta-analytic techniques to evaluate the evidential value of the empirical landscape in language acquisition research. We find broadly that this literature has strong evidential value, and thus that the effects reported in the literature should constrain our theorizing of language acquisition. We then turn toward the task of synthesizing these findings across phenomena and finally offer an example of quantitative evaluation of theories.

# Method
```{r load data}
source("../../dashboard/global.R", chdir = TRUE)
source("paper_analyses/pcurve.R")
library(metafor)
library(knitr)
library(lme4)
library(directlabels)
# Tried installling packrat, but got error: Error in hash(descFile) : 
#No DESCRIPTION file at path #'/Documents/GRADUATE_SCHOOL/Projects/metalab/metalab/write-ups/synthesis_paper/packrat#/lib/x86_64-apple-darwin13.4.0/3.2.1/BH/DESCRIPTION'!

#citations - [e.g., @bauer_2014; @bem_2011] 
#citations - @bauer_2014 → Baumer et al. (2014).
#italics - *R* 
#html - [RMarkdown](http://rmarkdown.rstudio.com/)
```

We analyzed `r nrow(datasets)` different phenomena in language acquisition. To a certain extent, these  phenomena were selected opportunistically, either because of high prevalence in the literature or because a published meta-analysis already existed. The phenomena cover development at many different levels of the language hierarchy, from the acquisition of prosody and phonemic contrasts, to gaze following in communicative interaction. This wide range of phenomena allowed us to compare the course of development across different domains, as well as to explore questions about the interactive nature of language acquisition (Table 1). 

To obtain estimates of effect size, we coded papers reporting experimental data (see SI for details). Within each paper, we calculated a separate effect size estimate for each experiment and age group (we refer to this as a "condition"). In total, our sample includes estimates from `r sum(datasets$num_papers)` papers, `r sum(datasets$num_experiments)` different conditions and `r format(floor(sum(datasets$num_subject)), big.mark=",", scientific=FALSE)` participants. The process for selecting papers from the literature differed by domain, with some individual meta-analyses using more systematic approaches than others (see SI).
\renewcommand{\arraystretch}{1.5}
\begin{table}[h!]
		\footnotesize
		\begin{tabular}{lp{4cm} p{5cm}r}
			\toprule
			\textbf{Level} & \textbf{Phenomenon}                                                               & \textbf{Description}                                                                                 & \textbf{N papers (conditions)}                                                                                                                                               \\
						\midrule

			Prosody        & IDS  preference  \newline  {\scriptsize (Dunst, Gorman, \& Hamby, 2012)}          & {\scriptsize  `r datasets[datasets$name == "Infant directed speech preference", "short_desc"]`}      & `r datasets[datasets$name == "Infant directed speech preference", "num_papers"]` (`r datasets[datasets$name == "Infant directed speech preference", "num_experiments"]`)     \\
			Sounds         & Phonotactic learning  \newline {\scriptsize (Cristia, in prep.)}                   & {\scriptsize `r datasets[datasets$name == "Phonotactic learning", "short_desc"]`  }                  & `r datasets[datasets$name == "Phonotactic learning", "num_papers"]` (`r datasets[datasets$name == "Phonotactic learning", "num_experiments"]`)                               \\
			~              & Vowel discrimination (native) \newline {\scriptsize (Tsuji \& Cristia, 2014)}     & {\scriptsize `r datasets[datasets$name == "Vowel discrimination (native)", "short_desc"]`  }         & `r datasets[datasets$name == "Vowel discrimination (native)", "num_papers"]` (`r datasets[datasets$name == "Vowel discrimination (native)", "num_experiments"]`)             \\ 
			~              & Vowel discrimination (non-native) \newline {\scriptsize (Tsuji \& Cristia, 2014)} & {\scriptsize `r datasets[datasets$name == "Vowel discrimination (non-native)", "short_desc"]`  }     & `r datasets[datasets$name == "Vowel discrimination (non-native)", "num_papers"]` (`r datasets[datasets$name == "Vowel discrimination (non-native)", "num_experiments"]`)     \\
			   & Statistical sound learning  \newline {\scriptsize (Cristia, in prep.)}             & {\scriptsize `r datasets[datasets$name == "Statistical sound category learning", "short_desc"]`   }  & `r datasets[datasets$name == "Statistical sound category learning", "num_papers"]` (`r datasets[datasets$name == "Statistical sound category learning", "num_experiments"]`) \\ 
			& Word segmentation \newline {\scriptsize  (Bergmann \& Cristia, 2015) }            & {\scriptsize `r datasets[datasets$name == "Word segmentation", "short_desc"]`  }                     & `r datasets[datasets$name == "Word segmentation", "num_papers"]` (`r datasets[datasets$name == "Word segmentation", "num_experiments"]`)                                     \\
			Words     &   Mutual exclusivity \newline {\scriptsize (Lewis \& Frank, in prep.)} &{\scriptsize  `r datasets[datasets$name == "Mutual exclusivity", "short_desc"]`}
			& `r datasets[datasets$name == "Mutual exclusivity", "num_papers"]` (`r datasets[datasets$name == "Mutual exclusivity", "num_experiments"]`)             \\
			~ &   Sound Symbolism \newline {\scriptsize (Lammertink et al., in prep.)} &{\scriptsize  `r datasets[datasets$name == "Sound symbolism", "short_desc"]`}
			& `r datasets[datasets$name == "Sound symbolism", "num_papers"]` (`r datasets[datasets$name == "Sound symbolism", "num_experiments"]`)             \\
			~              & Concept-label advantage   \newline {\scriptsize (Lewis \& Long, unpublished)}     & {\scriptsize `r datasets[datasets$name == "Label advantage in concept learning", "short_desc"]`    } & `r datasets[datasets$name == "Label advantage in concept learning", "num_papers"]` (`r datasets[datasets$name == "Label advantage in concept learning", "num_experiments"]`) \\
			~              & Online word recognition \newline {\scriptsize (Frank, Lewis, \& MacDonald, 2016)} & {\scriptsize `r datasets[datasets$name == "Online word recognition", "short_desc"]`   }              & `r datasets[datasets$name == "Online word recognition", "num_papers"]` (`r datasets[datasets$name == "Online word recognition", "num_experiments"]`)                         \\
			Communication  & Gaze following  \newline {\scriptsize  (Frank, Lewis, \& MacDonald, 2016)}        & {\scriptsize `r datasets[datasets$name == "Gaze following", "short_desc"]`   }                       & `r datasets[datasets$name == "Gaze following", "num_papers"]` (`r datasets[datasets$name == "Gaze following", "num_experiments"]`)                                           \\
			~              & Pointing and vocabulary  \newline {\scriptsize (Colonnesi et al., 2010)}          & {\scriptsize `r datasets[datasets$name == "Pointing and vocabulary", "short_desc"]`  }               & `r datasets[datasets$name == "Pointing and vocabulary", "num_papers"]` (`r datasets[datasets$name == "Pointing and vocabulary", "num_experiments"]`)                         \\ 
			\bottomrule
		\end{tabular}
		\caption{Overview of meta-analyses in dataset.}
	\end{table}
	


                  
# Replicability of the field
To assess the replicability of language acquisition phenomena, we conducted several diagnostic analyses: Meta-analytic estimates of effect size, fail-safe-N [@orwin1983fail], funnel plots, and p-curve [@simonsohn2014p; @simonsohn2014power; @simonsohn2015better]. These analytical approaches each have limitations, but taken together, they provide converging evidence about whether a true effect is likely to exist, and the extent to which publication bias and other questionable research practices are present in the literature. Overall, we find most phenomena in the language acquisition literature have evidential value, and should therefore provide the basis for theoretical development.  We also find evidence for some bias, as well as evidence that two phenomena---phonotactic learning and statistical sound learning---likely describe null or near-null effects. 

## Meta-Analytic Effect Size
```{r, overall_d}
overall_es <- function(ma_data){
  model = metafor::rma(ma_data$d_calc, ma_data$d_var_calc, method = "REML",
               control = list(maxiter = 1000, stepadj = 0.5))
    data.frame(dataset = ma_data$short_name[1],
               overall.d = model$b,
               ci_lower = model$ci.lb,
               ci_upper = model$ci.ub)
}

all_ds = all_data %>%
  split(.$short_name) %>%
  map(function(ma_data) overall_es(ma_data)) %>%
  bind_rows() %>%
  mutate_each_(funs(round(., digits = 2)), 
               vars = c("overall.d", "ci_lower", "ci_upper")) %>%
  mutate(d_string = paste0(overall.d, " [", ci_lower, ", ",ci_upper, "]")) %>%
  mutate(short_name = dataset)
```
To estimate the overall effect size of a literature, effect sizes are pooled across papers to obtain a single meta-analytic estimate. This meta-analytic effect-size can be thought of as the "best estimate" of the effect size for a phenomenon given all the available data in the literature.

Table 2, column 2 presents meta-analytic effect size estimates for each of our phenomena. We find evidence for a non-zero effect size in 11 out of 12 of the phenomena in our dataset, suggesting these literatures provide evidential value. In the case of phonotactic learning, however, we find that the meta-analytic effect size estimate does not differ from zero, suggesting that this literature does not describe a robust effect. 

We next turn to methods of assessing evidential value that describe the *degree* to which a literature has evidential value, and thus the degree to which it should constrain our theory building. In the following three analyses---fail-safe-N, funnel plots, and p-curves---we attempt to quantify the evidential value of these literatures.


## Fail-safe-N
```{r, fail_safe_N}
fsn.package.data = all_data %>%
  group_by(dataset) %>%
  summarise(fsn_package = fsn(d_calc, d_var_calc, data = all_data, 
                         target = .01, type="Orwin")$fsnum) %>%
  left_join(datasets %>% select(name, short_name), by= c("dataset" = "name")) %>%
  rename(fsn_string = fsn_package)
```

One approach for quantifying the reliability literature is to ask, How many missing studies with null effects would have to exist in the "file drawer" in order for the overall effect size to be zero? This is called the "fail-safe" number of studies [@orwin1983fail]. This number provides an  estimate of the size and variance of an effect using the intuitive unit of studies. To estimate this number, we estimated the overall effect size for each phenomenon (Table 2, column 2), and then used this to estimate the fail-safe-N (Table 2, column 3). 

Because of the large number of positive studies in many of the meta-analyses we assessed, this analysis suggests a very large number of studies would have to be "missing" in each literature ($M$ = `r round(mean(fsn.package.data$fsn_string[is.finite(fsn.package.data$fsn_string)]))`) in order for the overall effect sizes to be 0. Thus, while it is possible that some reporting bias is present in the literature, the large fail-safe-N suggests that the literature nonetheless likely describes a real effect. 

This analysis provides a quantitative estimate of the size of an effect in an intuitive unit, but it does not assess analytical or publication bias (CITE). Importantly, if experimenters are exercising analytical flexibility through practices like selective reporting and p-hacking, then the number and magnitude of observed true effects in the literature may be greatly inflated. In the next analysis, we assess the presence of bias through funnel plots.

## Funnel Plots 
Funnel plots provide a visual method for evaluating whether variability in effect sizes is due  only to differences in sample size. A funnel plot shows effect sizes versus a metric of sample size, standard error. If there is no bias in a literature, we should expect studies to be randomly sampled around the mean, with more variability for less precise studies. 

Figure 1 presents funnel plots for each of our `r nrow(datasets)` meta-analyses. These plots show evidence of asymmetry (bias) for several of our phenomenon (Table 2, column 4). However, an important limitation of this method is that it is difficult to determine the source of this bias. One possibility is that this bias reflects true heterogeneity in phenomena (e.g. different ages)\footnote{The role of moderators such as age can be interactively explored on the website, Metalab  (www.metalab.stanford.edu).}. P-curve analyses provide one method for addressing this issue, which we turn to next.

```{r,  fig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "Funnel plots for each meta-analysis. Each effect size estimate is represented by a point, and the mean effect size is shown as a red dashed line. The funnel corresponds to a 95%  CI around this mean. In the absence of true heterogeneity in effect sizes (no moderators) and bias, we should expect all points to fall inside the funnel."}

CRIT_95 = 1.96 
#CRIT_99 = 2.58

funnel.es.data = all_data %>%
  mutate(dataset = as.factor(dataset),
         dataset= gdata::reorder.factor(dataset, 
                                    new.order = c(2,6,10,11,9,12,4,8,3,5,1,7)),
         dataset = plyr::revalue(dataset, 
                                 c("Infant directed speech preference"="IDS preference",
                                   "Statistical sound category learning"="Statistical sound learning", 
                                   "Label advantage in concept learning"="Concept-label advantage",
                                   "Vowel discrimination (native)"="Vowel discrimination\n(native)",
                                   "Vowel discrimination (non-native)"="Vowel discrimination\n(non-native)"))) %>%
  group_by(dataset) %>%
  mutate(se = sqrt(d_var_calc), 
         es = d_calc, 
         center = mean(d_calc), 
         lower_lim = max(se) + .05 * max(se))


# separate df for 95 CI funnel shape
funnel95.data.wide <- funnel.es.data %>%
                select(center, lower_lim, dataset) %>%
                group_by(dataset) %>%
                summarise(x1 = (center-lower_lim * CRIT_95)[1], 
                          x2 = center[1],
                          x3 = center[1] + lower_lim[1] * CRIT_95,
                          y1 = -lower_lim[1],
                          y2 =  0, 
                          y3 = -lower_lim[1]) 

funnel95.data.x = funnel95.data.wide  %>%
                  select(dataset,  dplyr::contains("x")) %>%
                  gather("coordx", "x", 2:4) %>%
                  arrange(dataset, coordx) %>%
                  select(-coordx)

funnel95.data.y = funnel95.data.wide  %>%
                  select(dataset, dplyr::contains("y")) %>%
                  gather("coordy", "y", 2:4) %>%
                  arrange(dataset, coordy) %>%
                  select(-coordy)

funnel95.data = bind_cols(funnel95.data.x, funnel95.data.y)

# separate df for 99 CI funnel shape
# funnel99.data.wide <- funnel.es.data %>%
#                 select(center, lower_lim, dataset) %>%
#                 group_by(dataset) %>%
#                 summarise(x1 = (center-lower_lim* CRIT_99)[1], 
#                           x2 = center[1],
#                           x3 = center[1] + lower_lim[1] * CRIT_99,
#                           y1 = -lower_lim[1],
#                           y2 =  0, 
#                           y3 = -lower_lim[1]) 
# 
# funnel99.data.x = funnel99.data.wide  %>%
#                   select(dataset, contains("x")) %>%
#                   gather("coordx", "x", 2:4) %>%
#                   arrange(dataset, coordx) %>%
#                   select(-coordx)
# 
# funnel99.data.y = funnel99.data.wide  %>%
#                   select(dataset, contains("y")) %>%
#                   gather("coordy", "y", 2:4) %>%
#                   arrange(dataset, coordy) %>%
#                   select(-coordy)
# 
# funnel99.data = bind_cols(funnel99.data.x, funnel99.data.y)

ggplot(funnel.es.data, aes(x = es, y = -se)) +
  facet_wrap(~dataset, scales = "free") +
  xlab("Effect Size")  +
  ylab("Standard Error\n")  +
  scale_colour_solarized(name = "") +
  geom_polygon(aes(x = x, y = y), 
               data = funnel95.data,
               fill = "white") +
  #geom_polygon(aes(x = x, y = y), 
  #            data = funnel99.data,,
  #            fill = "white") +
  geom_vline(aes(xintercept=x2), 
             linetype = "dashed", color = "red", size = .8, data = funnel95.data.wide) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "darkgrey",  size = .8) +
  geom_point(size = .5) +
  theme(#panel.background = element_rect(fill = "gray"),
        panel.grid.major = element_line(colour = "grey", size = 0.2),
        panel.grid.minor = element_line(colour = "grey", size = 0.5)) 
```

```{r, funnel_skew}
eggers_tests <- function(ma_data){
    # model
    model.mod = metafor::rma(ma_data$d_calc ~ma_data$mean_age_1, ma_data$d_var_calc,
                             method = "REML",
                control = list(maxiter = 1000, stepadj = 0.5))
    
    # Eggers test
    egg.mod.random = regtest(model.mod) 
    
    data.frame(dataset = ma_data$short_name[1],
               egg.mod.random.z = egg.mod.random$zval,
               egg.mod.random.p = egg.mod.random$pval)
}

eggers.data = all_data %>%
  split(.$short_name) %>%
  map(function(ma_data) eggers_tests(ma_data)) %>%
  bind_rows() %>%
  mutate(egg.mod.random.z = round(egg.mod.random.z, digits = 2)) %>%
  mutate(egg.mod.random.p = round(egg.mod.random.p, digits = 2)) %>%
  mutate(egg_string = paste0(egg.mod.random.z, " (",egg.mod.random.p, ")"))  %>%
  rename(short_name = dataset) %>%
  select(short_name, egg_string)
```

## P-curves
```{r p_curve_plots,  fig.pos = "T!",  fig.width=7, fig.height=6, fig.cap = "P-curve for each meta-analysis (Simonsohn, Nelson, & Simmons, 2014), except those for which p-values were unavailable. In the absense of p-hacking, we should expect the observed p-curve (blue) to be right-skewed (more small values). The red dashed line shows the expected distribution of p-values when the effect is non-existent (the null is true). The green dashed line shows the expected distribution if the effect is real, but studies only have 33% power. [WILL FIX LEGEND LATER]"}

ALPHA = .05
P_INCREMENT = .01 

pc.data <- get_all_pc_data(all_data, ALPHA, P_INCREMENT)

pc.data %>%
  group_by(dataset) %>%
  do(get_p_curve_df(., ALPHA, P_INCREMENT)) %>%
  ungroup() %>%
  mutate(dataset = as.factor(dataset),
                 dataset = plyr::revalue(dataset, 
                                         c("Label advantage in concept learning" = "Concept-label advantage")),
         dataset = gdata::reorder.factor(dataset, 
                                    new.order = c(3,5,6,7,2,4,1))) %>%
  ggplot(aes(x = p, y = value, linetype = measure, color = measure)) + 
  geom_line(size = 1) +
  scale_colour_manual(name = "", values = c("red", "green", "blue"), 
                      labels=c("Null of no effect", "Null of 33% power", "Observed")) +
  scale_linetype_manual(values = c("dashed", "dashed", "solid"), guide = FALSE)  +
  ylab("Proportion p-values") +
  xlab("p-value") +
  #scale_colour_discrete(labels=c("baseline", "expected", "observed")) +
  facet_wrap(~ dataset) +
  theme_bw() + 
  theme(legend.position = "top",
        legend.key = element_blank(),
        legend.background = element_rect(fill = "transparent"),
        axis.line= element_line(size = 3),
        axis.text = element_text(colour = "black", size = 14),
        strip.text.x = element_text(size = 9),
        axis.title = element_text(colour = "black", size = 14))
```

```{r, p_curve_skew}
stouffer.data = pc.data %>%
  group_by(dataset) %>%
  do(data.frame(stouffer = stouffer_test(., ALPHA))) %>%
  filter(stouffer.pp.measure == "ppr.full") %>%
  full_join(datasets %>% select(name, short_name), by= c("dataset" = "name")) %>%
  select(short_name,stouffer.Z.pp, stouffer.p.Z.pp) %>%
  mutate_each_(funs(round(., digits = 2)), vars = c("stouffer.p.Z.pp",
                                                    "stouffer.Z.pp")) %>%
  mutate(stouff_string = ifelse(is.na(as.character(stouffer.Z.pp)), "",
                                paste0(stouffer.Z.pp, " (", stouffer.p.Z.pp,")"))) %>%
  select(short_name, stouff_string)
```

A p-curve is the distribution of p-values for the statistical test of the main hypothesis across a literature [@simonsohn2014p; @simonsohn2014power; @simonsohn2015better]. Critically, if there is a robust effect in the literature, the shape of the p-curve should reflect this. In particular, we should expect the p-curve to be right-skewed with more small values (e.g., .01) than large values (e.g., .04). An important property of this analysis is that we should expect this skew independent of any true heterogeneity in the data, such as age. Evidence that the curve is in fact right-skewed would suggest that the literature is not biased, and that it provides evidential value for theory building. 

```{r, make_table}
table.data = left_join(select(all_ds, c(short_name, d_string)), fsn.package.data) %>%
             left_join(eggers.data) %>%
             left_join(stouffer.data) %>%
             select(dataset, dplyr::contains("string")) %>%
             ungroup() %>%
             .[c(2,8,3,4,10,5,7,11,6,12,1,9),]  %>% # reorder rows 
             mutate(dataset = as.factor(dataset),
                 dataset = plyr::revalue(dataset, c("Infant directed speech preference" ="IDS preference", # shorten dataset names
                                                   "Statistical sound category learning"= "Statistical sound learning",
                                                   "Label advantage in concept learning" =  "Concept-label advantage")))

# kable(table.data, format = "latex", booktabs = TRUE) # paste below (so can edit latex)
# header:	\textbf{Phenomenon}& \textbf{\textit{d}} & \textbf{fail-safe-N} & \textbf{funnel skew} & \textbf{p-curve skew} & \textbf{power}  \\
```

\begin{table}[t]
\footnotesize
\begin{tabular}{lrrrr}
\toprule
\textbf{Phenomenon} & \textbf{\textit{d}} & \textbf{fail-safe-N} & \textbf{funnel skew} & \textbf{p-curve skew}\\
\midrule
IDS preference & 0.71 [0.53, 0.89] & 3762 & 1.88 (0.06) & \\
Phonotactic learning & 0.04 [-0.09, 0.16] & 45 & -1.08 (0.28) & -1.52 (0.06)\\
Vowel discrimination (native) & 0.6 [0.5, 0.71] & 9536 & 8.98 (<.01) & -5.42 (<.01)\\
Vowel discrimination (non-native) & 0.66 [0.42, 0.9] & 3391 & 4.13 (<.01) & -3.24 (<.01)\\
Statistical sound learning & -0.14 [-0.27, -0.02] & Inf & -1.87 (0.06) & \\
Word segmentation & 0.2 [0.15, 0.25] & 5645 & 1.54 (0.12) & -9.67 (<.01)\\
Mutual exclusivity & 1.01 [0.68, 1.33] & 6443 & 6.25 (<.01) & -5 (<.01)\\
Sound symbolism & 0.15 [0.04, 0.26] & 538 & -1.32 (0.19) & -2.16 (0.02)\\
Concept-label advantage & 0.4 [0.29, 0.51] & 3928 & 0.31 (0.76) & -6.15 (<.01)\\
Online word recognition & 1.89 [0.81, 2.96] & 2843 & 2.92 (<.01) & \\
Gaze following & 0.84 [0.26, 1.42] & 2641 & -1.69 (0.09) & \\
Pointing and vocabulary & 0.41 [0.32, 0.49] & 1202 & 0.59 (0.55) & \\
\bottomrule
\end{tabular}
\caption{Summary of replicability analyses. \textit{d} = Effect size (Cohen's {\it d}) estimated from a random-effect model; fail-safe-N = number of missing studies that would have to exist in order for the overall effect size to be {\it d} = 0; funnel skew = test of asymmetry in funnel plot using the random-effect Egger's test (Stern \& Eggers, 2005); p-curve skew = test of the right skew of the p-curve using the Stouffer method (Simonsohn, Simmons, \& Nelson, 2015); Brackets give 95\% confidence intervals, and parentheses show p-values.}
\end{table}

Figure 2 shows p-curves for `r  length(unique(pc.data$dataset))` of our `r nrow(datasets)` meta-analyses.\footnote{We did not conduct p-curves on all meta-analyses because previously published meta-analyses did not include the original test statistics in the summary report. In other cases, the key test statistics were inappropriate for p-curve.} With the exception of phonotactic learning, all p-curves show evidence of right skew. This is confirmed by formal analyses (Table 2, column 5).

In sum, then, meta-analytic methods, along with our dataset of effect sizes, provide an opportunity to assess the replicability of the field of language acquisition. Across a range of analyses, we find that this literature shows some evidence for bias, but overall,it is quite robust.

```{r, eval = F}

A literature is more likely to describe a real effect if studies are randomly sampled from the population of all possible  studies that researchers could in principle conduct. This assumption does not mean, however,  that there should be *no* variability in effect size across studies: We should expect random variation around the true mean effect size, with smaller studies showing more variability around this mean. 

Variability in effect sizes will be biased when this assumption of  random study sampling  does not hold. Bias may be introduced by the experimenter in a number of ways, including  failure to publish null findings ["publication bias", @rothstein2006publication; @rosenthal1979file; @Fanelli:2010kf], analytical flexibility [e.g., "p-hacking," @Simmons:2011iw; @simonsohn2014p], reporting errors, or even fraud. These biases are problematic for theoretical development because they  lead to large but often unknown errors in estimates of the effect size. If bias is present in the literature, estimates of effect size may be poor estimates of the true underlying effect size and thus be of limited evidential value. To make theoretical progress, we must therefore distinguish variability in effect sizes due to sample size from varability due to bias.


One possibility is that the failure was observed because the effect is in fact not real. This can occur when there was bias introduced by the experimenter in the original study. But another possibility is that an effect *is* real, but the failure to replicate is due to sampling error. Just as we would not conclude that a phenomenon does not exist because one or two participants do not show an effect, we should not conclude that an effect does not exist on the basis of one or two failed replications. Meta-analytic methods provide a principled approach for distinguising between these two sources of failed studies

 This bias can come from a number of sources, such as  data-contigent analytical decisions  (Simmons, 2011; Simonsohn, 2014a, 2014b, 2014c), data-contigent data reporting  (Rosenthal, 1979; Fanelli 2012), or even fraud. 

# filter(table.data, dataset == "Phonotactic learning")$power_string
And, without veridical data, it is difficult to make theoretical progress on a phenomenon.
Across studies we should expect some variability in effect size due to sampling error alone. But this variability in effect size should be *systematic*: There should be less variability around the mean for more precise studies, as measured by sample size. The presence of variability in effect sizes that is not accounted for sample size may suggest publication bias in a literature. A limitation of this approach:only accounts for XXXX.

if researchers are using power analysis appropriately to plan their effect sizes, then true large effects will be studied with small samples and true small effects will be studied with large samples, leading to an asymmetrical funnel plot and the illusion of research bias.

First, when true effect sizes differ across studies, as they inevitably do, the funnel plot and the excessive significance approaches risk falsely concluding publica- tion bias is present when in fact it is not 

Funnel plots should be seen as a generic means of examining small-study effects (the tendency for the smaller studies in a meta-analysis to show larger treatment effects) 

Strictly speaking, funnel plots probe whether studies with little precision (small studies) give different results from studies with greater precision (larger studies). Asymmetry in the funnel plot may therefore result not from a systematic under-reporting of negative trials but from an essential difference between smaller and larger studies that arises from inherent between-study heterogeneity.

* could be quality,  measure of precision affects precision

language bias (selective inclusion of studies published in English); availability bias (selective inclusion of studies that are easily accessible to the researcher); cost bias (selective inclusion of studies that are available free or at low cost); familiarity bias (selective inclusion of studies only from one’s own discipline, and outcome bias (selective reporting by the author of a primary study of some outcomes but not others, depending on the direction and statistical significance of the results). 
```

# Quantitative Evaluation of Theories

Next, we turn to how these data can be used to constrain and develop theories of language acquisition. 

Meta-analytic methods provide a precise, quantitative description of the developmental trajectory of individual phenomena. Figure 3 presents the developmental trajectories of the phenomena in our dataset at each level in the linguistic hierarchy.\footnote{The Pointing and Vocabulary dataset is excluded from this analysis because it does not contain effect sizes at multiple ages.} By describing how effect sizes change as a function of age,  we can begin to understand what factors might moderate that trajectory, such as aspects of a child's experience or maturation.  For example, the meta-analysis on mutual-exclusivity [bias for children to select a novel object, given a novel word; @markman1988] suggests a steep developmental trajectory of this skill. We can use these data to then build quantitative models to understand how aspects of experience (e.g. vocabulary development) or maturational constraints may be related to this trajectory [e.g., @frank2009using; @mcmurray2012word]. 

[There are also large differences in the relative magnitude of ES of different skills. Theoretical point about overt skills have larger ES]

```{r, fig.pos = 'T',  fig.width=8, fig.height=4, fig.cap = "Effect size plotted as a function of age across all developmental meta-analyses in our dataset. Lines show logarithmic model fits. Each point corresponds to a condition, with the size of the point indicating the number of participants. [WILL FIX LABELS LATER]"}
ld.df = data.frame(dataset = datasets$name,
                   domain = c("prosody", "words", "communication", "sounds",
                              "sounds", "sounds", "sounds", "sounds", "words",
                              "words", "communication", "words"))
ld.df$domain = factor(ld.df$domain, levels=c("prosody","sounds", "words", "communication"))

lin_lmer <- lmer(d_calc ~ mean_age + 
                   (log(mean_age)| dataset), 
                 weights = 1/d_var_calc, 
                 REML = F, data = all_data)

lin_lmer_preds <- predict(lin_lmer, re.form = ~ (log(mean_age)| dataset))

all_data_preds <- all_data %>%
  select(dataset, short_name, d_calc, d_var_calc, mean_age, study_ID, response_mode, n) %>%
  filter(complete.cases(.)) %>%
  mutate(lin_lmer_preds = lin_lmer_preds) %>% 
  filter(dataset != "Gaze following" | lin_lmer_preds > 0) %>%
  filter(dataset != "Pointing and vocabulary")

p3 = ggplot(left_join(all_data_preds, ld.df),
            aes(x = mean_age/365, y = lin_lmer_preds, col = dataset)) +
  facet_grid(~domain) +
  geom_point(aes(size = n, y = d_calc), alpha = .1) + 
  geom_smooth(method="lm", se = FALSE, size = 1, formula = y ~ log(x)) + 
  #ylim(c(-1, 3)) +
  ylim(c(-.3, 2))+
  xlim(0,3.1) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Age (years)") + 
  ylab("Effect size (d)") + 
  theme_bw() + 
  theme(legend.position = "bottom",
        legend.key = element_blank(),
        legend.background = element_rect(fill = "transparent"),
        axis.line= element_line(size = 3),
        axis.text = element_text(colour = "black", size = 20),
        strip.text.x = element_text(size = 16),
        axis.title = element_text(colour = "black", size = 20))

direct.label(p3, list("top.bumptwice", dl.trans(y = y + 0.1), cex=.7))
```



```{r,  fig.width=8, fig.height=4, fig.cap="The left two panels show the developmental trajectories predicted under different meta-theories of language acqusition. The sequentl theory predicts that a child will not begin learning the next skill in the linguistic hierarchy until the previous skill has been mastered. The synergistic theory predicts that multiple skills may be simultaneously acquired. The third panel shows other possible developmental trajectories for an particular phenomenon (decreasing, linear, and non-monotonic). The fourth panel shows the observed meta-analytic data. Effect size is plotted as a function of age from 0-3 years, across 11 different phenomena. These developmental curves suggest there is interactivity across language skills, rather than bottom-up, sequential learning of the linguistic hierarchy.", fig.height=4, fig.width=8, fig.pos = "T"}
# real data
all_data.f = filter(all_data, dataset != "Gaze following" | d_calc > 0) %>%
  filter(dataset != "Pointing and vocabulary") %>%
  filter(mean_age/365 < 3) %>%
  mutate(age.years = mean_age/365) %>%
  select(age.years, dataset, d_calc, n, d_var_calc) %>%
  mutate(model = "observed")

# simulated data
# sequential
x = seq(0, 3, .01)
slope = 1.1
y1 = ifelse((log(x-1) * slope) < 0, NA, log(x-1) * slope)
y2 = ifelse((log(x) * slope) < 0, NA, log(x) * slope)
y3 = ifelse((log(x+1) * slope) < 0, NA, log(x+1) * slope)

# synergistic 
y4 = log((x+1)) * .2
y5 = log((x+1)) * .5
y6 = log((x+1)) * 1.1

# ad hoc 
y7 = .2*x
y8 = dnorm(x, mean = 1.7, sd = .5) 
y9 = log(x+.0001, base = .02) +.3


simulated.development = data.frame(age.years = c(x,x,x,x,x,x,x,x,x),
                                   d_calc = c(y1, y2, y3, y4, y5, y6, y7, y8, y9),
                                   dataset = c(rep(1:3, each=length(x)), rep(1:3, each=length(x)),
                                               rep(1:3, each=length(x))),
                                   model = rep(c("sequential", "synergistic", "ad hoc"), each=length(x)*3),
                                   n = NA,
                                   d_var_calc = NA)

simulated.development.all = mutate(simulated.development, dataset = as.factor(dataset)) %>%
  rbind(all_data.f)

simulated.development.all$model <- ordered(simulated.development.all$model, 
                                            levels = c("sequential", "synergistic", "ad hoc", "observed"))

real.levels = levels(simulated.development.all$dataset)
fake.levels = c("1", "2", "3")

ggplot(simulated.development.all,
       aes(x = age.years, y = d_calc, col = dataset)) + 
  geom_line(data = filter(simulated.development.all, model != "observed"), size = 1) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  #geom_point(aes(size = n), alpha = .25) + 
  geom_smooth(data = filter(simulated.development.all, model == "observed"), method="lm", 
              se = FALSE, formula = y ~ log(x), size = 1) + 
  facet_grid(.~model) +
  scale_colour_manual(values=c( "grey70", "grey35", "grey20", "#F8766D", 
                                "#DB8E00" ,"#AEA200", "#64B200", "#00BD5C" ,"#00C1A7",
                                "#00BADE" ,"#00A6FF", "#B385FF" ,"#EF67EB" ,"#FF63B6"),
                      breaks = real.levels[real.levels!=fake.levels]) +
  #ggtitle("Meta-Meta-Analysis") +
  xlab("Age (years)") +
  ylab("Effect size (d)") +
  ylim(c(-.3, 2)) +
  xlim(0,3.1) +
  theme_bw() + 
  theme(legend.position = "bottom",
        legend.key = element_blank(),
        legend.background = element_rect(fill = "transparent"),
        legend.title = element_blank(),
        axis.line= element_line(size = 3),
        axis.text = element_text(colour = "black", size = 18),
        strip.text.x = element_text(size = 14),
        axis.title = element_text(colour = "black", size = 18))

#x = seq(0, 3, .01)
#y=  dnorm(x, mean = 2.1, sd = .3) 
#sd = data.frame(age.years = x, d_calc = y)

#qplot(data = sd, x = age.years, y = d_calc, geom = "line") +
  #ylim(-1, 3) +
  #geom_hline(aes(yintercept = 0)) +
  #geom_vline(aes(xintercept = 0))

```

In addition, meta-analytic methods provide an approach for synthesizing across different linguistic skills via the language of effect sizes. The ultimate goal is to use meta-analytic data to build a single, quantitative model of the language acquisition system, much like those developed for individual language acquisition phenomena, like word learning. Developing a single quantitative model is a lofty goal, however, and will likely require much more precise description of the phenomena than is available in our dataset. Nevertheless, we can use our data to distinguish between broad meta-theories about the relative timing of the acquistion of different skills.

There are two existing meta-theories in the literature about the dependencies between different skills in language acquisition. The first---the "bottom-up" theory---proposes that linguistic skills are acquired sequentially beginning with skills at the lowest level of the linguistic hierarchy. Under this theory, once a skill is mastered, it can be used to support the acquisition of skills higher in the linguistic hierarchy. In this way, a child sequentially acquires the skills of language, "bootstrapping" from existing knowledge at lower levels to new knowledge at higher levels. There is a wide range of evidence consistent with this view. For example, there is evidence that prosody supports the acquisition of sound categories [CITE], word boundaries [CITE], and even word learning [e.g., @shukla2011prosody]. 

A second possibility is that there is interactivity in the language system such that multiple skills are learned simultaneously across the system. Under this proposal, a child does not wait to begin learning the meanings of words until the sounds of a language are mastered, for example; rather, the child is jointly solving the problem of word learning in concert other language skills. This possibility is consistent with predictions of a class of hierarchical Bayesian models that suggest that more abstract knowledge may be acquired quickly, before lower level information, and may in turn support the acquisition of lower information ["blessing of abstraction," @goodman2011learning]. There is evidence for this proposal from work that suggests word learning supports the acquisition of lower-level information like phonemes [@feldman2013word]. More broadly, there is evidence that higher level skills like word learning may be acquired relatively early in development, likely before lower level skills have been mastered [e.g., @bergelson2016].

These two theories make different predictions about relative trajectories of skills across development. Within the meta-analytic framework,  we can represent  these different trajectories schematically by plotting the effect sizes for different skills across development. In particular, the bottom-up theory predicts serial acquistion of skills (Figure 4; left) while the interactive theory predcts simultaneous acquistion (left center). We can also specifiy an infinite number of other possible trajectories by varying the functional form and parameters of the model. Figure 4 (right center; "ad hoc") shows several other possible trajectories. For example, we might expect that a skill might have a non-monotonic trajectory, increasing with age, and then decreasing. By specifying the shape of these developmental trajectories and the age at which acquisition begins, we can consider many patterns of developmental trajectories. These different patterns, in turn, constrain our meta-theories of development. 

Our data allow us to begin to differentiate between this space of theories. Figure 4 (right) presents a synthetic representation of the developmental trajectories of all the skills in our dataset. We find strong evidence for the simultaneous acquistion of skills---children begin learning even high-level skills, like the meanings of words, early in development, and even low-level skills like sound categories show a protracted period of development. This pattern is consistent with an interactive theory of language aquisition. Moving forward, we can use this approach to distinguish between a larger space of meta-theories and, ultimately, refine our way towards a single quantitative theory of language acquisition.

# Discussion
Building a theory of a complex psychological phenomenon requires making good inductive inferences from the available data. We suggest that meta-analysis can support this process by allowing the researcher to verdically describe the to-be-explained behavior, and to do so with high-fidelity. Here, we apply the  meta-analytic toolkit to the domain of language acquisition---a domain where there are concerns of replicability, and where high-fidelity data is needed to explain its complexity. We find that the existing literature in this domain describe robust phenomena and thus should form the basis of theory development. We then offer a preliminary synthesis of the field by aggregating across language acquisition phenomena. We find evidence that linguistic skills are acquired interactively rather than in a strictly bottom-up fashion.

In this paper, we focused on seldom-discussed theoretical motivations for building meta-analysis. Naturally, there are many other practical reasons for conducting a quantitative synthesis. For example, when planning an experiment, an estimate of the size of an effect on the basis of prior literature can inform the sample size needed to achieve a desired
level of power. Meta-analytic estimates of effect sizes can also aid in design choices: If a certain paradigm or measure tends to yield overall larger effect sizes than another, the strategic researcher might select this paradigm in order to maximize the power achieved with a given
sample size. These and other advantages, illustrated with the same database used here, are explained in "Educational tool paper."

Despite its potential, there are a number of important limitations to the meta-analytic method as tool for theory building in psychological research. One challenging issue is that in many cases method and phenomenon are confounded in our dataset. This is problematic because a method with less noise than another will produce a bigger effect size for the same phenomenon. As a result, it is difficult to determine the extent to which a difference in effect size between two phenomena is due to an underlying difference in the phenomena, or merely to the difference in methods. While this may be true to some extent, we find that method does not have a large impact on effect size for a phenomena, relative to other moderators like age (see SI; WORK ON THIS). Nevertheless, the covariance between method and phenomenon in our dataset limits our ability to directly compare effect sizes across phenomena.

Second, meta-analysis, like all analysis, requires the researcher to make analytical decisions, and these decisions may be subject to the biases of the researcher. We believe that a virtue of the current approach is that we have applied the same analytical method across all phenomena we examined, thus limiting our “degrees of freedom” in the analysis. However, in some cases this uniform approach to data analysis means that we are unable to take into consideration aspects of a particular phenomenon that might be relevant. For example, in a stand-alone meta-analysis on vowel discrimination, Tsuji and Cristia (2014) elected only to include papers that tested at least two different age groups. This was done in order to control for confounds in the dataset, like method [I don’t actually understand what the motivation for this was]. Others however might have reasonably dealt with this issue in another way, by normalizing effect sizes across methods, for example. Notably, this analytical decision has consequences for interpretation: Tsuji and Cristia (2014) found an increase in effect size with age, while the current analysis suggests a decrease. We believe that the systematic, uniform analytical approach used here is the most likely to minimize bias by the researcher and reveal robust psychological phenomena. However, there may be cases where this one-size-fits-all approach is inappropriate, particularly in meta-analyses with high heterogeneity.

There are also limits to this method for inferring a meta theory of language acquisition. Meta-theories of language acquisition suggest a particular causal relationship between different  skills and how they change over development. For example, the interactive theory suggests that skills at lower levels *support* the acquisition at higher levels, even before skills at lower levels are mastered. In the meta-analytic framework, this predicts that there should be simultaneous development of skills across the language hierarchy---as we observe in the current work. Importantly, however, this analysis is inherently correlational, and therefore we are not licensed to infer a causal relationship between acquisition at lower levels and acquisition at higher levels: while the observed pattern is consistent with the interactive theory, it is also possible that there is no causal relationship between skills across the language hierarchy, merely parallel trajectories of acquisition. For this reason, experimental work goes hand-in-hand with meta-analysis to address these types of causal questions.

Finally, there are a number of important limitations to the meta-analytic more broadly. One issue is that the method relies on researchers conducting replications of the same study across a range of ages and, critically, reporting these data so that they can be used in meta-analyses. To the extent that researchers do not conduct these studies, or report the necessary statistics in their write-ups (e.g., means and standard deviations), the meta-analytic method cannot be applied. In addition, the meta-analytic method, as in the case of qualitative forms of synthesis (e.g., literature review), is limited by the potential presence of bias, which can come from a ranges of sources including non-representative participant populations, failure to publish null findings, and analytical degrees-of-freedom. To the extent these biases are present in the literature, methods of synthesizing these findings will also be biased.

In sum, understanding the psychological mechanisms underlying complex phenomena is a difficult inferential task: The researcher must develop a predictive and explanatory theory on the basis of limited and noisy experimental data. Here we have focused on language acquisition as a case study of how meta-analytic methods can be productively leveraged as a tool for theory building. Meta-analytic methods allow the researcher to  determine whether phenomena are robust, synthesize across contradictory findings, and ultimately, build an integrative theory across phenomena. Moving forward, we see this as a powerful tool in the researcher’s toolkit for developing quantitative theories to account for complex psychological phenomena.


#### Author Contributions

#### Acknowledgments

\newpage

### References 

---
nocite: | 
  @lfprep
  @dunst2012preference
  @frank2016performance
  @tsuji2014perceptual
  @bergmann2015development
  @sterne2005regression
...

```{r create_r-references}
r_refs(file = "metalab_synthesis.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{8pt}

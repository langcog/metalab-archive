---
title: "Reproducibility Analyses"
author: "Molly Lewis and Christina Bergmann"
date: "This report was rendered on `r Sys.Date()` and will be automatically re-rendered nightly, reflecting any changes in the data or code."
---
  
# Introduction
```{r, setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE)
ggplot2::theme_set(langcog::theme_mikabr(base_family = "Ubuntu"))
source("../dashboard/global.R", chdir = TRUE)
library(metafor)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(purrr)
library(langcog)
library(broom)
library(knitr)
library(stringr)
all_data = filter(all_data, dataset != "Statistical word segementation")
```

To draw inferences from null hypothesis testing, a range of assumptions must be made. Here we look at our database to examine the extent to which these assumptions are valid within the language acquisition literature. We find broadly that these assumptions are merited. 

# Data availability/reporting standards
An important component of reproducibility is complete description  of data in published report. This is critical both for evaluating an individual study, but also for the purposes of a cumulative science (e.g. meta-analysis). Here we explore the extent to which papers report desired statistics such as test-statistics mean and standard deviation, effect sizes and test statistics.
```{r, fig.width = 11}

counts = all_data %>%
            summarise(test_statistic = sum(!is.na(t) | !is.na(F) | !is.na(r)),
                      means = sum(!is.na(x_1)),
                      SD = sum(!is.na(SD_1)),
                      d = sum(!is.na(d_calc)),
                      g = sum(!is.na(g_calc)),
                      r = sum(!is.na(r_calc)),
                      age_range = sum(!is.na(age_range_1)),
                      gender = sum(!is.na(gender_1))) %>%
            gather("coded_variable", "n") %>%
            mutate(coded = "coded") %>%
            mutate(total = nrow(all_data))  %>%
            mutate(coded_variable = factor(coded_variable, levels = c("d", "g", "r", "means",
                                                                        "SD", "test_statistic",
                                                                        "age_range", "gender")))
counts = counts %>%
             mutate(n = total - n, 
                    coded = "uncoded")  %>%
             bind_rows(counts) %>%
             mutate(n_lab = ifelse(coded == "coded", n, "")) %>%
             arrange(coded)
ggplot(counts) + 
  geom_bar(aes(x = coded_variable, 
               y = n/total,
               fill = coded,
              order = coded), 
           stat = "identity") + 
  ylim(0,1) + 
  ylab("Proportion coded") + 
  xlab("Coded variable") + 
  ggtitle("All data") + 
   #annotate("text", x = 1, y = .9, 
    #        label = paste("N =", counts$total[1]), size = 6) + 
  scale_fill_manual(values=c( "lightgreen", "grey")) + 
  geom_text(aes(label = n_lab, x = coded_variable, y = n/total -.06) )+
  theme_bw() + 
  theme(panel.border = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
       axis.line = element_line(colour = "black"),
       text = element_text(size=20),
       axis.text.x = element_text(angle = 30, hjust = 1))


```
This analysis is in practice difficult because our many of our source MA's include effect sizes that were included by the coders. Similarly, the proportion of coded gender in many cases reflects only that the coder chose not to code that, not that this information was not present. Nevertheless, this analysis gives us a good sense of the the proportion of papers that report means and standard deviations (about two-thirds).

By MA
```{r, fig.width = 10, fig.height = 11}
counts = all_data %>%
            group_by(dataset) %>%
            summarise(test_statistic = sum(!is.na(t) | !is.na(F)),
                      means = sum(!is.na(x_1)),
                      SD = sum(!is.na(SD_1)),
                      d = sum(!is.na(d_calc)),
                      g = sum(!is.na(g_calc)),
                      r = sum(!is.na(r_calc)),
                      age_range = sum(!is.na(age_range_1)),
                      gender = sum(!is.na(gender_1)),
                      total = n()) %>%
            gather(coded_variable, n, -dataset, -total) %>%
            mutate(coded = "coded") %>%
            mutate(coded_variable = factor(coded_variable, levels = c("d", "g", "r", "means",
                                                                        "SD", "test_statistic",
                                                                        "age_range", "gender")))

counts = counts %>%
             mutate(n = total - n, 
                    coded = "uncoded")  %>%
             bind_rows(counts) %>%
             mutate(n_lab = ifelse(coded == "coded", n, "")) %>%
  arrange(coded)

ggplot(counts, aes(fill = coded)) + 
  geom_bar(aes(x = factor(coded_variable), 
               y = n/total),  ## FIX THIS
           stat = "identity",
           position = "fill") + 
  facet_wrap(~dataset, ncol=2) +
  ylim(0,1) + 
  ylab("Proportion coded") + 
  xlab("Coded variable") + 
  scale_fill_manual(values = c("lightgreen", "grey")) + 
  geom_text(aes(label = n_lab,
              x = coded_variable, 
              y = n/total -.06)) +
  theme_bw() + 
  theme(panel.border = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black"),
        text = element_text(size=20),
        strip.background  = element_blank(), 
        axis.text.x = element_text(angle = 30, hjust = 1))
```

# Sample size planning 

If sample size and effect sizes are appropriately coupled, we should expect sample size to decrease as effect size increases. 

## Age.
Given that effect size broadly increases with age, we would expect sample sizes to decrease. 

```{r}
all = all_data %>%
  mutate(n_total = ifelse(!is.na(n_2), n_1 + n_2, n_1)) %>%
  select(dataset, d_calc, 
         mean_age_1, n_total,
         n_excluded_1, num_trials, method)
 
ggplot(all, aes(x = mean_age_1, y = n_total, color = dataset)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~dataset, scales = "free") +
  theme_bw() +
  theme(legend.position="none") +
  xlab("age") +
  ylab("n")
```

## Effect sizes
Here we explore a more direct test that sample sizes are coupled with effect sizes. We test this by looking at the relationship between sample sizes and effect size, after residualizing out the effect of (1) phenomenon, (2) age, and (3) method.

Compute residuals.
```{r}
full.model = rma(d_calc ~ response_mode + method + dataset + mean_age_1, 
        vi = d_var_calc, data = all_data, method = "REML")

residuals = rstandard(full.model)

all_data = all_data %>%
            bind_cols(as.data.frame(residuals$resid),
                      as.data.frame(residuals$z)) %>%
            rename(residual.d = `residuals$resid`, 
                   residual.d.s = `residuals$z`) # standardized
```

```{r}
all_data = all_data %>%
  mutate(n_total = ifelse(!is.na(n_2), n_1 + n_2, n_1))  

ggplot(all_data , aes(y = n_total, x = residual.d.s, color = dataset)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw() +
  xlab("Residual effect size") + 
  ylab("n") +
  facet_wrap(~dataset, scales = "free") +
  theme(legend.position="none")
```

```{r}
all_data %>%
  group_by(dataset) %>%
  do(tidy(cor.test(.$residual.d.s,.$n_total))) %>%
  select(dataset, estimate, p.value) %>%
  mutate(sig = ifelse(p.value < .05, "*", "")) %>%
  kable()
```

Essentially no evidence that researchers are prospectively planning sample size.

# Publication Bias

If studies are randomly sampled from the population of possible studies, effect sizes should not be biased. Here we examine this assumption, using residualized effect sizes.

## Effect sizes

Compute residuals.
```{r}
age.method = rma(mean_age_1 ~ method + dataset, 
        vi = d_var_calc, data = all_data, method = "REML")

residuals.age.method = rstandard(age.method)

all_data = all_data %>%
            bind_cols(as.data.frame(residuals.age.method$resid),
                      as.data.frame(residuals.age.method$z)) %>%
            rename(residual.d.age = `residuals.age.method$resid`, 
                   residual.d.s.age = `residuals.age.method$z`) # standardized

```

```{r}
all_data = all_data %>%
  mutate(year = as.numeric(unlist(lapply(strsplit(unlist(study_ID),
                                                  "[^0-9]+"),  function(x) unlist(x)[2])))) %>%
  mutate(year = ifelse(grepl("submitted",study_ID), 2016, year)) %>%
  mutate(year = ifelse(dataset == "Phonotactic learning" | dataset == "Statistical sound category learning", as.numeric(unlist(lapply(strsplit(unlist(short_cite),
                                                  "[^0-9]+"),  function(x) unlist(x)[2]))), year))

ggplot(all_data , aes(x = year, y = residual.d.s, color = dataset)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "black") +
  facet_wrap(~ dataset, scales = "free_y") +
  #facet_wrap(~ dataset, scales = "free") +
  theme_bw() +
  xlab("published year") +
  theme(legend.position="none")
```

```{r}
all_data %>%
  group_by(dataset) %>%
  do(tidy(cor.test(.$residual.d.s,.$year))) %>%
  select(dataset, estimate, p.value) %>%
  mutate(sig = ifelse(p.value < .05, "*", "")) %>%
  kable()
```

We see some bias of year here: In four cases (gaze following, IDS, statistical sound category, and word segementation), we see effect size decrease with year. For ME, we see a positive effect (but that's probably due to Frank et al 2015).

Here's the same analysis, but fitting meta-analytic moedels instead (which allows us to weight by study precision).

```{r}
overall_es <- function(ma_data){
  # get datasets where we only have one levels for method
  bad_datasets = all_data %>%
    group_by(dataset, method) %>%
    slice(1) %>%
    select(dataset, method) %>%
    group_by(dataset) %>%
    summarize(n = n()) %>%
    filter(n == 1) %>%
    select(dataset)

if(is.element(ma_data$dataset[1], bad_datasets$dataset)){  
        model = metafor::rma(ma_data$d_calc~ ma_data$mean_age_1 +                  ma_data$year, ma_data$d_var_calc, method = "REML",
               control = list(maxiter = 1000, stepadj = 0.5))
} else {
        model = metafor::rma(ma_data$d_calc~ ma_data$mean_age_1 +                  ma_data$year + ma_data$method , ma_data$d_var_calc, method = "REML",
             control = list(maxiter = 1000, stepadj = 0.5))
}
  data.frame(dataset = ma_data$dataset[1],
               year.effect = model$b[3],
               year.pvalue = model$pval[3])
}

all_data %>%
  split(.$dataset) %>%
  map(function(ma_data) overall_es(ma_data)) %>%
  bind_rows() %>%
  mutate(sig = ifelse(year.pvalue < .05, "*", "")) %>%
  kable()
```

Same as before but now here also effect of year for native vowels.

## Age
```{r}
ggplot(all_data, aes(x = year, y = residual.d.s.age, color = dataset)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~ dataset, scales = "free_y") +
  theme_bw() +
  xlab("published year")  +
    theme(legend.position="none")
```
In some literatures, we see a trend toward looking for the phenomenon in younger children over time.

# Journal Bias
```{r}
pat<- "developmental science|cognition|child development|cognitive psychology|journal of experimental child psychology|developmental psychology|plos one|language learning and development|infancy|first language|journal of memory and language|proceedings|language and speech|language and cognitive processes|psychological science|unpublished|journal of phonetics"

all_data$journal = str_extract(tolower(all_data$long_cite), pat)

 all_data %>%
  filter(!is.na(journal)) %>%
  ggplot( aes(x = journal, y = residual.d.s)) +
    geom_violin() +
  geom_dotplot(binaxis='y', stackdir='center', dotsize = .2, aes(fill = dataset)) +

  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6))+
  theme(legend.position="none") +
  geom_hline(aes(yintercept = 0))

all_data %>%
  filter(!is.na(journal)) %>%
  group_by(journal) %>%
  multi_boot_standard(column = "residual.d.s")  %>%
  ggplot( aes(x = reorder(journal, mean), y = mean, fill = journal)) +
  geom_bar(position="dodge", stat="identity") +  
  geom_errorbar(aes(ymin = ci_lower, ymax= ci_upper), 
                width=0.2, position="dodge") +
  ylab("residual effect size") +
  xlab("journal") +
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 70, hjust = 1, size = 9))
```

No evidence of major bias by journal.

# Individual meta-analysis power
## What is the power to determine if the overall effect is greater than zero?
Here I calculated the power to detect an effect using a random effect model (Hedges & Pigott, 2001; Valentine, Pigott, & Rothstein, 2010). Note that this information is redundant with confidence intervals on the estimated effect size, and the power in all of these MAs is essentially 1. Below, I ran a simulation sampling n papers from each meta-analysis, and calculating power. We need surprisingly few studies to get relatively high power.

```{r, eval = F}
calculate_ma_power <- function(sample_data) {
  ALPHA <-  .05
  NTAILS <-  2
  c_crit <-  qnorm(ALPHA/NTAILS, lower.tail = F)
  model <- rma(d_calc, vi = d_var_calc, data = sample_data,
               method = "REML", control = list(maxiter = 1000, stepadj = 0.5))
  lambda  <-  model$b[1] / model$se[1] # Valentine, Eq. [6]
  power <-  1 - pnorm(c_crit - lambda)
  power
}

one_sample <- function(ma_data, summary_function, n) {
   function(k) {
      do.call(summary_function, list(sample_n(ma_data, size = n, replace = F)))
   }
}

all_samples <- function(ma_data, summary_function, n, nboot) {
  sample_values <- 1:nboot %>%
    map(one_sample(ma_data, summary_function, n)) %>%
    unlist()
  data.frame(n = n,
             mean = mean(sample_values),
             ci_lower = ci_lower(sample_values),
             ci_upper = ci_upper(sample_values),
             row.names = NULL)
}

dataset_data <- function(ma_data, summary_function, nboot) {
  overall <- do.call(summary_function, list(ma_data))
  seq(1, nrow(ma_data), by = 1) %>%
    map(function(n) all_samples(ma_data, 
                                summary_function, n, nboot)) %>%
    bind_rows() %>%
    mutate(dataset = unique(ma_data$dataset),
           overall_est = overall)
}

power_data <- all_data %>%
  split(.$dataset) %>%
  map(function(ma_data) dataset_data(ma_data, 
                                     calculate_ma_power, 50)) %>%
  bind_rows()

ggplot(power_data, aes(x = n, y = mean, fill = dataset)) +
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = 0.5) +
  geom_hline(aes(yintercept = overall_est), linetype = "dashed") +
  geom_line( aes(x = n, y = mean)) +
  facet_wrap(~dataset) +
  xlab("Number of effect sizes") +
  ylab("Meta-analytic power") +
  xlim(0,150) +
  scale_fill_solarized(guide = FALSE) +
  theme_bw() +
  theme(text = element_text(family = "Open Sans"),
        panel.grid.minor = element_blank())
```



---
title: "Reproducibility and Publishing Bias"
author: "Molly Lewis and Christina Bergmann"
date: "This report was rendered on `r Sys.Date()` and will be automatically re-rendered nightly, reflecting any changes in the data or code."
---
  
# Introduction
```{r, setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE)
#ggplot2::theme_set(langcog::theme_mikabr(base_family = "Ubuntu"))
source("../dashboard/global.R", chdir = TRUE)
library(metafor)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(purrr)
library(langcog)
library(broom)
library(knitr)
library(stringr)

all_data = filter(all_data, dataset != "Statistical word segementation")
```

To summarize, we find:

* Sample size and effect size are not negatively correlated (in fact they're *positively* correlated), suggesting that researchers are not prospectively planning sample sizes.

* Consonant with previous findings, we find that effect sizes decrease over time.

* We also find that sample sizes increase over time, which may account for the decrease in effect sizes over time.

* We find no effect of impact factor of the journal on effect size.


# Data availability/reporting standards
An important component of reproducibility is complete description  of data in published report. This is critical both for evaluating an individual study, but also for the purposes of a cumulative science (e.g. meta-analysis). Here we explore the extent to which papers report desired statistics such as test-statistics mean and standard deviation, effect sizes and test statistics.

```{r, fig.width = 11}
counts = all_data %>%
            summarise(test_statistic = sum(!is.na(t) | !is.na(F) | !is.na(r)),
                      means = sum(!is.na(x_1)),
                      SD = sum(!is.na(SD_1)),
                      d = sum(!is.na(d_calc)),
                      g = sum(!is.na(g_calc)),
                      r = sum(!is.na(r_calc)),
                      age_range = sum(!is.na(age_range_1)),
                      gender = sum(!is.na(gender_1))) %>%
            gather("coded_variable", "n") %>%
            mutate(coded = "coded") %>%
            mutate(total = nrow(all_data))  %>%
            mutate(coded_variable = factor(coded_variable, levels = c("d", "g", "r", "means",
                                                                        "SD", "test_statistic",
                                                                        "age_range", "gender")))
counts = counts %>%
             mutate(n = total - n, 
                    coded = "uncoded")  %>%
             bind_rows(counts) %>%
             mutate(n_lab = ifelse(coded == "coded", n, "")) %>%
             arrange(coded)
ggplot(counts) + 
  geom_bar(aes(x = coded_variable, 
               y = n/total,
               fill = coded,
              order = coded), 
           stat = "identity") + 
  ylim(0,1) + 
  ylab("Proportion coded") + 
  xlab("Coded variable") + 
  ggtitle("All data") + 
   #annotate("text", x = 1, y = .9, 
    #        label = paste("N =", counts$total[1]), size = 6) + 
  scale_fill_manual(values=c( "lightgreen", "grey")) + 
  geom_text(aes(label = n_lab, x = coded_variable, y = n/total -.06) )+
  theme_bw() + 
  theme(panel.border = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
       axis.line = element_line(colour = "black"),
       text = element_text(size=20),
       axis.text.x = element_text(angle = 30, hjust = 1))


```
This analysis is in practice difficult because our many of our source MA's include effect sizes that were included by the coders. Similarly, the proportion of coded gender in many cases reflects only that the coder chose not to code that, not that this information was not present. Nevertheless, this analysis gives us a good sense of the the proportion of papers that report means and standard deviations (about two-thirds).

By MA
```{r, fig.width = 10, fig.height = 11}
counts = all_data %>%
            group_by(dataset) %>%
            summarise(test_statistic = sum(!is.na(t) | !is.na(F)),
                      means = sum(!is.na(x_1)),
                      SD = sum(!is.na(SD_1)),
                      d = sum(!is.na(d_calc)),
                      g = sum(!is.na(g_calc)),
                      r = sum(!is.na(r_calc)),
                      age_range = sum(!is.na(age_range_1)),
                      gender = sum(!is.na(gender_1)),
                      total = n()) %>%
            gather(coded_variable, n, -dataset, -total) %>%
            mutate(coded = "coded") %>%
            mutate(coded_variable = factor(coded_variable, levels = c("d", "g", "r", "means",
                                                                        "SD", "test_statistic",
                                                                        "age_range", "gender")))

counts = counts %>%
             mutate(n = total - n, 
                    coded = "uncoded")  %>%
             bind_rows(counts) %>%
             mutate(n_lab = ifelse(coded == "coded", n, "")) %>%
  arrange(coded)

ggplot(counts, aes(fill = coded)) + 
  geom_bar(aes(x = factor(coded_variable), 
               y = n/total),  ## FIX THIS
           stat = "identity",
           position = "fill") + 
  facet_wrap(~dataset, ncol=2) +
  ylim(0,1) + 
  ylab("Proportion coded") + 
  xlab("Coded variable") + 
  scale_fill_manual(values = c("lightgreen", "grey")) + 
  geom_text(aes(label = n_lab,
              x = coded_variable, 
              y = n/total -.06)) +
  theme_bw() + 
  theme(panel.border = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black"),
        text = element_text(size=20),
        strip.background  = element_blank(), 
        axis.text.x = element_text(angle = 30, hjust = 1))
```

# Sample size planning 

If sample size and effect sizes are appropriately coupled, we should expect sample size to decrease as effect size increases.  We test this by looking at the relationship between sample sizes and effect size, after residualizing out the effect of (1) phenomenon, (2) age, (3) method, and (4) response mode.

Compute residuals.
```{r}
full.model = rma(d_calc ~ method + response_mode + mean_age_1 + dataset, 
        vi = d_var_calc, data = all_data, method = "REML")

p.model = rma(d_calc ~  method + mean_age_1 + dataset, 
        vi = d_var_calc, data = all_data, method = "REML")

residuals = rstandard(full.model)

all_data = all_data %>%
            bind_cols(as.data.frame(residuals$resid),
                      as.data.frame(residuals$z)) %>%
            rename(residual.d = `residuals$resid`, 
                   residual.d.s = `residuals$z`) # standardized
```

```{r}
all_data = all_data %>%
  mutate(n_total = ifelse(!is.na(n_2), n_1 + n_2, n_1))  

ggplot(all_data , aes(y = n_total, x = residual.d.s, color = dataset)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw() +
  xlab("Standardized residual effect size") + 
  ylab("Sample size") +
  facet_wrap(~dataset, scales = "free") +
  theme(legend.position="none")
```

```{r}
all_data %>%
  group_by(dataset) %>%
  do(tidy(cor.test(.$residual.d.s,.$n_total))) %>%
  select(dataset, estimate, p.value) %>%
  mutate(sig = ifelse(p.value < .05, "*", "")) %>%
  kable()
```

Essentially no evidence that researchers are prospectively planning sample size.

Collapsing across MAs:
```{r}
ggplot(all_data , aes(y = n_total, x = residual.d.s)) +
  geom_point(aes( color = dataset)) +
  geom_smooth(method = "lm") +
  theme_bw() +
  xlab("Standardized residual effect size") + 
  ylab("Sample size") +
  theme(legend.position="none")

kable(tidy(cor.test(all_data$residual.d.s,all_data$n_total)))
```

Here we see a *positive* relationship between effect size and sample size: As effect sizes get bigger, sample sizes get bigger. If researchers were prospectively planning studies, we would expect that opposite pattern.


# Year bias
If studies are randomly sampled from the population of possible studies, effect sizes should not be biased by year. Alternatively, there is previous work suggesting that effect sizes decrease over time in a literature (Jennions & MÃ¸ller, 2002; Leimu & Koricheva 2004, Lehrer 2010). Here we examine this possibiilty, using residualized effect sizes.

```{r}
all_data = all_data %>%
  mutate(year = as.numeric(unlist(lapply(strsplit(unlist(study_ID),
                                                  "[^0-9]+"),  function(x) unlist(x)[2])))) %>%
  mutate(year = ifelse(grepl("submitted",study_ID), 2016, year)) %>%
  mutate(year = ifelse(dataset == "Phonotactic learning" | dataset == "Statistical sound category learning", as.numeric(unlist(lapply(strsplit(unlist(short_cite),
                                                  "[^0-9]+"),  function(x) unlist(x)[2]))), year))

ggplot(all_data , aes(x = year, y = residual.d.s, color = dataset)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "black") +
  facet_wrap(~ dataset, scales = "free_y") +
  theme_bw() +
  xlab("published year") +
  ylab("standardized residual effect size") +
  theme(legend.position="none")
```

We see some bias of year here: In four cases (gaze following, IDS, statistical sound category, and word segementation), we see effect size decrease with year. For ME, we see a positive effect (but that's probably due to Frank et al 2015).

```{r}
all_data %>%
  group_by(dataset) %>%
  do(tidy(cor.test(.$residual.d.s,.$year))) %>%
  select(dataset, estimate, p.value) %>%
  mutate(sig = ifelse(p.value < .05, "*", "")) %>%
  kable()
```

Here's the same analysis, but fitting meta-analytic models instead (which allows us to weight by study precision).

```{r}
overall_es <- function(ma_data){
  # get datasets where we only have one levels for method
  bad_datasets = all_data %>%
    group_by(dataset, method) %>%
    slice(1) %>%
    select(dataset, method) %>%
    group_by(dataset) %>%
    summarize(n = n()) %>%
    filter(n == 1) %>%
    select(dataset)

if(is.element(ma_data$dataset[1], bad_datasets$dataset)){  
        model = metafor::rma(ma_data$d_calc~ ma_data$mean_age_1 + ma_data$year, ma_data$d_var_calc, method = "REML",
               control = list(maxiter = 1000, stepadj = 0.5))
} else {
        model = metafor::rma(ma_data$d_calc~ ma_data$mean_age_1 + ma_data$year + ma_data$method , ma_data$d_var_calc, method = "REML",
             control = list(maxiter = 1000, stepadj = 0.5))
}
  data.frame(dataset = ma_data$dataset[1],
               year.effect = model$b[3],
               year.pvalue = model$pval[3])
}

all_data %>%
  split(.$dataset) %>%
  map(function(ma_data) overall_es(ma_data)) %>%
  bind_rows() %>%
  mutate(sig = ifelse(year.pvalue < .05, "*", "")) %>%
  kable()
```
Same as before but now here also effect of year for native vowels.

Collapsing across meta-analyses.
```{r}
ggplot(all_data , aes(x = year, y = residual.d.s, color = dataset)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "black") +
  theme_bw() +
  xlab("published year") +
  ylab("standardized residual effect size") +
  theme(legend.position="none")
```

Effect size decreases over time. It's not clear what the right analysis here is.

Correlation with raw effect sizes in significant.
```{r}
kable(tidy(cor.test(all_data$d_calc,all_data$year)))
```

Correlation with residualized effect sizes is not significant.
```{r}
kable(tidy(cor.test(all_data$residual.d.s,all_data$year)))
```

RMA with year as moderator is significant when method is included as moderator.
```{r}
rma(d_calc ~ method + mean_age_1 + dataset + year, 
        vi = d_var_calc, data = all_data, method = "REML")
```

RMA with year as moderator is significant when response mode is included as moderator.
```{r}
rma(d_calc ~ response_mode + mean_age_1 + dataset + year, 
        vi = d_var_calc, data = all_data, method = "REML")
```

RMA with year as moderator is marginal when response mode and method are included as moderator.
```{r}
rma(d_calc ~ response_mode + method + mean_age_1 + dataset + year, 
        vi = d_var_calc, data = all_data, method = "REML")
```

## Sample size and year
One possible explanation for the observed decrease in effect size over time is that sample sizes increase over time. We find evidence to support this.
```{r}
ggplot(all_data , aes(x = year, y = n_total, color = dataset)) +
  geom_point() +
  facet_wrap(~dataset) +
  geom_smooth(method = "lm", colour = "black") +
  theme_bw() +
  xlab("published year") +
  ylab("standardized residual effect size") +
  theme(legend.position="none")
```

Again, not sure what the right analysis is here. But, predicting sample size with year, controling for everything else, sample sizes tend to get bigger with time.
```{r}
kable(tidy(lm(n_total ~ year + response_mode + dataset + method, data = all_data)))
```

# Journal Bias
Do some journals publish more robust effects than others?
```{r, fig.height = 8}
pat <- "developmental science|cognition|child development|cognitive psychology|journal of experimental child psychology|developmental psychology|plos one|language learning and development|infancy|first language|journal of memory and language|proceedings|language and speech|language and cognitive processes|psychological science|unpublished|journal of phonetics|journal of cognition and development|journal of child language|poster|the journal of the acoustical society of america|perception [&] psychophysics|journal of experimental psychology: human perception and performance|psychonomic bulletin [&] review|journal of speech, language, and hearing research|frontiers in psychology|cortex|science|infant behavior and development|bmc neuroscience|the journal of neuroscience|developmental psychobiology|international journal of bilingualism|psicothema|language learning [&] development"

all_data$journal = str_extract(tolower(all_data$long_cite), pat)

 all_data %>%
  filter(!is.na(journal)) %>%
  ggplot( aes(x = journal, y = residual.d.s, fill = journal)) +
  geom_violin() +
  #geom_dotplot(binaxis='y', stackdir='center', dotsize = .2, aes(fill = dataset)) +
  ylab("Standardized residual effect size") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 70, hjust = 1, size = 9))+
  theme(legend.position="none") +
  geom_hline(aes(yintercept = 0)) 

all_data %>%
  filter(!is.na(journal)) %>%
  group_by(journal) %>%
  multi_boot_standard(column = "residual.d.s")  %>%
  ggplot( aes(x = reorder(journal, mean), y = mean, fill = journal)) +
  geom_bar(position="dodge", stat="identity") +  
  geom_errorbar(aes(ymin = ci_lower, ymax= ci_upper), 
                width=0.2, position="dodge") +
  ylab("Standardized residual effect size") +
  xlab("journal") +
  theme_bw() + 
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 70, hjust = 1, size = 9)) 
 
```

No evidence of major bias by journal.

## Impact factor
Is the impact factor of a journal related to its effect size?
```{r}
d.if = read.csv("impact_factors.csv")

all_data = left_join(all_data, d.if, by = "journal")

ggplot(all_data, aes(IF)) +
  geom_histogram() +
  theme_bw()

all_data %>%
  filter(!is.na(IF)) %>%
  filter(IF < 10) %>%
  ggplot(aes(x= IF, y = residual.d.s)) +
  geom_point(aes(color = dataset)) +
  xlab("Impact factor") +
  ylab("standardized residual effect size") +
  geom_smooth(method = "lm", color = "black") +
  theme_bw() +
  theme(legend.position="none") 
```

No evidence for an effect of impact factor on effect size.

Correlation between residualized effect sizes and impact factors:
```{r}
all_data %>%
  filter(!is.na(IF)) %>%
  filter(IF < 10)  %>%
  do(tidy(cor.test(.$residual.d.s,.$IF))) %>%
  kable()
```

No evidence of impact factor on residiualized effect size.

Meta-analytic model:
```{r}
rma(d_calc ~ response_mode + method + mean_age_1 + dataset + IF, 
        vi = d_var_calc, data = all_data, method = "REML")
```

### Are impact factors related to sample sizes?
```{r}
all_data %>%
  filter(!is.na(IF)) %>%
  filter(IF < 10) %>%
  ggplot(aes(x= IF, y = n_total)) +
  geom_point(aes(color = dataset)) +
  xlab("Impact factor") +
  ylab("Sample size") +
  geom_smooth(method = "lm", color = "black") +
  theme_bw() +
  theme(legend.position="none") 

all_data %>%
  filter(!is.na(IF)) %>%
  filter(IF < 10)  %>%
  do(tidy(cor.test(.$n_total,.$IF))) %>%
  kable()
```

No evidence of impact factor on sample size.

```{r, eval = F}
# Citation network
# using google scholar
```

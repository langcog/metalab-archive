---
title: Metalab Research Networks from WOS - individ papers
author: Molly Lewis
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    highlight: tango
    toc_float: true
    theme: cerulean
    code_folding: hide
---

```{r, setup, include = FALSE}
rm(list=ls())

library(knitr)
library(tidyr)
library(langcog)
library(tidyverse)
library(stringr)
library(bibliometrix) 
library(ggnetwork)
library(intergraph)
library(igraph)
library(broom)
library(lme4)
library(metafor)

knitr::opts_chunk$set(fig.width = 8, fig.height = 3, echo = TRUE,
                      warning = FALSE, message = FALSE, cache = FALSE)
```

_Question_: Are properties of the commmunity structure of a literature predictive of effect size measures?

These analy

***
***

Parameters:
```{r}
MIN_NUM_PAPERS <- 4 # per MA

# Analysis-network pairs of interest
ANALYSES <- c("co-citation", "collaboration","coupling", "co-occurrences")
NETWORKS <- c("references", "authors", "authors", "keywords")
```

# Read and process data

## (1) Web of Science data

Get dois for each paper in metalab dataset
(obtained from: https://apps.crossref.org/SimpleTextQuery/) .
```{r}
# copy-pasting 50 at a time into search engine
dois <- read.csv("dois.csv") %>%
      select(study_ID, doi) %>%
      filter(doi != "" & !is.na(doi)) %>%
      mutate(doi = ifelse(grepl(".org/", doi),
                          unlist(lapply(str_split(doi, ".org/"), 
                                   function(x) {x[2]})), as.character(doi)))
```

Scrape data from web of science at paper level using bibliometrix package
```{r, echo = FALSE}
## copy paste this string into WOS search engine
string = ""
for (i in 1:length(dois$doi)) {
  string = paste(string, dois$doi[i], sep = " OR ")
}

## search web of science-> save to marked list -> download marked list as .txt
wos.raw <- readFiles("wos_data.txt")

wos <- convert2df(wos.raw, dbsource = "isi", format = "plaintext") %>%
  mutate(DI = tolower(DI),
         DI = str_replace_all(DI, "//", "/"),
         ID = str_replace_all(ID, ";;", ";")) %>%
  arrange(DI) %>%
  filter(!is.na(DI)) %>%
  mutate_each(funs(as.factor), -AU, -DE, -ID)

wos <- metaTagExtraction(wos, Field = "AU_CO", sep = ";")

#write.csv(wos, "wos_bib1_6.csv")
```

For each paper in a meta-analysis, merge in a doi. Then for each doi, get the mean d_calc, d_var_calc, n (average across conditions/experiments in the same paper). This is what we're trying to predict.
```{r}
source("../../dashboard/global.R", chdir = TRUE) # all_data source

all_mas = read.csv("ES_data_for_networks2.csv")

paper.data = all_data %>%
  select(short_name, study_ID, d_calc, d_var_calc, n_1, n_2) %>%
  inner_join(dois, by = "study_ID") %>%
  mutate(doi = tolower(doi)) %>%
  group_by(doi, short_name) %>%
  summarize(n = mean(c(n_1, n_2), na.rm = T),
            d_calc = mean(d_calc), 
            d_var_calc = mean(d_var_calc)) %>%
  select(short_name, d_calc, d_var_calc, n) %>%
  left_join(all_mas %>% select(short_name, overall.d))
```

Add in residuals
```{r}
single_method_datasets = all_data %>%
  group_by(short_name) %>%
  summarise(n_methods = length(levels(as.factor(method)))) %>%
  filter(n_methods == 1) %>%
  .[["short_name"]]

# get model fits
all_data.resid = data.frame()
for (i in 1:length(datasets$short_name)) {
    d = filter(all_data, short_name == datasets$short_name[i])
    if (datasets$short_name[i] %in% single_method_datasets) {
      full.model = rma(d_calc ~ mean_age, vi = d_var_calc, data = d)
    } else {
      #full.model = rma(d_calc ~ method + mean_age, vi = d_var_calc, data = d)
       full.model = rma(d_calc ~ mean_age, vi = d_var_calc, data = d)
    }
  
  d = as.data.frame(rstandard(full.model)$resid) %>%
       cbind(d) %>%
       rename(residual.d = `rstandard(full.model)$resid`) %>%
       mutate(residual.d = residual.d + full.model$b[1]) %>% # add in intercept term
       inner_join(all_data) 
  
  all_data.resid = all_data.resid %>%
                      bind_rows(d) %>%
                      select(short_name, study_ID, residual.d)
}

paper.data = all_data.resid %>%
               left_join(dois, by = "study_ID") %>%
               filter(!is.na(doi)) %>%
               group_by(doi, short_name) %>%
               summarize(residual.d = mean(residual.d)) %>%
               ungroup() %>%
               mutate(doi = tolower(doi)) %>%
               inner_join(paper.data)
```

Merge in web of science data for each doi.

Number of papers with dois in web of science in each MA:
```{r}
ns = paper.data %>%
  inner_join(wos, by = c("doi" = "DI")) %>%
  group_by(short_name) %>%
  summarize(n.papers = n()) 

kable(ns)

paper.data = paper.data %>%
  left_join(ns) %>%
  filter(n.papers > MIN_NUM_PAPERS) 

n.mas = length(unique(paper.data$short_name))
```

MAs with `r MIN_NUM_PAPERS` or fewer papers are excluded. This leaves us with `r n.mas` MAs. Note that we're losing papers here in two ways - those that don't have dois (e.g. conference papers, n = approx. 40) and those with dois that are not in web of science (n = approx. 50).


## (2) Network data
```{r}
getNodeData <- function (my.paper.data, 
                          this.analysis,
                          this.network,
                          remove.multiple = TRUE,
                          noloops = TRUE) {
    
    # FOR DEBUGGING
     #my.paper.data = filter(paper.data, short_name == unique(paper.data$short_name)[2])
     #this.analysis = "collaboration"
     #this.network = "authors"
  
    # get web of science data for domain
    this.wos = left_join(my.paper.data, wos, by = c("doi" = "DI")) %>%
             as.data.frame() # necessary for biblioNetwork
    
    #sep = ifelse(this.analysis == "co-citation", ". ", "; ") #bibliometrix_1_5
    sep = ifelse(this.analysis == "co-citation", ". ", ifelse(this.network == "authors", ";", "; ")) #bibliometrix_1_6

    # make the network
    this.net <- biblioNetwork(this.wos, 
                   analysis = this.analysis, 
                   network = this.network, 
                   sep = sep) 
    
    if (length(this.net > 0)) {
      # munge the network
      graph <- graph.adjacency(this.net, mode = "undirected")
      graph <- igraph::simplify(graph, remove.multiple = remove.multiple, 
                          remove.loops = noloops)
      
        # make into df for plotting
      gn = asNetwork(graph)
      

    }
    
    cc <- try(ggnetwork(gn), silent = TRUE)  # gets rid of too-small networks
    
    if(!is(cc,"try-error")) {
      deg.df = data.frame(analysis = this.analysis, network = this.network, 
                          node_name = names(degree(graph)), 
                          degree = degree(graph), 
                          closeness = closeness(graph), row.names = NULL)
       return(deg.df)
    } 
}
```

Make four different networks out of each MA. Then get individual node degrees and closeness.
```{r}
analyses_list <- rep(ANALYSES, each = n.mas) 
networks_list <- rep(NETWORKS, each = n.mas)

dfs <- paper.data %>% 
  mutate_each(funs(as.factor)) %>%
  split(.$short_name) %>%
  rep(length(ANALYSES))

args = list(dfs, analyses_list, networks_list)
```

dfs is a list of dataframes containing length(analyses_list) * length(networks_list) number of copies of each raw meta-analysis data (each row is an effect size in an experiment).


Now, for each MA and network-type we create a network, and return the mean degree and log mean closeness of every node in that network. 
```{r}
node.data = args %>%
  pmap(getNodeData) %>%
  bind_rows(.id = "short_name")
```

# Individual papers{.tabset}

## co-citation and references
```{r, eval = F}
paper.data2 = paper.data %>%
              left_join(node.data %>% 
                         filter(analysis == "co-citation" & 
                                network == "references") %>%
                           rename(doi = node_name) %>%  
                           select(short_name, doi, degree))


makeNodePlots(paper.data2 %>% rename(mean.degree = degree))
```

## collaboration and authors
```{r, eval = F}
author.collaborations = paper.data %>%
                        inner_join(wos %>% select(DI, AU), 
                                   by = c("doi" = "DI")) %>%
                        mutate(authors =  strsplit(AU, ";"))

ac.node.data = node.data %>% 
              filter(analysis == "collaboration" & 
                       network == "authors")

get_mean_author_degree <- function(this.doi, node.data){
  
  these.authors = author.collaborations %>% 
                    filter(doi == this.doi) %>%
                    ungroup() %>%
                    select(authors) %>%
                    unlist() %>%
                    as.character()
  
  author.degrees = map(these.authors, 
        function(x){data.frame(filter(node.data, node_name == x))}) %>%
    bind_rows()
    
  mean.author.degrees = mean(author.degrees$degree, na.rm = TRUE)
  
  data.frame(doi = this.doi, mean.degree = mean.author.degrees)
}
  
author.data = author.collaborations$doi %>%
  map(get_mean_author_degree, ac.node.data) %>%
  bind_rows() %>%
  left_join(paper.data)

makeNodePlots(author.data)
```

MODELS
```{r, eval = F}
summary(lmer(d_calc ~ mean.degree + n + (1|short_name), author.data))
summary(lmer(residual.d ~ mean.degree + n + (1|short_name), author.data))
```


## coupling and authors

This is some amount of work to do.
```{r, eval = F}
author.coupling = paper.data %>%
                        inner_join(wos %>% select(DI, AU), 
                                   by = c("doi" = "DI")) %>%
                        mutate(authors =  strsplit(AU, ";"))

ac.node.data.coupling = node.data %>% 
              filter(analysis == "coupling" & 
                       network == "authors")

author.data = author.collaborations$doi %>%
      map(get_mean_author_degree, ac.node.data.coupling) %>%
      bind_rows() %>%
      left_join(paper.data)
```

## co-occurence and keywords
```{r, eval = F}
keywords =  paper.data %>%
              inner_join(wos %>% select(DI, ID), 
                                   by = c("doi" = "DI")) %>%
              mutate(keywords =  strsplit(ID, ";"))

k.node.data = node.data %>% 
              filter(analysis == "co-occurrences" & 
                       network == "keywords")

get_mean_keyword_degree <- function(this.doi, node.data){
  
  these.keywords = keywords %>% 
                    filter(doi == this.doi) %>%
                    ungroup() %>%
                    select(keywords) %>%
                    unlist() %>%
                    as.character()
  
  keyword.degrees = map(these.keywords, 
        function(x){data.frame(filter(node.data, node_name == x))}) %>%
    bind_rows()
    
  mean.keyword.degrees = mean(keyword.degrees$degree, na.rm = TRUE)
  
  data.frame(doi = this.doi, mean.degree = mean.keyword.degrees)
}
  
keyword.data = keywords$doi %>%
    map(get_mean_keyword_degree, k.node.data) %>%
    bind_rows() %>%
    left_join(paper.data)

makeNodePlots(keyword.data)
```

```{r, eval = F}
summary(lmer(d_calc ~ mean.degree + n + (1|short_name), keyword.data))
summary(lmer(residual.d ~ mean.degree + n + (1|short_name), keyword.data))

```

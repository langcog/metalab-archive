---
title: "Developmental curve comparison"
author: "Page Piccinini and Michael Frank"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    highlight: tango
    theme: united
---

```{r, include=FALSE}
knitr::opts_chunk$set(fig.width = 8, fig.height = 5, echo = TRUE,
                      warning = FALSE, message = FALSE, cache = TRUE)
ggplot2::theme_set(langcog::theme_mikabr(base_family = "Ubuntu"))
source("../dashboard/global.R", chdir = TRUE)
```

```{r, echo=FALSE, cache=FALSE}
library(knitr)
library(ggplot2)
library(broom)
library(dplyr)
library(tidyr)
library(lme4)
library(metafor)
library(RColorBrewer)
library(langcog)
```

# Introduction

The goal for this analysis was to better understand the shape of the developmental curve. To do this we examined both general curves across all meta-analyses and curves taking into account meta-analysis specific variance.

Summary of investigations:

* Running regressions over the entire data set is not appropriate.
* Meta-analytic mixed-effects are promising, but...
* The random effects structure (specifically if we nest paper effects within data sets) makes a big difference to the implied curve shape.

# Simple Regressions

## One Predictor

First we looked at the entire data set with various curve types: 1) linear, 2) log, and 3) quadratic.

```{r, echo=FALSE}
ggplot(all_data, aes(x = mean_age, y = d_calc)) +
  geom_point(col = "black", aes(size = n)) + 
  geom_smooth(method="lm", formula = y ~ x, aes(col = "Linear"), 
              se = FALSE) + 
  geom_smooth(method="lm", formula = y ~ log(x), aes(col = "Log"), 
              se = FALSE) + 
  geom_smooth(method="lm", formula = y ~ I(x^2), aes(col = "Quadratic"), 
              se = FALSE) + 
  geom_hline(yintercept = 0, lty = 2, col = "black") + 
  xlab("Mean age in days") +
  ylab("d (original and calculated)") +
  scale_colour_solarized(name="Models", breaks=c("Linear", "Log", "Quadratic"),
                          labels=c("Linear" = "Linear",
                                   "Log" = "Log",
                                   "Quadratic" = "Quadratic"))
```

To see if one particular type of model is a better fit of the data, we built each model and then compared them with a Chi-squared test. Head to head comparisons find no significant differences between any of the three models.

```{r}
lin_lm <- lm(d_calc ~ mean_age, weights = 1/d_var_calc, data = all_data)
log_lm <- lm(d_calc ~ log(mean_age), weights = 1/d_var_calc, data = all_data)
quad_lm <- lm(d_calc ~ I(mean_age^2), weights = 1/d_var_calc, data = all_data)

kable(coef(summary(lin_lm)))
kable(coef(summary(log_lm)))
kable(coef(summary(quad_lm)))

kable(anova(lin_lm, log_lm, test = "Chisq"))
kable(anova(lin_lm, quad_lm, test = "Chisq"))
kable(anova(log_lm, quad_lm, test = "Chisq"))
```

Overall these results suggest that when modelling the full data set no particular curve best represents the data.

## Two Predictors

We also built a model that included two predictors. Specifically, we built a model that includes both mean age as a predictor and the log of mean age as a predictor. A figure with the two single predictor regressions (linear and log) and the new two predictor regression is provided below.

```{r, echo=FALSE}
ggplot(all_data, 
       aes(x = mean_age, y = d_calc)) + 
  geom_point(aes(size = 1/d_var_calc)) + 
  geom_smooth(method = "lm", formula = y ~ x, 
              se = FALSE, aes(col = "Linear")) +
  geom_smooth(method = "lm", formula = y ~ log(x), 
              se = FALSE, aes(col = "Log")) + 
  geom_smooth(method = "lm", formula = y ~ x + log(x), 
              se = FALSE, aes(col = "Linear and Log")) +
  xlab("Mean age in days") +
  ylab("d (original and calculated)") +
  scale_colour_solarized(name="Models", breaks = c("Linear", "Log", "Linear and Log"),
                                     labels=c("Linear" = "Linear",
                                              "Log" = "Log",
                                              "Linear and Log" = "Linear and Log"))
```

Similar to the comparison of the single predictor models, we built a new model with our two predictors and then used Chi-squared tests to compare it to our two single predictor models. In both comparisons the model with both the linear and logarithmic predictors of mean age outperformed the two simple, single predictor models.

```{r}
lin_log_lm <- lm(d_calc ~ mean_age + log(mean_age), 
                  weights = 1/d_var_calc, 
                  data = all_data)

kable(coef(summary(lin_log_lm)))

kable(anova(lin_lm, lin_log_lm, test = "Chisq"))
kable(anova(log_lm, lin_log_lm, test = "Chisq"))
```

While it is interesting to learn that the model with both types of predictors is a better fit of the data, in general our previous conclusion is confirmed. Modeling the whole data set with simple regressions is not the best approach. 


# Mixed-effects regression

## Simple mixed-effects

Since modeling the entire data set with simple regressions did not result in one model clearly winning out as superior, we moved to mixed-effects regression models. Below is our data set with the curves previously analyzed but separated by meta-analysis.

```{r, echo=FALSE}
ggplot(all_data, aes(x = mean_age, y = d_calc, 
                     weight = 1/d_var_calc)) +
  geom_point(aes(size = 1/d_var_calc)) + 
  geom_smooth(method="lm", formula = y ~ x, 
              aes(col = "Linear"), 
              se = FALSE) + 
  geom_smooth(method="lm", formula = y ~ log(x), 
              aes(col = "Log"),
              se = FALSE) + 
  geom_smooth(method="lm", formula = y ~ I(x^2),
              aes(col = "Quadratic"),
              se = FALSE) +
  geom_smooth(method="lm", formula = y ~ x + log(x), 
              aes(col = "Linear and Log"),
              se = FALSE) + 
  facet_wrap(~ dataset, scales = "free") + 
  geom_hline(yintercept = 0, lty = 2, col = "black") + 
  xlab("Mean age in days") +
  ylab("d (original and calculated)") +
  scale_colour_solarized(name="Models", breaks = c("Linear", "Log",
                                                "Quadratic", "Linear and Log"),
                                     labels=c("Linear" = "Linear",
                                              "Log" = "Log",
                                              "Quadratic" = "Quadratic",
                                              "Linear and Log" = "Linear and Log"))
```

As a second step we ran mixed effects regressions where meta-analysis (data set) was included as a random intercept for our four models: 1) linear, 2) logarithmic, 3) quadratic, and 4) joint linear and logarithmic. As for the simple regressions, we then ran Chi-squared tests to compare the different models to one another. Model comparison found the quadratic model to be a better fit than the linear model and the logarithmic model, but no difference between the linear and logarithmic model. The two predictor model (linear and log) also outperformed both the single predictor linear model and the single predictor logarithmic model.

```{r}
lin_lmer <- lmer(d_calc ~ mean_age + 
                (1 | dataset), 
                weights = 1/d_var_calc, 
                REML = F, data = all_data)

log_lmer <- lmer(d_calc ~ log(mean_age) + 
                (1 | dataset), 
                weights = 1/d_var_calc, 
                REML = F, data = all_data)

quad_lmer <- lmer(d_calc ~ I(mean_age^2) + 
                 (1 | dataset), 
                 weights = 1/d_var_calc, 
                 REML = F, data = all_data)

lin_log_lmer <- lmer(d_calc ~ mean_age + log(mean_age) + 
                    (1| dataset), 
                    weights = 1/d_var_calc, 
                    REML = F, data = all_data)

kable(coef(summary(lin_lmer)))
kable(coef(summary(log_lmer)))
kable(coef(summary(quad_lmer)))
kable(coef(summary(lin_log_lmer)))

kable(anova(lin_lmer, log_lmer))
kable(anova(lin_lmer, quad_lmer))
kable(anova(log_lmer, quad_lmer))
kable(anova(lin_lmer, lin_log_lmer))
kable(anova(log_lmer, lin_log_lmer))
```

Focusing on the linear, log, and two predictor models, we built predictions for all complete meta-analyses. The plot separated by meta analysis is presented below. The linear and two predictor models are pretty similar, mainly being different at very early ages.

```{r}
all_data_preds <- all_data %>%
  select(dataset, d_calc, d_var_calc, mean_age, unique_ID) %>%
  filter(complete.cases(.)) %>%
  mutate(lin_pred = predict(lin_lmer)) %>%
  mutate(log_pred = predict(log_lmer)) %>%
  mutate(lin_log_pred = predict(lin_log_lmer))
```

```{r, echo=FALSE}
ggplot(all_data_preds, aes(x = mean_age, y = d_calc)) +
  geom_point(aes(size = 1/d_var_calc)) + 
  geom_line(aes(x = mean_age, y = lin_pred, size = 1, col = "Linear")) + 
  geom_line(aes(x = mean_age, y = log_pred, size = 1, col = "Log")) + 
  geom_line(aes(x = mean_age, y = lin_log_pred, size = 1, col = "Linear and Log")) + 
  facet_wrap(~ dataset, scales = "free") + 
  geom_hline(yintercept = 0, lty = 2, col = "black") + 
  xlab("Mean age in days") +
  ylab("d (original and calculated)") +
  scale_colour_solarized(name="Models", breaks = c("Linear",
                                                "Log",
                                                "Linear and Log"),
                                     labels=c("Linear" = "Linear",
                                              "Log" = "Log",
                                              "Linear and Log" = "Linear and Log"))
```

Overall this method looks pretty good. It seems like the linear predictor in the two predictor model is smoothing the logs at the edges, and the random effects are working to capture cross data set variability. 

## Complex mixed-effects (nested paper effects)

Since adding meta-analysis as a random effect appeared to improve the model we decided to expand our random effects structure to include paper (unique_ID) nested in meta-analysis (data set) to capture the fact that a given meta-analysis has a specific set of papers, and may have multiple entries from the same paper. Here we focused on the two predictor model. The simpler random effects model from above is repeated here for easy comparison. A Chi-squared test finds a significant difference between the models, with the model with the more complex random effects structure having a lower AIC.

```{r}
lin_log_paper_lmer <- lmer(d_calc ~ mean_age + log(mean_age) + 
                          (1 | dataset / unique_ID),
                          weights = 1/d_var_calc, 
                          REML = F, data = all_data)

kable(coef(summary(lin_log_paper_lmer)))
kable(coef(summary(lin_log_lmer)))

kable(anova(lin_log_lmer, lin_log_paper_lmer))
```

We then predicted from this new, more complex model and compare it to the model without the nested paper random effects structure. Initial testing found that including the nested random effect in the predict algorithm led to massive overfitting, so just the simple random effects structure was used to compute predictions. Below is the figure comparing the two curves. The curves are very similar, suggesting that while adding the nesting of paper does significantly improve the model, visually the difference is very small.

```{r}
lin_log_paper_preds <- predict(lin_log_paper_lmer, re.form = ~ (1 | dataset))

all_data_preds <- all_data_preds %>%
  mutate(lin_log_paper_pred = lin_log_paper_preds)
```

```{r, echo=FALSE}
ggplot(all_data_preds, aes(x = mean_age, y = d_calc)) +
  geom_point(aes(size = 1/d_var_calc)) + 
  geom_line(aes(x = mean_age, y = lin_log_pred, size = 1,
                col = "Linear and Log")) + 
  geom_line(aes(x = mean_age, y = lin_log_paper_pred, size = 1, 
                col = "Linear and Log Paper")) + 
  facet_wrap(~ dataset, scales = "free") + 
  geom_hline(yintercept = 0, lty = 2, col = "black") +
  xlab("Mean age in days") +
  ylab("d (original and calculated)") +
  scale_colour_solarized(name = "Models", labels = c("Linear and Log" = "Linear and Log",
                                                 "Linear and Log Paper"= "Linear and Log Paper"))
```

## Complex mixed-effects (random slope)

Another way to make the random effects structure more complex is to add a random slope to the model. Two more models were built with a random slope by mean age log transformed, one without the nesting of paper, and one with the nesting of paper. Summaries of the new models with the random slope and the previous models without the random slope are below. Model comparison found that the model with nested structure and the slope was better than the simplest model (no nesting, no slope). The model with both the nested structure and the slope was also better than than the model with only the slope. Based on this it appears that the best model is one that includes both the nested random effects structure and a random slope of mean age log transformed.

```{r}
lin_log_slope_lmer <- lmer(d_calc ~ mean_age + log(mean_age) + 
                          (log(mean_age)| dataset),
                          weights = 1/d_var_calc, 
                          REML = F, data = all_data)

lin_log_paper_slope_lmer <- lmer(d_calc ~ mean_age + log(mean_age) + 
                          (log(mean_age)| dataset / unique_ID),
                          weights = 1/d_var_calc, 
                          REML = F, data = all_data)

kable(coef(summary(lin_log_slope_lmer)))
kable(coef(summary(lin_log_paper_slope_lmer)))
kable(coef(summary(lin_log_lmer)))
kable(coef(summary(lin_log_paper_lmer)))

kable(anova(lin_log_lmer, lin_log_slope_lmer))
kable(anova(lin_log_lmer, lin_log_paper_slope_lmer))
kable(anova(lin_log_paper_lmer, lin_log_slope_lmer))
kable(anova(lin_log_paper_lmer, lin_log_paper_slope_lmer))
kable(anova(lin_log_slope_lmer, lin_log_paper_slope_lmer))
```

As before, predictions were built for the new models and plotted on the data. Overall the models do look pretty similar in terms of their predicted fit of the data.

```{r}
lin_log_slope_preds <- predict(lin_log_slope_lmer, re.form = ~ (log(mean_age) | dataset))
lin_log_paper_slope_preds <- predict(lin_log_paper_slope_lmer, re.form = ~ (log(mean_age) | dataset))


all_data_preds <- all_data_preds %>%
  mutate(lin_log_slope_pred = lin_log_slope_preds) %>%
  mutate(lin_log_paper_slope_pred = lin_log_paper_slope_preds)
```

```{r, echo=FALSE}
ggplot(all_data_preds, aes(x = mean_age, y = d_calc)) +
  geom_point(aes(size = 1/d_var_calc)) + 
  geom_line(aes(x = mean_age, y = lin_log_pred, size = 1,
                col = "Linear and Log")) + 
  geom_line(aes(x = mean_age, y = lin_log_paper_pred, size = 1, 
                col = "Linear and Log Paper")) + 
  geom_line(aes(x = mean_age, y = lin_log_slope_pred, size = 1, 
                col = "Linear and Log Slope")) + 
  geom_line(aes(x = mean_age, y = lin_log_paper_slope_pred, size = 1, 
                col = "Linear and Log Paper Slope")) + 
  facet_wrap(~ dataset, scales = "free") + 
  geom_hline(yintercept = 0, lty = 2, col = "black") +
  xlab("Mean age in days") +
  ylab("d (original and calculated)") +
  scale_colour_solarized(name = "Models", breaks = c("Linear and Log",
                                                  "Linear and Log Paper",
                                                  "Linear and Log Slope",
                                                  "Linear and Log Paper Slope"),
                                       labels = c("Linear and Log" = "Linear and Log",
                                                 "Linear and Log Paper" = "Linear and Log Paper",
                                                 "Linear and Log Slope" = "Linear and Log Slope",
                                                 "Linear and Log Paper Slope" = "Linear and Log Paper Slope"))
```

## Subset to only those papers with multiple ages

One concern with using a random slope with meta-analysis nested by paper is that not all studies included more than one age group. To be sure that our random slope is really appropriate, here we look at only papers where more than one age group was tested.

### Two age groups

To start we arbitrarily decided on two age groups, at least more than two months apart. Below we build a new model only looking at data where papers had at least two age groups. The structure of the model is the same as the model with papers nested in meta-analysis and a random slope of mean age log transformed.

```{r}
multiage_data <- all_data_preds %>%
  group_by(dataset, unique_ID) %>%
  mutate(count_ages = length(unique(floor(mean_age/2)))) %>%
  filter(count_ages > 1)

lin_log_paper_slope_multiage_lmer <- lmer(d_calc ~ mean_age + log(mean_age) + 
                                     (log(mean_age) | dataset / unique_ID),
                                     weights = 1/d_var_calc, 
                                     REML = F, data = multiage_data)

kable(coef(summary(lin_log_paper_slope_multiage_lmer)))
```

Predictions were calculated for the new model and visually compared to the simple model on the full data set with no nested random effects and no random slope. The plot below suggests that for this reduced data set, the addition of the nested random effects and random slope does differ greatly from the simpler model.

```{r}
lin_log_paper_slope_multiage_preds <- predict(lin_log_paper_slope_multiage_lmer, re.form = ~ (log(mean_age)| dataset))

multiage_data <- multiage_data %>%
  ungroup %>%
  select(dataset, unique_ID, d_calc, mean_age, d_var_calc, lin_log_pred) %>%
  mutate(lin_log_paper_slope_multiage_pred = lin_log_paper_slope_multiage_preds)
```

```{r, echo=FALSE}
ggplot(multiage_data, aes(x = mean_age, y = d_calc)) +
  geom_point(aes(size = 1/d_var_calc)) + 
  geom_line(aes(x = mean_age, y = lin_log_pred, size = 1,
                col = "Linear and Log")) + 
  geom_line(aes(x = mean_age, y = lin_log_paper_slope_multiage_pred, size = 1,
                col = "Linear and Log Paper Slope\n2 Groups")) + 
  facet_wrap(~ dataset, scales = "free") + 
  geom_hline(yintercept = 0, lty = 2, col = "black") +
  xlab("Mean age in days") +
  ylab("d (original and calculated)") +
  scale_colour_solarized(name="Models", labels=c("Linear and Log" = "Linear and Log",
                                              "Linear and Log Paper Slope\n2 Groups" = "Linear and Log Paper Slope\n2 Groups"))
```

### Three age groups

We also did the same analysis, but further reducing the data set to only papers that included at least three different age groups.

```{r}
multiage_3_data <- all_data_preds %>%
  group_by(dataset, unique_ID) %>%
  mutate(count_ages = length(unique(floor(mean_age/2)))) %>%
  filter(count_ages > 2)

lin_log_paper_slope_multiage_3_lmer <- lmer(d_calc ~ mean_age + log(mean_age) + 
                                     (log(mean_age) | dataset / unique_ID),
                                     weights = 1/d_var_calc, 
                                     REML = F, data = multiage_3_data)

kable(coef(summary(lin_log_paper_slope_multiage_3_lmer)))
```

Predictions were calculated for the new model and visually compared to the simple model on the full data set with no nested random effects and no random slope. The plot below suggests that for this further reduced data set, the difference becomes the two models becomes even more apparent.

```{r}
lin_log_paper_slope_multiage_3_preds <- predict(lin_log_paper_slope_multiage_3_lmer, re.form = ~ (log(mean_age)| dataset))

multiage_3_data <- multiage_3_data %>%
  ungroup %>%
  select(dataset, unique_ID, d_calc, mean_age, d_var_calc, lin_log_pred) %>%
  mutate(lin_log_paper_slope_multiage_3_pred = lin_log_paper_slope_multiage_3_preds)
```

```{r, echo=FALSE}
ggplot(multiage_3_data, aes(x = mean_age, y = d_calc)) +
  geom_point(aes(size = 1/d_var_calc)) + 
  geom_line(aes(x = mean_age, y = lin_log_pred, size = 1,
                col = "Linear and Log")) + 
  geom_line(aes(x = mean_age, y = lin_log_paper_slope_multiage_3_pred, size = 1,
                col = "Linear and Log Paper Slope\n3 Groups")) + 
  facet_wrap(~ dataset, scales = "free") + 
  geom_hline(yintercept = 0, lty = 2, col = "black") +
  xlab("Mean age in days") +
  ylab("d (original and calculated)") +
  scale_colour_solarized(name="Models", labels=c("Linear and Log" = "Linear and Log",
                                              "Linear and Log Paper Slope\n3 Groups" = "Linear and Log Paper Slope\n3 Groups"))
```


# Conclusions

In this report we sought to find the best curve to describe developmental data. We found that, regarding predictors, the best fit of the data appears to be a model that includes both mean age and mean age log transformed as predictors. We further improved the curve by using mixed-effects regressions. We found that a more complicated random effects structure was best by: 1) nesting paper in meta-analysis, and 2) including a random slope by mean age log transformed. This structure was most appropriate when we only looked at papers that had at least two or three age groups, to ensure the random slope applied to all data analyzed.

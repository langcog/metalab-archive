geom_smooth(method="lm", color = "steelblue",alpha = .3, size = 1.25) +
theme_bw() +
scale_y_continuous(limits = c(-1, 2.5)) +
theme(text = element_text(size=30),
plot.title=element_text(size=20, face = "bold"),
plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
axis.line = element_line(color = 'black',size = 1.5),
axis.ticks = element_line(size = 1.5)) +
geom_text(fontface = 2, size = 6) +
annotate("text", x=4.25, y=2, size = 10,
label=paste("r=",round(cor(md$d_calculate, log(md$CDI_prod_mean), use = "complete"), 2)),
color="red")
dev.off()
dev.off()
ggplot(md, aes(x=log(CDI_prod_mean), y=d_calculate, label = num_lab))+
ylab("Cohen's d")+
xlab("Log mean CDI \n productive vocabulary")+
#geom_smooth(method="lm", formula=y~log(x)) +
geom_smooth(method="lm", color = "steelblue",alpha = .3, size = 1.25) +
theme_bw() +
scale_y_continuous(limits = c(-1, 2.5)) +
theme(text = element_text(size=30),
plot.title=element_text(size=20, face = "bold"),
plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
axis.line = element_line(color = 'black',size = 1.5),
axis.ticks = element_line(size = 1.5)) +
geom_text(fontface = 2, size = 6) +
annotate("text", x=4.25, y=2, size = 10,
label=paste("r=",round(cor(md$d_calculate, log(md$CDI_prod_mean), use = "complete"), 2)),
color="red")
plot vocab as predictor of effect size with basic linear model
```{r}
pdf("/Users/mll/Desktop/vocab.pdf", width = 5, height =5)
ggplot(md, aes(x=log(CDI_prod_mean), y=d_calculate, label = num_lab))+
ylab("Cohen's d")+
xlab("Log mean CDI \n productive vocabulary")+
#geom_smooth(method="lm", formula=y~log(x)) +
geom_smooth(method="lm", color = "steelblue",alpha = .3, size = 1.25) +
theme_bw() +
scale_y_continuous(limits = c(-1, 2.5)) +
theme(text = element_text(size=30),
plot.title=element_text(size=20, face = "bold"),
plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
axis.line = element_line(color = 'black',size = 1.5),
axis.ticks = element_line(size = 1.5)) +
geom_text(fontface = 2, size = 6) +
annotate("text", x=5.5, y=-.5, size = 10,
label=paste("r=",round(cor(md$d_calculate, log(md$CDI_prod_mean), use = "complete"), 2)),
color="red")
cor.test(md$d_calculate, log(md$CDI_prod_mean), use = "complete.obs")
dev.off()
## Analysis #3: Vocab size as predictor
plot vocab as predictor of effect size with basic linear model
```{r}
pdf("/Users/mll/Desktop/vocab.pdf", width = 5, height =6)
ggplot(md, aes(x=log(CDI_prod_mean), y=d_calculate, label = num_lab))+
ylab("Cohen's d")+
xlab("Log mean CDI \n productive vocabulary")+
#geom_smooth(method="lm", formula=y~log(x)) +
geom_smooth(method="lm", color = "steelblue",alpha = .3, size = 1.25) +
theme_bw() +
scale_y_continuous(limits = c(-1, 2.5)) +
theme(text = element_text(size=30),
plot.title=element_text(size=20, face = "bold"),
plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
axis.line = element_line(color = 'black',size = 1.5),
axis.ticks = element_line(size = 1.5)) +
geom_text(fontface = 2, size = 6) +
annotate("text", x=5.5, y=-.5, size = 10,
label=paste("r=",round(cor(md$d_calculate, log(md$CDI_prod_mean), use = "complete"), 2)),
color="red")
cor.test(md$d_calculate, log(md$CDI_prod_mean), use = "complete.obs")
dev.off()
```
c( -0.1976 ,1.0309     )-1
.41-1
pdf("/Users/mll/Desktop/vocab.pdf", width = 5, height =5)
ggplot(md, aes(x=log(CDI_prod_mean), y=d_calculate, label = num_lab))+
ylab("Cohen's d")+
xlab("Log CDI productive vocab.")+
#geom_smooth(method="lm", formula=y~log(x)) +
geom_smooth(method="lm", color = "steelblue",alpha = .3, size = 1.25) +
theme_bw() +
scale_y_continuous(limits = c(-1, 2.5)) +
theme(text = element_text(size=30),
plot.title=element_text(size=20, face = "bold"),
plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
axis.line = element_line(color = 'black',size = 1.5),
axis.ticks = element_line(size = 1.5)) +
geom_text(fontface = 2, size = 6) +
annotate("text", x=5.5, y=-.5, size = 10,
label=paste("r=",round(cor(md$d_calculate, log(md$CDI_prod_mean), use = "complete"), 2)),
color="red")
cor.test(md$d_calculate, log(md$CDI_prod_mean), use = "complete.obs")
dev.off()
pdf("/Users/mll/Desktop/vocab.pdf", width = 5, height =5)
ggplot(md, aes(x=log(CDI_prod_mean), y=d_calculate, label = num_lab))+
ylab("Cohen's d")+
xlab("Log CDI prod. vocab.")+
#geom_smooth(method="lm", formula=y~log(x)) +
geom_smooth(method="lm", color = "steelblue",alpha = .3, size = 1.25) +
theme_bw() +
scale_y_continuous(limits = c(-1, 2.5)) +
theme(text = element_text(size=30),
plot.title=element_text(size=20, face = "bold"),
plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
axis.line = element_line(color = 'black',size = 1.5),
axis.ticks = element_line(size = 1.5)) +
geom_text(fontface = 2, size = 6) +
annotate("text", x=5.5, y=-.5, size = 10,
label=paste("r=",round(cor(md$d_calculate, log(md$CDI_prod_mean), use = "complete"), 2)),
color="red")
cor.test(md$d_calculate, log(md$CDI_prod_mean), use = "complete.obs")
dev.off()
## Analysis #3: Vocab size as predictor
plot vocab as predictor of effect size with basic linear model
```{r}
pdf("/Users/mll/Desktop/vocab.pdf", width = 6, height =6)
ggplot(md, aes(x=log(CDI_prod_mean), y=d_calculate, label = num_lab))+
ylab("Cohen's d")+
xlab("Log CDI prod. vocab.")+
#geom_smooth(method="lm", formula=y~log(x)) +
geom_smooth(method="lm", color = "steelblue",alpha = .3, size = 1.25) +
theme_bw() +
scale_y_continuous(limits = c(-1, 2.5)) +
theme(text = element_text(size=30),
plot.title=element_text(size=20, face = "bold"),
plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
axis.line = element_line(color = 'black',size = 1.5),
axis.ticks = element_line(size = 1.5)) +
geom_text(fontface = 2, size = 6) +
annotate("text", x=5.5, y=-.5, size = 10,
label=paste("r=",round(cor(md$d_calculate, log(md$CDI_prod_mean), use = "complete"), 2)),
color="red")
cor.test(md$d_calculate, log(md$CDI_prod_mean), use = "complete.obs")
dev.off()
```
## Analysis #3: Vocab size as predictor
plot vocab as predictor of effect size with basic linear model
```{r}
pdf("/Users/mll/Desktop/vocab.pdf", width = 6, height =6)
ggplot(md, aes(x=log(CDI_prod_mean), y=d_calculate, label = num_lab))+
ylab("Cohen's d")+
xlab("Log CDI productive. vocab.")+
#geom_smooth(method="lm", formula=y~log(x)) +
geom_smooth(method="lm", color = "steelblue",alpha = .3, size = 1.25) +
theme_bw() +
scale_y_continuous(limits = c(-1, 2.5)) +
theme(text = element_text(size=30),
plot.title=element_text(size=20, face = "bold"),
plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
axis.line = element_line(color = 'black',size = 1.5),
axis.ticks = element_line(size = 1.5)) +
geom_text(fontface = 2, size = 6) +
annotate("text", x=5.5, y=-.5, size = 10,
label=paste("r=",round(cor(md$d_calculate, log(md$CDI_prod_mean), use = "complete"), 2)),
color="red")
cor.test(md$d_calculate, log(md$CDI_prod_mean), use = "complete.obs")
dev.off()
## Analysis #3: Vocab size as predictor
plot vocab as predictor of effect size with basic linear model
```{r}
pdf("/Users/mll/Desktop/vocab.pdf", width = 6, height =6)
ggplot(md, aes(x=log(CDI_prod_mean), y=d_calculate, label = num_lab))+
ylab("Cohen's d")+
xlab("Log CDI productive vocab.")+
#geom_smooth(method="lm", formula=y~log(x)) +
geom_smooth(method="lm", color = "steelblue",alpha = .3, size = 1.25) +
theme_bw() +
scale_y_continuous(limits = c(-1, 2.5)) +
theme(text = element_text(size=30),
plot.title=element_text(size=20, face = "bold"),
plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
axis.line = element_line(color = 'black',size = 1.5),
axis.ticks = element_line(size = 1.5)) +
geom_text(fontface = 2, size = 6) +
annotate("text", x=5.5, y=-.5, size = 10,
label=paste("r=",round(cor(md$d_calculate, log(md$CDI_prod_mean), use = "complete"), 2)),
color="red")
cor.test(md$d_calculate, log(md$CDI_prod_mean), use = "complete.obs")
dev.off()
```
Now, compare BOTH age and vocab in random effect model
```{r}
md.good = md[!is.na(md$CDI_prod_mean),]
result.all.mods= rma(md.good$d_calculate ~ log(md.good$CDI_prod_mean) +
log(md.good$age_mean..months.), vi = md.good$d_var, method = "REML")
result.all.mods
```
m1 = lm(scale(d_calculate) ~ scale(log(CDI_prod_mean)) +
scale(log(age_mean..months.)), d=md)
summary(m1)
m1 = lm(scale(d_calculate) ~ scale(log(CDI_prod_mean)) +
scale(log(age_mean..months.)), d=md)
pdf("/Users/mll/Desktop/vocab.pdf", width = 6, height =6)
ggplot(md, aes(x=log(CDI_prod_mean), y=d_calculate, label = num_lab))+
ylab("Cohen's d")+
#xlab("Log CDI productive vocab.")+
xlab("Age as moderator")+
#geom_smooth(method="lm", formula=y~log(x)) +
geom_smooth(method="lm", color = "steelblue",alpha = .3, size = 1.25) +
theme_bw() +
scale_y_continuous(limits = c(-1, 2.5)) +
theme(text = element_text(size=30),
plot.title=element_text(size=20, face = "bold"),
plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
axis.line = element_line(color = 'black',size = 1.5),
axis.ticks = element_line(size = 1.5)) +
geom_text(fontface = 2, size = 6) +
annotate("text", x=5.5, y=-.5, size = 10,
label=paste("r=",round(cor(md$d_calculate, log(md$CDI_prod_mean), use = "complete"), 2)),
color="red")
cor.test(md$d_calculate, log(md$CDI_prod_mean), use = "complete.obs")
dev.off()
---
title: "Disambiguation Meta-Analysis"
output: html_document
---
```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```
load libraries and functions
```{r}
library(ggplot2)
library(metafor)
source("/Documents/GRADUATE_SCHOOL/Ranalysis/useful.R")
# calculate variance on d, assumes variance in (imaginery) control group same as experimental
d_var<- function(n, d){
vd = ((2*n)/(n^2)) + ((d^2)/(4*n))
return(vd)
}
```
load data
```{r}
metad = read.csv("/Documents/GRADUATE_SCHOOL/Projects/ME_meta/SRCD_abstract/data/Disambiguation Meta-Analysis Data.csv")
```
calculate d for conditions not reported
```{r}
metad$d_by_t <- metad$t/sqrt(metad$N)
metad$d_by_M <- (metad$M_experimental-metad$M_baseline) / metad$SD
```
get a d for each condition
```{r}
metad$d_calculate <- ifelse(!is.na(metad$d), metad$d,
ifelse(!is.na(metad$d_by_t),metad$d_by_t,
ifelse(!is.na(metad$d_by_M), metad$d_by_M, NA)))
```
```{r echo = FALSE, eval = F}
#look at rows with discrepant d's
metad$d_error = ifelse((abs(metad$d_by_t-metad$d_by_M )> .1) | (abs(metad$d-metad$d_by_M ) > .1) | (abs(metad$d-metad$d_by_t)> .1), 1, 0)
metad[which(metad$d_error == 1),c("paper_key","t","N","M_experimental", "M_baseline" ,"SD", "d", "d_by_t", "d_by_M")] # Diesendruck and demarchena
```
remove conditions for which we have no d or we're excluding for methodological reasons
```{r}
#metad[!is.na(metad$d_calculate),c("paper_key","t","N","M_experimental", "M_baseline" ,"SD", "d", "d_by_t", "d_by_M",'d_calculate', 'd_error')]
md <- metad[!is.na(metad$d_calculate),]
md <- md[md$include.in.basic.analysis.==1,]
md <- md[md$paper_key != "frank2015",]
# sanity check for indexing: which condition has the max
#md[which(md$d_calculate==(max(md$d_calculate, na.rm=T))),
#  c("paper_key", "d_calculate")]
#md.order = md[order(md$d_calculate),]
```
```{r, eval = F, echo =F}
# look at conditions we have d's for
#md$notes_short = substr(md$Notes,1,40)
#md[, c("paper_key", 'notes_short', "expt_num",'d_calculate', "N")]
#md <- md[order(md$paper_key),]
#md[order(md$d_calculate),c("paper_key", 'notes_short', "expt_num",'d_calculate')]  #sorted by effectsize
```
get var on d
```{r}
md$d_var = apply(md[,c('N','d_calculate')], 1, function(x) d_var(x[1],x[2]))
```
summary of final sample of studies
```{r}
# total number of unique studies
length(unique(metad$paper_key))
# number of unique studies usable
length(unique(md$paper_key))#21
# total number of conditions
dim(md)[1] #63
```
## Analysis #1: Effect size across studies
distribution of raw cohen's d's.
```{r}
qplot(d_calculate, data=md, geom="histogram") +
geom_vline(xintercept = 0, color = "red", cex = 2)
```
get random effect model
```{r}
result.all = rma(md$d_calculate, vi = md$d_var, method = "REML")
result.all
```
result.all
forest(result.all,
slab = md$lab,
ilab = cbind(md$year, md$age_mean..months.,
md$N, as.character(md$DV.type)),
ilab.xpos = c(-6.25,-5,-4,-3),
order = order(md$DV.type, md$age_mean..months.),
mlab = "Grand effect size",
xlab ="Effect size estimate",
xlim = c(-9, 2),
annotate = F)
forest(result.all,
#slab = md$lab,
ilab = cbind(md$year, md$age_mean..months.,
md$N, as.character(md$DV.type)),
ilab.xpos = c(-6.25,-5,-4,-3),
order = order(md$DV.type, md$age_mean..months.),
mlab = "Grand effect size",
xlab ="Effect size estimate",
xlim = c(-9, 2),
annotate = F)
text(-8, 54, "First author", po
225/836
m = 225/836
927*m
system('python /Documents/GRADUATE_SCHOOL/Misc./psiturk/sqltocsv.py')
system('python /Documents/GRADUATE_SCHOOL/Misc./psiturk/sqltocsv.py')
library(rPython)
install.packages(rPython)
install.packages('rPython')
library(rPackages)
library(rPython)
python.load('/Documents/GRADUATE_SCHOOL/Misc./psiturk/sqltocsv.py')
python.load('/Documents/GRADUATE_SCHOOL/Misc./psiturk/sqltocsv.py')
python.load('/Documents/GRADUATE_SCHOOL/Misc./psiturk/sqltocsv.py')
python.load('/Documents/GRADUATE_SCHOOL/Misc./psiturk/sqltocsv.py')
library(shiny)
runApp('/Documents/GRADUATE_SCHOOL/Projects/ME_meta/metaviz/')
```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```
load libraries and functions
```{r}
library(ggplot2)
library(metafor)
source("/Documents/GRADUATE_SCHOOL/Ranalysis/useful.R")
# calculate variance on d, assumes variance in (imaginery) control group same as experimental
d_var<- function(n, d){
vd = ((2*n)/(n^2)) + ((d^2)/(4*n))
return(vd)
}
```
load data
```{r}
metad = read.csv("/Documents/GRADUATE_SCHOOL/Projects/ME_meta/SRCD_abstract/data/Disambiguation Meta-Analysis Data.csv")
```
calculate d for conditions not reported
```{r}
metad$d_by_t <- metad$t/sqrt(metad$N)
metad$d_by_M <- (metad$M_experimental-metad$M_baseline) / metad$SD
```
get a d for each condition
```{r}
metad$d_calculate <- ifelse(!is.na(metad$d), metad$d,
ifelse(!is.na(metad$d_by_t),metad$d_by_t,
ifelse(!is.na(metad$d_by_M), metad$d_by_M, NA)))
```
```{r echo = FALSE, eval = F}
#look at rows with discrepant d's
metad$d_error = ifelse((abs(metad$d_by_t-metad$d_by_M )> .1) | (abs(metad$d-metad$d_by_M ) > .1) | (abs(metad$d-metad$d_by_t)> .1), 1, 0)
metad[which(metad$d_error == 1),c("paper_key","t","N","M_experimental", "M_baseline" ,"SD", "d", "d_by_t", "d_by_M")] # Diesendruck and demarchena
```
remove conditions for which we have no d or we're excluding for methodological reasons
```{r}
#metad[!is.na(metad$d_calculate),c("paper_key","t","N","M_experimental", "M_baseline" ,"SD", "d", "d_by_t", "d_by_M",'d_calculate', 'd_error')]
md <- metad[!is.na(metad$d_calculate),]
md <- md[md$include.in.basic.analysis.==1,]
md <- md[md$paper_key != "frank2015",]
# sanity check for indexing: which condition has the max
#md[which(md$d_calculate==(max(md$d_calculate, na.rm=T))),
#  c("paper_key", "d_calculate")]
#md.order = md[order(md$d_calculate),]
```
```{r, eval = F, echo =F}
# look at conditions we have d's for
#md$notes_short = substr(md$Notes,1,40)
#md[, c("paper_key", 'notes_short', "expt_num",'d_calculate', "N")]
#md <- md[order(md$paper_key),]
#md[order(md$d_calculate),c("paper_key", 'notes_short', "expt_num",'d_calculate')]  #sorted by effectsize
```
get var on d
```{r}
md$d_var = apply(md[,c('N','d_calculate')], 1, function(x) d_var(x[1],x[2]))
```
summary of final sample of studies
```{r}
# total number of unique studies
length(unique(metad$paper_key))
# number of unique studies usable
length(unique(md$paper_key))#21
# total number of conditions
dim(md)[1] #63
```
head(md)
write.csv(md,'~/Desktop/
mutual_exclusivity.csv')
summary(md)
dim(md)
write.csv(md,'~/Desktop/
mutual_exclusivity.csv')
rm(list=ls())
library(shiny)
runApp('/Documents/GRADUATE_SCHOOL/Projects/ME_meta/metaviz/')
md
---
title: "Disambiguation Meta-Analysis"
output: html_document
---
```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```
load libraries and functions
```{r}
library(ggplot2)
library(metafor)
source("/Documents/GRADUATE_SCHOOL/Ranalysis/useful.R")
# calculate variance on d, assumes variance in (imaginery) control group same as experimental
d_var<- function(n, d){
vd = ((2*n)/(n^2)) + ((d^2)/(4*n))
return(vd)
}
```
load data
```{r}
metad = read.csv("/Documents/GRADUATE_SCHOOL/Projects/ME_meta/SRCD_abstract/data/Disambiguation Meta-Analysis Data.csv")
```
calculate d for conditions not reported
```{r}
metad$d_by_t <- metad$t/sqrt(metad$N)
metad$d_by_M <- (metad$M_experimental-metad$M_baseline) / metad$SD
```
get a d for each condition
```{r}
metad$d_calculate <- ifelse(!is.na(metad$d), metad$d,
ifelse(!is.na(metad$d_by_t),metad$d_by_t,
ifelse(!is.na(metad$d_by_M), metad$d_by_M, NA)))
```
```{r echo = FALSE, eval = F}
#look at rows with discrepant d's
metad$d_error = ifelse((abs(metad$d_by_t-metad$d_by_M )> .1) | (abs(metad$d-metad$d_by_M ) > .1) | (abs(metad$d-metad$d_by_t)> .1), 1, 0)
metad[which(metad$d_error == 1),c("paper_key","t","N","M_experimental", "M_baseline" ,"SD", "d", "d_by_t", "d_by_M")] # Diesendruck and demarchena
```
remove conditions for which we have no d or we're excluding for methodological reasons
```{r}
#metad[!is.na(metad$d_calculate),c("paper_key","t","N","M_experimental", "M_baseline" ,"SD", "d", "d_by_t", "d_by_M",'d_calculate', 'd_error')]
md <- metad[!is.na(metad$d_calculate),]
md <- md[md$include.in.basic.analysis.==1,]
#md <- md[md$paper_key != "frank2015",]
# sanity check for indexing: which condition has the max
#md[which(md$d_calculate==(max(md$d_calculate, na.rm=T))),
#  c("paper_key", "d_calculate")]
#md.order = md[order(md$d_calculate),]
```
```{r, eval = F, echo =F}
# look at conditions we have d's for
#md$notes_short = substr(md$Notes,1,40)
#md[, c("paper_key", 'notes_short', "expt_num",'d_calculate', "N")]
#md <- md[order(md$paper_key),]
#md[order(md$d_calculate),c("paper_key", 'notes_short', "expt_num",'d_calculate')]  #sorted by effectsize
```
get var on d
```{r}
md$d_var = apply(md[,c('N','d_calculate')], 1, function(x) d_var(x[1],x[2]))
```
head(md)
write.csv(md,'~/Desktop/
mutual_exclusivity.csv')
head(md)
names(md)
shiny::runApp('/Documents/GRADUATE_SCHOOL/Projects/ME_meta/metaviz')
wd
getwd()
setwd('/Documents/GRADUATE_SCHOOL/Projects/ME_meta/metaviz/data')
setwd('/Documents/GRADUATE_SCHOOL/Projects/ME_meta/metaviz/')
shiny::runApp('/Documents/GRADUATE_SCHOOL/Projects/ME_meta/metaviz')
